{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with BN in TensorFlow\n",
    "\n",
    "* DCGAN 의 CNN 모델을 만들어서 MNIST classification 을 수행해보자. \n",
    "* TF 에서 BN 을 적용하는 걸 연습하는 용도. \n",
    "* MNIST 로 정확도를 테스트하기 어렵다면 다른 데이터셋도 구해서 적용해보자.\n",
    "\n",
    "Discriminator of DCGAN:\n",
    "\n",
    "![Discriminator of DCGAN](http://bamos.github.io/data/2016-08-09/discrim-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_init(shape):\n",
    "    return tf.truncated_normal(shape, stddev=0.1)\n",
    "\n",
    "def bias_init(shape):\n",
    "    return tf.constant(0.1, shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 참조한 image completion 코드에서는 다른 식으로 구현하는데, 그게 더 빠른가?\n",
    "# 특이하게 구현함. https://github.com/bamos/dcgan-completion.tensorflow/blob/master/ops.py\n",
    "def lrelu(x, leak=0.2):\n",
    "    return tf.maximum(x, x*leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(summed_input, n_out, is_training):\n",
    "    return tf.layers.batch_normalization(summed_input, training=is_training)\n",
    "#     return tf.contrib.layers.batch_norm(summed_input, center=True, scale=True, is_training=is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(x, n_out, phase_train, scope='bn'):\n",
    "    \"\"\"\n",
    "    Batch normalization on convolutional maps.\n",
    "    Args:\n",
    "        x:           Tensor, 4D BHWD input maps\n",
    "        n_out:       integer, depth of input maps\n",
    "        phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "        scope:       string, variable scope\n",
    "    Return:\n",
    "        normed:      batch-normalized maps\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "                                     name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "                                      name='gamma', trainable=True)\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        mean, var = tf.cond(phase_train,\n",
    "                            mean_var_with_update,\n",
    "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 일단 MNIST datset 은 28x28x1 이므로, \n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "isTraining = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reshape for CNN\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "# first conv layer: \n",
    "W1 = tf.Variable(weight_init([5, 5, 1, 64]))\n",
    "\n",
    "a1 = tf.nn.conv2d(X_img, W1, strides=[1, 2, 2, 1], padding='SAME')\n",
    "bn1 = batch_norm(a1, 64, isTraining)\n",
    "h1 = lrelu(bn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W2 = tf.Variable(weight_init([5, 5, 64, 128]))\n",
    "\n",
    "a2 = tf.nn.conv2d(h1, W2, strides=[1, 2, 2, 1], padding='SAME')\n",
    "bn2 = batch_norm(a2, 128, isTraining)\n",
    "h2 = lrelu(bn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W3 = tf.Variable(weight_init([5, 5, 128, 256]))\n",
    "\n",
    "a3 = tf.nn.conv2d(h2, W3, strides=[1, 2, 2, 1], padding='SAME')\n",
    "bn3 = batch_norm(a3, 256, isTraining)\n",
    "h3 = lrelu(bn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FC layer. 원래 더 해야 하지만 이건 MNIST 니까 여기까지만 하자.\n",
    "# 원래는 sigmoid 로 0/1 만 판별하는데, MNIST 는 10개니까 softmax 로 해야함\n",
    "\n",
    "W4 = tf.Variable(weight_init([4096, 10]))\n",
    "b4 = tf.Variable(bias_init([10]))\n",
    "h3_flat = tf.reshape(h3, [-1, 4096])\n",
    "\n",
    "# last layer activation = logit\n",
    "logits = tf.matmul(h3_flat, W4) + b4\n",
    "y_prob = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "solver = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = tf.argmax(logits, axis=1)\n",
    "correction = tf.equal(pred, tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # moving average 때문에 얘를 해줘야 된다는데 잘 모르겠음\n",
    "#     update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "#     with tf.control_dependencies(update_ops):\n",
    "#         solver = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    batch_size = 100\n",
    "    total_batch = mnist.train.num_examples / batch_size\n",
    "    history = []\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(20):\n",
    "        loss_sum = 0\n",
    "        for i in range(total_batch / 20):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            loss_cur, _ = sess.run([loss, solver], feed_dict={X: batch[0], Y: batch[1], isTraining: True})\n",
    "            loss_sum += np.average(loss_cur)\n",
    "\n",
    "        # accuracy & loss calc\n",
    "        train_loss = loss_sum / total_batch\n",
    "        test_loss = np.average(sess.run(loss, feed_dict={X: mnist.test.images, Y: mnist.test.labels, isTraining: False}))\n",
    "\n",
    "        # calculate accuracy for both train and test\n",
    "        train_acc = sess.run(accuracy, {X: mnist.train.images[:10000], Y: mnist.train.labels[:10000], isTraining: False})\n",
    "        test_acc = sess.run(accuracy, {X: mnist.test.images, Y: mnist.test.labels, isTraining: False})\n",
    "        print(\"[{:3}] train: {:.5f} / test: {:.5f} | [acc] train: {:.4f} / test: {:.4f}\"\n",
    "              .format(epoch+1, train_loss, test_loss, train_acc, test_acc))\n",
    "        history.append([train_acc, test_acc])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1] train: 0.07143 / test: 2.04948 | [acc] train: 0.7314 / test: 0.7382\n",
      "[  2] train: 0.03412 / test: 1.76274 | [acc] train: 0.7806 / test: 0.7875\n",
      "[  3] train: 0.02681 / test: 2.02391 | [acc] train: 0.7755 / test: 0.7755\n",
      "[  4] train: 0.02151 / test: 1.14994 | [acc] train: 0.8283 / test: 0.8347\n",
      "[  5] train: 0.01732 / test: 0.75705 | [acc] train: 0.8806 / test: 0.8918\n",
      "[  6] train: 0.01300 / test: 1.60495 | [acc] train: 0.7862 / test: 0.7912\n",
      "[  7] train: 0.01150 / test: 0.65120 | [acc] train: 0.8867 / test: 0.8966\n",
      "[  8] train: 0.01122 / test: 0.65268 | [acc] train: 0.8903 / test: 0.8984\n",
      "[  9] train: 0.01274 / test: 0.69672 | [acc] train: 0.8739 / test: 0.8912\n",
      "[ 10] train: 0.01236 / test: 0.37112 | [acc] train: 0.9342 / test: 0.9377\n",
      "[ 11] train: 0.00931 / test: 0.47703 | [acc] train: 0.9220 / test: 0.9301\n",
      "[ 12] train: 0.00936 / test: 0.39207 | [acc] train: 0.9332 / test: 0.9372\n",
      "[ 13] train: 0.01030 / test: 0.72610 | [acc] train: 0.8951 / test: 0.8956\n",
      "[ 14] train: 0.00975 / test: 0.42332 | [acc] train: 0.9303 / test: 0.9301\n",
      "[ 15] train: 0.01033 / test: 0.58341 | [acc] train: 0.9047 / test: 0.9066\n",
      "[ 16] train: 0.01384 / test: 1.58433 | [acc] train: 0.8126 / test: 0.8164\n",
      "[ 17] train: 0.01441 / test: 0.60093 | [acc] train: 0.9135 / test: 0.9176\n",
      "[ 18] train: 0.00980 / test: 0.27311 | [acc] train: 0.9496 / test: 0.9595\n",
      "[ 19] train: 0.00656 / test: 0.35354 | [acc] train: 0.9436 / test: 0.9460\n",
      "[ 20] train: 0.00522 / test: 0.52739 | [acc] train: 0.9259 / test: 0.9242\n"
     ]
    }
   ],
   "source": [
    "history_bn = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = np.array(history)\n",
    "history_bn = np.array(history_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history[:, 1], label=\"without bn\")\n",
    "plt.plot(history_bn[:, 1], label=\"bn\")\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
