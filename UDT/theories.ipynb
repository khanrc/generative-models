{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Domain Transfer\n",
    "\n",
    "a.k.a. Unpaired Image-to-Image Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CycleGAN, DiscoGAN, DualGAN\n",
    "\n",
    "* Zhu, Jun-Yan, et al. \"Unpaired image-to-image translation using cycle-consistent adversarial networks.\" arXiv preprint arXiv:1703.10593 (2017).\n",
    "* Kim, Taeksoo, et al. \"Learning to discover cross-domain relations with generative adversarial networks.\" arXiv preprint arXiv:1703.05192 (2017).\n",
    "* Yi, Zili, Hao Zhang, and Ping Tan Gong. \"DualGAN: Unsupervised Dual Learning for Image-to-Image Translation.\" arXiv preprint arXiv:1704.02510 (2017).\n",
    "\n",
    "\n",
    "세 논문 전부 동일한 아이디어를 제안함 - Cycle consistency (http://ruotianluo.github.io/2017/04/12/triplets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others\n",
    "\n",
    "UDT (UIT) 로 제안되었거나, UDT task 에 적용될 수 있는 케이스\n",
    "\n",
    "* Proposed for UDT\n",
    "    * **DTN** (Domain Transfer Networks) - Taigman, Yaniv, Adam Polyak, and Lior Wolf. \"Unsupervised cross-domain image generation.\" arXiv preprint arXiv:1611.02200 (2016).\n",
    "* Can be applied to UDT\n",
    "    * **CoGAN** is a model which also works on unpaired images; the idea is to use two shared-weight generators to generate two images (in two domains) from one single random noise $z$, the generated images should fool the discriminator in each domain. \n",
    "    * **BiGAN** originally is to find a function $E$, which is the inverse function of generator $G$, i.e. given a image, outputs what $z$ generates the image. If you treat $z$ as a image in another domain, then BiGAN can be used for this task.\n",
    "* Supervised Domain Transfer (Paired Image-to-Image Translation)\n",
    "    * pix2pix - upper bound performance of above models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTN\n",
    "\n",
    "* DTN - 3 concepts\n",
    "    * source domain S, target domain T 에 대해, X = S + T 라 하자.\n",
    "    * G 는 AE 형태로, X 를 받아서 X 을 생성한다. 단, GAN 로스를 사용하므로 reconstruction 이 될 필요는 없음.\n",
    "    * 1) G(x)~X. 일단 G 는 X 를 생성 (즉 S 든 T 든 잘 만들어야함. D 가 속도록.)\n",
    "    * 2) G(t)=t. T 는 reconstruction 도 되어야함 (identity mapping)\n",
    "    * 3) f(G(s))=f(s). S 는 feature-reconstruction 이 되어야함. 즉, pixel-level 로 동일하지 않아도 되지만 deep feature level 로는 동일해야 함.\n",
    "    \n",
    "다른 관점에서 요약하면, X 에 대한 오토인코더인데 T_loss 는 그대로 recon_loss 지만, S_loss 는 deep feature 로 계산하는 것 - 그리고 여기에 AE 를 학습할 때 GAN loss 를 더함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pix2pix\n",
    "\n",
    "* U-Net (AE) + GAN loss\n",
    "* References\n",
    "    * https://hackernoon.com/remastering-classic-films-in-tensorflow-with-pix2pix-f4d551fa0503\n",
    "    * https://affinelayer.com/pix2pix/\n",
    "    * https://github.com/taey16/pix2pixBEGAN.pytorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 - tf.latest",
   "language": "python",
   "name": "python2-tf-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
