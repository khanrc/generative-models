{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "D_W1 = tf.Variable(xavier_init([784, 128]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "D_params = [D_W1, D_b1, D_W2, D_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Z is noised input\n",
    "# G(z) => generated new x\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "G_W1 = tf.Variable(xavier_init([100, 128]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "G_W2 = tf.Variable(xavier_init([128, 784]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "G_params = [G_W1, G_b1, G_W2, G_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "    D_a1 = tf.matmul(x, D_W1) + D_b1\n",
    "    D_h1 = tf.nn.relu(D_a1)\n",
    "    # what is logit?\n",
    "    # logit is inverse function of sigmoid function\n",
    "    # therefore, D_logit = logit(D_prob).\n",
    "    # 즉, sigmoid 함수에 들어가는 값이 logit 이라고 할 수 있음.\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    \n",
    "    return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    G_a1 = tf.matmul(z, G_W1) + G_b1\n",
    "    G_h1 = tf.nn.relu(G_a1)\n",
    "    # what is differ between logit <> log_prob ?\n",
    "    # 이것도 걍 로짓이 맞는거 같은데? logit_prob 의 약자일것 같음.\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "    \n",
    "    return G_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G_sample = generator(Z)\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# V1. paper-based learning (다른 코드를 보면 logit 을 이용해서 CE로 계산을 함. 이게 더 성능이 잘 나오나봄)\n",
    "# http://bamos.github.io/2016/08/09/deep-completion/ 참고\n",
    "\n",
    "# maximize x => minimize -x\n",
    "#D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1-D_fake))\n",
    "#G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "\n",
    "# V2.\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=D_params)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=G_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "Z_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D_losses = []\n",
    "G_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0] D_loss: 1.3176 / G_loss: 2.3101\n",
      "[   1000] D_loss: 0.0168 / G_loss: 10.1525\n",
      "[   2000] D_loss: 0.0074 / G_loss: 8.2010\n",
      "[   3000] D_loss: 0.0356 / G_loss: 5.4216\n",
      "[   4000] D_loss: 0.0739 / G_loss: 5.7155\n",
      "[   5000] D_loss: 0.2324 / G_loss: 6.0828\n",
      "[   6000] D_loss: 0.2134 / G_loss: 4.8982\n",
      "[   7000] D_loss: 0.3079 / G_loss: 5.2566\n",
      "[   8000] D_loss: 0.4939 / G_loss: 4.1673\n",
      "[   9000] D_loss: 0.5883 / G_loss: 3.3370\n",
      "[  10000] D_loss: 0.4114 / G_loss: 3.7350\n",
      "[  11000] D_loss: 0.4883 / G_loss: 2.7931\n",
      "[  12000] D_loss: 0.4879 / G_loss: 2.6027\n",
      "[  13000] D_loss: 0.5578 / G_loss: 2.7210\n",
      "[  14000] D_loss: 0.7348 / G_loss: 2.4082\n",
      "[  15000] D_loss: 0.5036 / G_loss: 2.3248\n",
      "[  16000] D_loss: 0.5502 / G_loss: 2.5480\n",
      "[  17000] D_loss: 0.6905 / G_loss: 2.1593\n",
      "[  18000] D_loss: 0.7744 / G_loss: 1.9736\n",
      "[  19000] D_loss: 0.7927 / G_loss: 2.1514\n",
      "[  20000] D_loss: 0.5857 / G_loss: 1.9863\n",
      "[  21000] D_loss: 0.6792 / G_loss: 2.0490\n",
      "[  22000] D_loss: 0.6250 / G_loss: 2.2876\n",
      "[  23000] D_loss: 0.6230 / G_loss: 2.0727\n",
      "[  24000] D_loss: 0.6646 / G_loss: 1.9748\n",
      "[  25000] D_loss: 0.6416 / G_loss: 2.4843\n",
      "[  26000] D_loss: 0.7843 / G_loss: 2.1687\n",
      "[  27000] D_loss: 0.6622 / G_loss: 2.0285\n",
      "[  28000] D_loss: 0.6921 / G_loss: 2.5590\n",
      "[  29000] D_loss: 0.6881 / G_loss: 2.2060\n",
      "[  30000] D_loss: 0.6922 / G_loss: 2.1020\n",
      "[  31000] D_loss: 0.6630 / G_loss: 2.2051\n",
      "[  32000] D_loss: 0.6653 / G_loss: 2.2292\n",
      "[  33000] D_loss: 0.7566 / G_loss: 2.0642\n",
      "[  34000] D_loss: 0.7495 / G_loss: 2.2780\n",
      "[  35000] D_loss: 0.7053 / G_loss: 1.7900\n",
      "[  36000] D_loss: 0.8078 / G_loss: 2.3594\n",
      "[  37000] D_loss: 0.6733 / G_loss: 2.5752\n",
      "[  38000] D_loss: 0.5546 / G_loss: 2.3888\n",
      "[  39000] D_loss: 0.5319 / G_loss: 2.3170\n",
      "[  40000] D_loss: 0.6762 / G_loss: 2.1776\n",
      "[  41000] D_loss: 0.6119 / G_loss: 2.1537\n",
      "[  42000] D_loss: 0.7268 / G_loss: 2.5392\n",
      "[  43000] D_loss: 0.6248 / G_loss: 2.2803\n",
      "[  44000] D_loss: 0.7789 / G_loss: 1.9757\n",
      "[  45000] D_loss: 0.7333 / G_loss: 2.4367\n",
      "[  46000] D_loss: 0.5588 / G_loss: 2.8215\n",
      "[  47000] D_loss: 0.6858 / G_loss: 2.3992\n",
      "[  48000] D_loss: 0.6049 / G_loss: 2.2200\n",
      "[  49000] D_loss: 0.5930 / G_loss: 2.4047\n",
      "[  50000] D_loss: 0.6559 / G_loss: 2.3283\n",
      "[  51000] D_loss: 0.7396 / G_loss: 2.0188\n",
      "[  52000] D_loss: 0.6253 / G_loss: 2.3721\n",
      "[  53000] D_loss: 0.7124 / G_loss: 2.3005\n",
      "[  54000] D_loss: 0.5745 / G_loss: 2.1456\n",
      "[  55000] D_loss: 0.5381 / G_loss: 2.2569\n",
      "[  56000] D_loss: 0.6252 / G_loss: 2.4940\n",
      "[  57000] D_loss: 0.5565 / G_loss: 2.2452\n",
      "[  58000] D_loss: 0.5278 / G_loss: 2.1077\n",
      "[  59000] D_loss: 0.4371 / G_loss: 2.2206\n",
      "[  60000] D_loss: 0.7199 / G_loss: 2.6715\n",
      "[  61000] D_loss: 0.6340 / G_loss: 2.4770\n",
      "[  62000] D_loss: 0.5627 / G_loss: 2.3750\n",
      "[  63000] D_loss: 0.6117 / G_loss: 2.2920\n",
      "[  64000] D_loss: 0.6641 / G_loss: 2.4624\n",
      "[  65000] D_loss: 0.8124 / G_loss: 2.1644\n",
      "[  66000] D_loss: 0.6682 / G_loss: 2.6986\n",
      "[  67000] D_loss: 0.5728 / G_loss: 2.2445\n",
      "[  68000] D_loss: 0.5731 / G_loss: 2.2615\n",
      "[  69000] D_loss: 0.5777 / G_loss: 2.5604\n",
      "[  70000] D_loss: 0.6487 / G_loss: 2.3390\n",
      "[  71000] D_loss: 0.4410 / G_loss: 2.4347\n",
      "[  72000] D_loss: 0.4198 / G_loss: 2.3162\n",
      "[  73000] D_loss: 0.6805 / G_loss: 2.5070\n",
      "[  74000] D_loss: 0.6280 / G_loss: 2.6811\n",
      "[  75000] D_loss: 0.5542 / G_loss: 2.5591\n",
      "[  76000] D_loss: 0.7072 / G_loss: 2.4417\n",
      "[  77000] D_loss: 0.5368 / G_loss: 2.1787\n",
      "[  78000] D_loss: 0.7521 / G_loss: 2.7140\n",
      "[  79000] D_loss: 0.5554 / G_loss: 2.5524\n",
      "[  80000] D_loss: 0.5798 / G_loss: 2.7037\n",
      "[  81000] D_loss: 0.6010 / G_loss: 2.1651\n",
      "[  82000] D_loss: 0.5388 / G_loss: 2.7340\n",
      "[  83000] D_loss: 0.4843 / G_loss: 2.3757\n",
      "[  84000] D_loss: 0.4316 / G_loss: 3.0240\n",
      "[  85000] D_loss: 0.5146 / G_loss: 2.4781\n",
      "[  86000] D_loss: 0.6319 / G_loss: 2.4955\n",
      "[  87000] D_loss: 0.5981 / G_loss: 2.7125\n",
      "[  88000] D_loss: 0.6024 / G_loss: 2.5834\n",
      "[  89000] D_loss: 0.4562 / G_loss: 2.3751\n",
      "[  90000] D_loss: 0.5245 / G_loss: 2.4297\n",
      "[  91000] D_loss: 0.5662 / G_loss: 2.3564\n",
      "[  92000] D_loss: 0.5893 / G_loss: 2.6230\n",
      "[  93000] D_loss: 0.4764 / G_loss: 2.5599\n",
      "[  94000] D_loss: 0.6215 / G_loss: 2.8431\n",
      "[  95000] D_loss: 0.5172 / G_loss: 2.3270\n",
      "[  96000] D_loss: 0.4713 / G_loss: 2.3198\n",
      "[  97000] D_loss: 0.6534 / G_loss: 2.2775\n",
      "[  98000] D_loss: 0.5092 / G_loss: 2.8363\n",
      "[  99000] D_loss: 0.6022 / G_loss: 3.0576\n",
      "[ 100000] D_loss: 0.4074 / G_loss: 2.6921\n",
      "[ 101000] D_loss: 0.5232 / G_loss: 2.2487\n",
      "[ 102000] D_loss: 0.5520 / G_loss: 2.4085\n",
      "[ 103000] D_loss: 0.4915 / G_loss: 2.8245\n",
      "[ 104000] D_loss: 0.5006 / G_loss: 2.4558\n",
      "[ 105000] D_loss: 0.6304 / G_loss: 2.8348\n",
      "[ 106000] D_loss: 0.4516 / G_loss: 2.7199\n",
      "[ 107000] D_loss: 0.6537 / G_loss: 2.3864\n",
      "[ 108000] D_loss: 0.6274 / G_loss: 2.6161\n",
      "[ 109000] D_loss: 0.5005 / G_loss: 2.9289\n",
      "[ 110000] D_loss: 0.5121 / G_loss: 2.5399\n",
      "[ 111000] D_loss: 0.6404 / G_loss: 2.5260\n",
      "[ 112000] D_loss: 0.4191 / G_loss: 2.3264\n",
      "[ 113000] D_loss: 0.5911 / G_loss: 2.3667\n",
      "[ 114000] D_loss: 0.4945 / G_loss: 2.6201\n",
      "[ 115000] D_loss: 0.6199 / G_loss: 2.9199\n",
      "[ 116000] D_loss: 0.6042 / G_loss: 2.6007\n",
      "[ 117000] D_loss: 0.5266 / G_loss: 2.6268\n",
      "[ 118000] D_loss: 0.6569 / G_loss: 2.4343\n",
      "[ 119000] D_loss: 0.5080 / G_loss: 2.8253\n",
      "[ 120000] D_loss: 0.5093 / G_loss: 2.5460\n",
      "[ 121000] D_loss: 0.4712 / G_loss: 2.6914\n",
      "[ 122000] D_loss: 0.4989 / G_loss: 2.5784\n",
      "[ 123000] D_loss: 0.4005 / G_loss: 2.8547\n",
      "[ 124000] D_loss: 0.4903 / G_loss: 2.5340\n",
      "[ 125000] D_loss: 0.4629 / G_loss: 2.8182\n",
      "[ 126000] D_loss: 0.4589 / G_loss: 2.6562\n",
      "[ 127000] D_loss: 0.4215 / G_loss: 2.9514\n",
      "[ 128000] D_loss: 0.4279 / G_loss: 2.6273\n",
      "[ 129000] D_loss: 0.6004 / G_loss: 2.5916\n",
      "[ 130000] D_loss: 0.6023 / G_loss: 2.4470\n",
      "[ 131000] D_loss: 0.5024 / G_loss: 2.7746\n",
      "[ 132000] D_loss: 0.5042 / G_loss: 2.8842\n",
      "[ 133000] D_loss: 0.4331 / G_loss: 2.9766\n",
      "[ 134000] D_loss: 0.4103 / G_loss: 2.7049\n",
      "[ 135000] D_loss: 0.5038 / G_loss: 3.0835\n",
      "[ 136000] D_loss: 0.3286 / G_loss: 3.4627\n",
      "[ 137000] D_loss: 0.4698 / G_loss: 2.8221\n",
      "[ 138000] D_loss: 0.6132 / G_loss: 2.3118\n",
      "[ 139000] D_loss: 0.4556 / G_loss: 2.6359\n",
      "[ 140000] D_loss: 0.6590 / G_loss: 3.4199\n",
      "[ 141000] D_loss: 0.4156 / G_loss: 2.7273\n",
      "[ 142000] D_loss: 0.4073 / G_loss: 3.0818\n",
      "[ 143000] D_loss: 0.4566 / G_loss: 2.6694\n",
      "[ 144000] D_loss: 0.3673 / G_loss: 3.3967\n",
      "[ 145000] D_loss: 0.5048 / G_loss: 3.1181\n",
      "[ 146000] D_loss: 0.4650 / G_loss: 3.2321\n",
      "[ 147000] D_loss: 0.3869 / G_loss: 2.4543\n",
      "[ 148000] D_loss: 0.4308 / G_loss: 3.0242\n",
      "[ 149000] D_loss: 0.4463 / G_loss: 2.8695\n",
      "[ 150000] D_loss: 0.4658 / G_loss: 2.5299\n",
      "[ 151000] D_loss: 0.3967 / G_loss: 2.5622\n",
      "[ 152000] D_loss: 0.4782 / G_loss: 2.9424\n",
      "[ 153000] D_loss: 0.5608 / G_loss: 3.1679\n",
      "[ 154000] D_loss: 0.3928 / G_loss: 2.5768\n",
      "[ 155000] D_loss: 0.4438 / G_loss: 2.4663\n",
      "[ 156000] D_loss: 0.5402 / G_loss: 2.7845\n",
      "[ 157000] D_loss: 0.3865 / G_loss: 2.8269\n",
      "[ 158000] D_loss: 0.3244 / G_loss: 2.8891\n",
      "[ 159000] D_loss: 0.5466 / G_loss: 2.9313\n",
      "[ 160000] D_loss: 0.4290 / G_loss: 2.9490\n",
      "[ 161000] D_loss: 0.5146 / G_loss: 3.0441\n",
      "[ 162000] D_loss: 0.3171 / G_loss: 2.9339\n",
      "[ 163000] D_loss: 0.4184 / G_loss: 3.0301\n",
      "[ 164000] D_loss: 0.4880 / G_loss: 3.6747\n",
      "[ 165000] D_loss: 0.3993 / G_loss: 3.2062\n",
      "[ 166000] D_loss: 0.4626 / G_loss: 2.8484\n",
      "[ 167000] D_loss: 0.4083 / G_loss: 2.7568\n",
      "[ 168000] D_loss: 0.4898 / G_loss: 3.3496\n",
      "[ 169000] D_loss: 0.4401 / G_loss: 2.8764\n",
      "[ 170000] D_loss: 0.4842 / G_loss: 3.0586\n",
      "[ 171000] D_loss: 0.5546 / G_loss: 3.2759\n",
      "[ 172000] D_loss: 0.5342 / G_loss: 2.8558\n",
      "[ 173000] D_loss: 0.4983 / G_loss: 2.9651\n",
      "[ 174000] D_loss: 0.4197 / G_loss: 3.0695\n",
      "[ 175000] D_loss: 0.3247 / G_loss: 2.9740\n",
      "[ 176000] D_loss: 0.4133 / G_loss: 2.7371\n",
      "[ 177000] D_loss: 0.4872 / G_loss: 3.2975\n",
      "[ 178000] D_loss: 0.3828 / G_loss: 2.8393\n",
      "[ 179000] D_loss: 0.3164 / G_loss: 3.0108\n",
      "[ 180000] D_loss: 0.3241 / G_loss: 2.3919\n",
      "[ 181000] D_loss: 0.4663 / G_loss: 3.0650\n",
      "[ 182000] D_loss: 0.4208 / G_loss: 3.1607\n",
      "[ 183000] D_loss: 0.2400 / G_loss: 3.4799\n",
      "[ 184000] D_loss: 0.3910 / G_loss: 3.6011\n",
      "[ 185000] D_loss: 0.4300 / G_loss: 3.3751\n",
      "[ 186000] D_loss: 0.4035 / G_loss: 3.0838\n",
      "[ 187000] D_loss: 0.3244 / G_loss: 3.1635\n",
      "[ 188000] D_loss: 0.3632 / G_loss: 3.3734\n",
      "[ 189000] D_loss: 0.3877 / G_loss: 3.1603\n",
      "[ 190000] D_loss: 0.4567 / G_loss: 3.0939\n",
      "[ 191000] D_loss: 0.3388 / G_loss: 2.8895\n",
      "[ 192000] D_loss: 0.4440 / G_loss: 3.1210\n",
      "[ 193000] D_loss: 0.4671 / G_loss: 3.1249\n",
      "[ 194000] D_loss: 0.5196 / G_loss: 2.9740\n",
      "[ 195000] D_loss: 0.3403 / G_loss: 3.4128\n",
      "[ 196000] D_loss: 0.4027 / G_loss: 3.0037\n",
      "[ 197000] D_loss: 0.3818 / G_loss: 3.0418\n",
      "[ 198000] D_loss: 0.4220 / G_loss: 2.6335\n",
      "[ 199000] D_loss: 0.4184 / G_loss: 3.1852\n",
      "[ 200000] D_loss: 0.3692 / G_loss: 3.7254\n",
      "[ 201000] D_loss: 0.3658 / G_loss: 3.1852\n",
      "[ 202000] D_loss: 0.4142 / G_loss: 3.4355\n",
      "[ 203000] D_loss: 0.5277 / G_loss: 2.9857\n",
      "[ 204000] D_loss: 0.4460 / G_loss: 3.0425\n",
      "[ 205000] D_loss: 0.5741 / G_loss: 3.1002\n",
      "[ 206000] D_loss: 0.3940 / G_loss: 3.5781\n",
      "[ 207000] D_loss: 0.2781 / G_loss: 3.3799\n",
      "[ 208000] D_loss: 0.3006 / G_loss: 3.2748\n",
      "[ 209000] D_loss: 0.4443 / G_loss: 3.4161\n",
      "[ 210000] D_loss: 0.4038 / G_loss: 2.7771\n",
      "[ 211000] D_loss: 0.3506 / G_loss: 3.3558\n",
      "[ 212000] D_loss: 0.4127 / G_loss: 2.8968\n",
      "[ 213000] D_loss: 0.4128 / G_loss: 2.5176\n",
      "[ 214000] D_loss: 0.4411 / G_loss: 2.7533\n",
      "[ 215000] D_loss: 0.5599 / G_loss: 2.9125\n",
      "[ 216000] D_loss: 0.3554 / G_loss: 2.9782\n",
      "[ 217000] D_loss: 0.4228 / G_loss: 3.1959\n",
      "[ 218000] D_loss: 0.3918 / G_loss: 3.2569\n",
      "[ 219000] D_loss: 0.3497 / G_loss: 2.9350\n",
      "[ 220000] D_loss: 0.4891 / G_loss: 3.2991\n",
      "[ 221000] D_loss: 0.3718 / G_loss: 3.0190\n",
      "[ 222000] D_loss: 0.3524 / G_loss: 2.6222\n",
      "[ 223000] D_loss: 0.3692 / G_loss: 2.8447\n",
      "[ 224000] D_loss: 0.3102 / G_loss: 3.3484\n",
      "[ 225000] D_loss: 0.3183 / G_loss: 3.2673\n",
      "[ 226000] D_loss: 0.3115 / G_loss: 2.6743\n",
      "[ 227000] D_loss: 0.4103 / G_loss: 2.9876\n",
      "[ 228000] D_loss: 0.3576 / G_loss: 3.2603\n",
      "[ 229000] D_loss: 0.4444 / G_loss: 2.7973\n",
      "[ 230000] D_loss: 0.4007 / G_loss: 3.0760\n",
      "[ 231000] D_loss: 0.3077 / G_loss: 2.8787\n",
      "[ 232000] D_loss: 0.3397 / G_loss: 3.4620\n",
      "[ 233000] D_loss: 0.3449 / G_loss: 2.8490\n",
      "[ 234000] D_loss: 0.4199 / G_loss: 3.1544\n",
      "[ 235000] D_loss: 0.4506 / G_loss: 3.1609\n",
      "[ 236000] D_loss: 0.3802 / G_loss: 3.5319\n",
      "[ 237000] D_loss: 0.4613 / G_loss: 3.1275\n",
      "[ 238000] D_loss: 0.3093 / G_loss: 2.6705\n",
      "[ 239000] D_loss: 0.3219 / G_loss: 3.5343\n",
      "[ 240000] D_loss: 0.4063 / G_loss: 2.9847\n",
      "[ 241000] D_loss: 0.2717 / G_loss: 3.0043\n",
      "[ 242000] D_loss: 0.2351 / G_loss: 3.1658\n",
      "[ 243000] D_loss: 0.3684 / G_loss: 2.9393\n",
      "[ 244000] D_loss: 0.3203 / G_loss: 3.1998\n",
      "[ 245000] D_loss: 0.3668 / G_loss: 3.3659\n",
      "[ 246000] D_loss: 0.3545 / G_loss: 3.4992\n",
      "[ 247000] D_loss: 0.2719 / G_loss: 3.1859\n",
      "[ 248000] D_loss: 0.2848 / G_loss: 3.3236\n",
      "[ 249000] D_loss: 0.4533 / G_loss: 3.3229\n",
      "[ 250000] D_loss: 0.2445 / G_loss: 3.0297\n",
      "[ 251000] D_loss: 0.3626 / G_loss: 3.1443\n",
      "[ 252000] D_loss: 0.3454 / G_loss: 2.9207\n",
      "[ 253000] D_loss: 0.5116 / G_loss: 2.9269\n",
      "[ 254000] D_loss: 0.2301 / G_loss: 2.9399\n",
      "[ 255000] D_loss: 0.3416 / G_loss: 2.8313\n",
      "[ 256000] D_loss: 0.3146 / G_loss: 3.0610\n",
      "[ 257000] D_loss: 0.4112 / G_loss: 2.7772\n",
      "[ 258000] D_loss: 0.3207 / G_loss: 3.1727\n",
      "[ 259000] D_loss: 0.3405 / G_loss: 2.9594\n",
      "[ 260000] D_loss: 0.2910 / G_loss: 3.5586\n",
      "[ 261000] D_loss: 0.3227 / G_loss: 3.1195\n",
      "[ 262000] D_loss: 0.2930 / G_loss: 2.6752\n",
      "[ 263000] D_loss: 0.2955 / G_loss: 2.8551\n",
      "[ 264000] D_loss: 0.4306 / G_loss: 2.7118\n",
      "[ 265000] D_loss: 0.3745 / G_loss: 3.4543\n",
      "[ 266000] D_loss: 0.3149 / G_loss: 3.1470\n",
      "[ 267000] D_loss: 0.2298 / G_loss: 3.3806\n",
      "[ 268000] D_loss: 0.3431 / G_loss: 2.7941\n",
      "[ 269000] D_loss: 0.4047 / G_loss: 3.2206\n",
      "[ 270000] D_loss: 0.3255 / G_loss: 3.5070\n",
      "[ 271000] D_loss: 0.2728 / G_loss: 3.6762\n",
      "[ 272000] D_loss: 0.2936 / G_loss: 3.0168\n",
      "[ 273000] D_loss: 0.2506 / G_loss: 3.4375\n",
      "[ 274000] D_loss: 0.4629 / G_loss: 2.5607\n",
      "[ 275000] D_loss: 0.2480 / G_loss: 3.3734\n",
      "[ 276000] D_loss: 0.3353 / G_loss: 2.8029\n",
      "[ 277000] D_loss: 0.2972 / G_loss: 3.5784\n",
      "[ 278000] D_loss: 0.2989 / G_loss: 2.9360\n",
      "[ 279000] D_loss: 0.4415 / G_loss: 2.8738\n",
      "[ 280000] D_loss: 0.2698 / G_loss: 2.8805\n",
      "[ 281000] D_loss: 0.4228 / G_loss: 3.4050\n",
      "[ 282000] D_loss: 0.2129 / G_loss: 3.2610\n",
      "[ 283000] D_loss: 0.2841 / G_loss: 3.2080\n",
      "[ 284000] D_loss: 0.3565 / G_loss: 2.7251\n",
      "[ 285000] D_loss: 0.3187 / G_loss: 3.5752\n",
      "[ 286000] D_loss: 0.2730 / G_loss: 3.2113\n",
      "[ 287000] D_loss: 0.4275 / G_loss: 2.8950\n",
      "[ 288000] D_loss: 0.2196 / G_loss: 3.5582\n",
      "[ 289000] D_loss: 0.3105 / G_loss: 2.9143\n",
      "[ 290000] D_loss: 0.3800 / G_loss: 2.9917\n",
      "[ 291000] D_loss: 0.3052 / G_loss: 2.7356\n",
      "[ 292000] D_loss: 0.3395 / G_loss: 2.9225\n",
      "[ 293000] D_loss: 0.2799 / G_loss: 2.8926\n",
      "[ 294000] D_loss: 0.2069 / G_loss: 3.4250\n",
      "[ 295000] D_loss: 0.4016 / G_loss: 3.3386\n",
      "[ 296000] D_loss: 0.3129 / G_loss: 2.9055\n",
      "[ 297000] D_loss: 0.3378 / G_loss: 2.7285\n",
      "[ 298000] D_loss: 0.3387 / G_loss: 3.0068\n",
      "[ 299000] D_loss: 0.2671 / G_loss: 3.5624\n",
      "[ 300000] D_loss: 0.4340 / G_loss: 3.0396\n",
      "[ 301000] D_loss: 0.3293 / G_loss: 2.8491\n",
      "[ 302000] D_loss: 0.3095 / G_loss: 3.5679\n",
      "[ 303000] D_loss: 0.2912 / G_loss: 2.9335\n",
      "[ 304000] D_loss: 0.3080 / G_loss: 2.9842\n",
      "[ 305000] D_loss: 0.4087 / G_loss: 2.6549\n",
      "[ 306000] D_loss: 0.2928 / G_loss: 3.2506\n",
      "[ 307000] D_loss: 0.3203 / G_loss: 2.8724\n",
      "[ 308000] D_loss: 0.2048 / G_loss: 3.1517\n",
      "[ 309000] D_loss: 0.3189 / G_loss: 2.9190\n",
      "[ 310000] D_loss: 0.3701 / G_loss: 3.0582\n",
      "[ 311000] D_loss: 0.4769 / G_loss: 2.6139\n",
      "[ 312000] D_loss: 0.1816 / G_loss: 3.4251\n",
      "[ 313000] D_loss: 0.3198 / G_loss: 2.9153\n",
      "[ 314000] D_loss: 0.1305 / G_loss: 3.3613\n",
      "[ 315000] D_loss: 0.2738 / G_loss: 2.7188\n",
      "[ 316000] D_loss: 0.2962 / G_loss: 3.1559\n",
      "[ 317000] D_loss: 0.2303 / G_loss: 3.3180\n",
      "[ 318000] D_loss: 0.1962 / G_loss: 3.2070\n",
      "[ 319000] D_loss: 0.4441 / G_loss: 2.7718\n",
      "[ 320000] D_loss: 0.2030 / G_loss: 3.1297\n",
      "[ 321000] D_loss: 0.2365 / G_loss: 3.0160\n",
      "[ 322000] D_loss: 0.3371 / G_loss: 3.3107\n",
      "[ 323000] D_loss: 0.3318 / G_loss: 3.0338\n",
      "[ 324000] D_loss: 0.2236 / G_loss: 2.9936\n",
      "[ 325000] D_loss: 0.3848 / G_loss: 2.9899\n",
      "[ 326000] D_loss: 0.3547 / G_loss: 2.8601\n",
      "[ 327000] D_loss: 0.1729 / G_loss: 3.1059\n",
      "[ 328000] D_loss: 0.2818 / G_loss: 3.3019\n",
      "[ 329000] D_loss: 0.2881 / G_loss: 3.2687\n",
      "[ 330000] D_loss: 0.2650 / G_loss: 3.1191\n",
      "[ 331000] D_loss: 0.3139 / G_loss: 3.2443\n",
      "[ 332000] D_loss: 0.2250 / G_loss: 3.3291\n",
      "[ 333000] D_loss: 0.2607 / G_loss: 2.7424\n",
      "[ 334000] D_loss: 0.3348 / G_loss: 2.7279\n",
      "[ 335000] D_loss: 0.2076 / G_loss: 2.8287\n",
      "[ 336000] D_loss: 0.3092 / G_loss: 3.3461\n",
      "[ 337000] D_loss: 0.2742 / G_loss: 3.2605\n",
      "[ 338000] D_loss: 0.2262 / G_loss: 3.0356\n",
      "[ 339000] D_loss: 0.3903 / G_loss: 2.8876\n",
      "[ 340000] D_loss: 0.2315 / G_loss: 3.0132\n",
      "[ 341000] D_loss: 0.1220 / G_loss: 3.3055\n",
      "[ 342000] D_loss: 0.3512 / G_loss: 2.5941\n",
      "[ 343000] D_loss: 0.2771 / G_loss: 3.1988\n",
      "[ 344000] D_loss: 0.2915 / G_loss: 3.4337\n",
      "[ 345000] D_loss: 0.1693 / G_loss: 3.1378\n",
      "[ 346000] D_loss: 0.3897 / G_loss: 2.9574\n",
      "[ 347000] D_loss: 0.3430 / G_loss: 2.7407\n",
      "[ 348000] D_loss: 0.2696 / G_loss: 2.6998\n",
      "[ 349000] D_loss: 0.3789 / G_loss: 2.6757\n",
      "[ 350000] D_loss: 0.3451 / G_loss: 3.2238\n",
      "[ 351000] D_loss: 0.2712 / G_loss: 2.7049\n",
      "[ 352000] D_loss: 0.1773 / G_loss: 3.4055\n",
      "[ 353000] D_loss: 0.2241 / G_loss: 2.7140\n",
      "[ 354000] D_loss: 0.2866 / G_loss: 3.1208\n",
      "[ 355000] D_loss: 0.2384 / G_loss: 3.1046\n",
      "[ 356000] D_loss: 0.2252 / G_loss: 2.9006\n",
      "[ 357000] D_loss: 0.1053 / G_loss: 3.3208\n",
      "[ 358000] D_loss: 0.2456 / G_loss: 3.2459\n",
      "[ 359000] D_loss: 0.2997 / G_loss: 2.8390\n",
      "[ 360000] D_loss: 0.2314 / G_loss: 2.8341\n",
      "[ 361000] D_loss: 0.2196 / G_loss: 3.3993\n",
      "[ 362000] D_loss: 0.2193 / G_loss: 3.1690\n",
      "[ 363000] D_loss: 0.3120 / G_loss: 2.8141\n",
      "[ 364000] D_loss: 0.3087 / G_loss: 3.1485\n",
      "[ 365000] D_loss: 0.1663 / G_loss: 3.0016\n",
      "[ 366000] D_loss: 0.2798 / G_loss: 3.0430\n",
      "[ 367000] D_loss: 0.2847 / G_loss: 3.1934\n",
      "[ 368000] D_loss: 0.3334 / G_loss: 2.8670\n",
      "[ 369000] D_loss: 0.2954 / G_loss: 3.2369\n",
      "[ 370000] D_loss: 0.1769 / G_loss: 3.0246\n",
      "[ 371000] D_loss: 0.2754 / G_loss: 3.0067\n",
      "[ 372000] D_loss: 0.2434 / G_loss: 3.0866\n",
      "[ 373000] D_loss: 0.2968 / G_loss: 2.8380\n",
      "[ 374000] D_loss: 0.2516 / G_loss: 3.2144\n",
      "[ 375000] D_loss: 0.3164 / G_loss: 3.3154\n",
      "[ 376000] D_loss: 0.2096 / G_loss: 2.8944\n",
      "[ 377000] D_loss: 0.2888 / G_loss: 2.6755\n",
      "[ 378000] D_loss: 0.3021 / G_loss: 3.0155\n",
      "[ 379000] D_loss: 0.2052 / G_loss: 3.0043\n",
      "[ 380000] D_loss: 0.2609 / G_loss: 3.0048\n",
      "[ 381000] D_loss: 0.3963 / G_loss: 3.0678\n",
      "[ 382000] D_loss: 0.3356 / G_loss: 3.3187\n",
      "[ 383000] D_loss: 0.2512 / G_loss: 3.3081\n",
      "[ 384000] D_loss: 0.2238 / G_loss: 2.9864\n",
      "[ 385000] D_loss: 0.2185 / G_loss: 3.1734\n",
      "[ 386000] D_loss: 0.2858 / G_loss: 3.1309\n",
      "[ 387000] D_loss: 0.3079 / G_loss: 3.4099\n",
      "[ 388000] D_loss: 0.3414 / G_loss: 2.9728\n",
      "[ 389000] D_loss: 0.2947 / G_loss: 3.0857\n",
      "[ 390000] D_loss: 0.1641 / G_loss: 3.3782\n",
      "[ 391000] D_loss: 0.1891 / G_loss: 3.3471\n",
      "[ 392000] D_loss: 0.4264 / G_loss: 3.1670\n",
      "[ 393000] D_loss: 0.2820 / G_loss: 3.2886\n",
      "[ 394000] D_loss: 0.2640 / G_loss: 3.3076\n",
      "[ 395000] D_loss: 0.2426 / G_loss: 3.4181\n",
      "[ 396000] D_loss: 0.1647 / G_loss: 2.7427\n",
      "[ 397000] D_loss: 0.1221 / G_loss: 2.9488\n",
      "[ 398000] D_loss: 0.2265 / G_loss: 3.2197\n",
      "[ 399000] D_loss: 0.1775 / G_loss: 3.1307\n",
      "[ 400000] D_loss: 0.2182 / G_loss: 2.7022\n",
      "[ 401000] D_loss: 0.3056 / G_loss: 3.4019\n",
      "[ 402000] D_loss: 0.3430 / G_loss: 2.8585\n",
      "[ 403000] D_loss: 0.2993 / G_loss: 3.0540\n",
      "[ 404000] D_loss: 0.2346 / G_loss: 3.0667\n",
      "[ 405000] D_loss: 0.3145 / G_loss: 3.3169\n",
      "[ 406000] D_loss: 0.3338 / G_loss: 3.0612\n",
      "[ 407000] D_loss: 0.3922 / G_loss: 3.3676\n",
      "[ 408000] D_loss: 0.1783 / G_loss: 2.9911\n",
      "[ 409000] D_loss: 0.2104 / G_loss: 3.2348\n",
      "[ 410000] D_loss: 0.3467 / G_loss: 3.5791\n",
      "[ 411000] D_loss: 0.2405 / G_loss: 2.7553\n",
      "[ 412000] D_loss: 0.3844 / G_loss: 2.9208\n",
      "[ 413000] D_loss: 0.1996 / G_loss: 3.2132\n",
      "[ 414000] D_loss: 0.2879 / G_loss: 3.2703\n",
      "[ 415000] D_loss: 0.2887 / G_loss: 3.2618\n",
      "[ 416000] D_loss: 0.3337 / G_loss: 2.9511\n",
      "[ 417000] D_loss: 0.2377 / G_loss: 3.0523\n",
      "[ 418000] D_loss: 0.2493 / G_loss: 3.0229\n",
      "[ 419000] D_loss: 0.2674 / G_loss: 3.2026\n",
      "[ 420000] D_loss: 0.0982 / G_loss: 3.4225\n",
      "[ 421000] D_loss: 0.3639 / G_loss: 2.8252\n",
      "[ 422000] D_loss: 0.2031 / G_loss: 3.0328\n",
      "[ 423000] D_loss: 0.1475 / G_loss: 3.1565\n",
      "[ 424000] D_loss: 0.2785 / G_loss: 2.9624\n",
      "[ 425000] D_loss: 0.2289 / G_loss: 2.9938\n",
      "[ 426000] D_loss: 0.2357 / G_loss: 3.2535\n",
      "[ 427000] D_loss: 0.2868 / G_loss: 2.6381\n",
      "[ 428000] D_loss: 0.3634 / G_loss: 2.8315\n",
      "[ 429000] D_loss: 0.2138 / G_loss: 2.7550\n",
      "[ 430000] D_loss: 0.1938 / G_loss: 3.0950\n",
      "[ 431000] D_loss: 0.2704 / G_loss: 2.9705\n",
      "[ 432000] D_loss: 0.2957 / G_loss: 3.2119\n",
      "[ 433000] D_loss: 0.2868 / G_loss: 3.1241\n",
      "[ 434000] D_loss: 0.3285 / G_loss: 2.8363\n",
      "[ 435000] D_loss: 0.3126 / G_loss: 3.0325\n",
      "[ 436000] D_loss: 0.3046 / G_loss: 2.6934\n",
      "[ 437000] D_loss: 0.2385 / G_loss: 3.0944\n",
      "[ 438000] D_loss: 0.2955 / G_loss: 3.2099\n",
      "[ 439000] D_loss: 0.1126 / G_loss: 3.0628\n",
      "[ 440000] D_loss: 0.2457 / G_loss: 2.7178\n",
      "[ 441000] D_loss: 0.3767 / G_loss: 3.4747\n",
      "[ 442000] D_loss: 0.1796 / G_loss: 2.7294\n",
      "[ 443000] D_loss: 0.3806 / G_loss: 3.4298\n",
      "[ 444000] D_loss: 0.2560 / G_loss: 3.1254\n",
      "[ 445000] D_loss: 0.1698 / G_loss: 3.1423\n",
      "[ 446000] D_loss: 0.2436 / G_loss: 3.0381\n",
      "[ 447000] D_loss: 0.2042 / G_loss: 2.8971\n",
      "[ 448000] D_loss: 0.3778 / G_loss: 2.8637\n",
      "[ 449000] D_loss: 0.2691 / G_loss: 3.2919\n",
      "[ 450000] D_loss: 0.3449 / G_loss: 2.7421\n",
      "[ 451000] D_loss: 0.2246 / G_loss: 2.6183\n",
      "[ 452000] D_loss: 0.2966 / G_loss: 3.1816\n",
      "[ 453000] D_loss: 0.2354 / G_loss: 3.0473\n",
      "[ 454000] D_loss: 0.2317 / G_loss: 3.1892\n",
      "[ 455000] D_loss: 0.3144 / G_loss: 3.3560\n",
      "[ 456000] D_loss: 0.2037 / G_loss: 3.3907\n",
      "[ 457000] D_loss: 0.3030 / G_loss: 3.1378\n",
      "[ 458000] D_loss: 0.1861 / G_loss: 3.0581\n",
      "[ 459000] D_loss: 0.3051 / G_loss: 2.9835\n",
      "[ 460000] D_loss: 0.2843 / G_loss: 2.9598\n",
      "[ 461000] D_loss: 0.2488 / G_loss: 3.1725\n",
      "[ 462000] D_loss: 0.2700 / G_loss: 3.0446\n",
      "[ 463000] D_loss: 0.1914 / G_loss: 3.1825\n",
      "[ 464000] D_loss: 0.2379 / G_loss: 2.8480\n",
      "[ 465000] D_loss: 0.2402 / G_loss: 2.9459\n",
      "[ 466000] D_loss: 0.3012 / G_loss: 3.0299\n",
      "[ 467000] D_loss: 0.1383 / G_loss: 3.0187\n",
      "[ 468000] D_loss: 0.2911 / G_loss: 3.0712\n",
      "[ 469000] D_loss: 0.2060 / G_loss: 3.1816\n",
      "[ 470000] D_loss: 0.3760 / G_loss: 3.0147\n",
      "[ 471000] D_loss: 0.2137 / G_loss: 3.0027\n",
      "[ 472000] D_loss: 0.2049 / G_loss: 3.2207\n",
      "[ 473000] D_loss: 0.2085 / G_loss: 3.1467\n",
      "[ 474000] D_loss: 0.4131 / G_loss: 3.2376\n",
      "[ 475000] D_loss: 0.2470 / G_loss: 3.2048\n",
      "[ 476000] D_loss: 0.1222 / G_loss: 3.6171\n",
      "[ 477000] D_loss: 0.2612 / G_loss: 3.3372\n",
      "[ 478000] D_loss: 0.3249 / G_loss: 3.2812\n",
      "[ 479000] D_loss: 0.2224 / G_loss: 3.2273\n",
      "[ 480000] D_loss: 0.1703 / G_loss: 3.1682\n",
      "[ 481000] D_loss: 0.3588 / G_loss: 3.2696\n",
      "[ 482000] D_loss: 0.3632 / G_loss: 3.1186\n",
      "[ 483000] D_loss: 0.2988 / G_loss: 3.0780\n",
      "[ 484000] D_loss: 0.1730 / G_loss: 3.2483\n",
      "[ 485000] D_loss: 0.2521 / G_loss: 3.3150\n",
      "[ 486000] D_loss: 0.2615 / G_loss: 3.2549\n",
      "[ 487000] D_loss: 0.2387 / G_loss: 2.8928\n",
      "[ 488000] D_loss: 0.3847 / G_loss: 3.0482\n",
      "[ 489000] D_loss: 0.2973 / G_loss: 3.0323\n",
      "[ 490000] D_loss: 0.2535 / G_loss: 3.1257\n",
      "[ 491000] D_loss: 0.2409 / G_loss: 2.9935\n",
      "[ 492000] D_loss: 0.3543 / G_loss: 3.0118\n",
      "[ 493000] D_loss: 0.2278 / G_loss: 3.4779\n",
      "[ 494000] D_loss: 0.2830 / G_loss: 3.4309\n",
      "[ 495000] D_loss: 0.2957 / G_loss: 3.1110\n",
      "[ 496000] D_loss: 0.3544 / G_loss: 3.3654\n",
      "[ 497000] D_loss: 0.1393 / G_loss: 3.1829\n",
      "[ 498000] D_loss: 0.3150 / G_loss: 3.7116\n",
      "[ 499000] D_loss: 0.2409 / G_loss: 3.4181\n",
      "[ 500000] D_loss: 0.3280 / G_loss: 3.2688\n",
      "[ 501000] D_loss: 0.1623 / G_loss: 3.3795\n",
      "[ 502000] D_loss: 0.1792 / G_loss: 3.1586\n",
      "[ 503000] D_loss: 0.1914 / G_loss: 3.0793\n",
      "[ 504000] D_loss: 0.2395 / G_loss: 3.1070\n",
      "[ 505000] D_loss: 0.1819 / G_loss: 3.5861\n",
      "[ 506000] D_loss: 0.3312 / G_loss: 3.4095\n",
      "[ 507000] D_loss: 0.2006 / G_loss: 3.0557\n",
      "[ 508000] D_loss: 0.1736 / G_loss: 3.1903\n",
      "[ 509000] D_loss: 0.1676 / G_loss: 3.1503\n",
      "[ 510000] D_loss: 0.3186 / G_loss: 3.1160\n",
      "[ 511000] D_loss: 0.2736 / G_loss: 3.4654\n",
      "[ 512000] D_loss: 0.2565 / G_loss: 3.4850\n",
      "[ 513000] D_loss: 0.1670 / G_loss: 3.2184\n",
      "[ 514000] D_loss: 0.1840 / G_loss: 3.0649\n",
      "[ 515000] D_loss: 0.2196 / G_loss: 3.1702\n",
      "[ 516000] D_loss: 0.1277 / G_loss: 3.7196\n",
      "[ 517000] D_loss: 0.1778 / G_loss: 3.1585\n",
      "[ 518000] D_loss: 0.2039 / G_loss: 2.9561\n",
      "[ 519000] D_loss: 0.1633 / G_loss: 3.3244\n",
      "[ 520000] D_loss: 0.2613 / G_loss: 3.0810\n",
      "[ 521000] D_loss: 0.2829 / G_loss: 2.8520\n",
      "[ 522000] D_loss: 0.1851 / G_loss: 2.9565\n",
      "[ 523000] D_loss: 0.2179 / G_loss: 3.1005\n",
      "[ 524000] D_loss: 0.1942 / G_loss: 3.1376\n",
      "[ 525000] D_loss: 0.2615 / G_loss: 3.2905\n",
      "[ 526000] D_loss: 0.2131 / G_loss: 3.0769\n",
      "[ 527000] D_loss: 0.1578 / G_loss: 3.1774\n",
      "[ 528000] D_loss: 0.1860 / G_loss: 3.4398\n",
      "[ 529000] D_loss: 0.2899 / G_loss: 3.1227\n",
      "[ 530000] D_loss: 0.3113 / G_loss: 3.1251\n",
      "[ 531000] D_loss: 0.1863 / G_loss: 3.3375\n",
      "[ 532000] D_loss: 0.2606 / G_loss: 3.4509\n",
      "[ 533000] D_loss: 0.2285 / G_loss: 3.4306\n",
      "[ 534000] D_loss: 0.1199 / G_loss: 3.6963\n",
      "[ 535000] D_loss: 0.2401 / G_loss: 3.5925\n",
      "[ 536000] D_loss: 0.2774 / G_loss: 3.2299\n",
      "[ 537000] D_loss: 0.2779 / G_loss: 3.4972\n",
      "[ 538000] D_loss: 0.2190 / G_loss: 2.8830\n",
      "[ 539000] D_loss: 0.2151 / G_loss: 3.5492\n",
      "[ 540000] D_loss: 0.2406 / G_loss: 2.9375\n",
      "[ 541000] D_loss: 0.3474 / G_loss: 3.1286\n",
      "[ 542000] D_loss: 0.1754 / G_loss: 3.3264\n",
      "[ 543000] D_loss: 0.1278 / G_loss: 3.3318\n",
      "[ 544000] D_loss: 0.1432 / G_loss: 3.3601\n",
      "[ 545000] D_loss: 0.2943 / G_loss: 3.7792\n",
      "[ 546000] D_loss: 0.2093 / G_loss: 3.1913\n",
      "[ 547000] D_loss: 0.2734 / G_loss: 3.3450\n",
      "[ 548000] D_loss: 0.2252 / G_loss: 3.0946\n",
      "[ 549000] D_loss: 0.2603 / G_loss: 3.1970\n",
      "[ 550000] D_loss: 0.2002 / G_loss: 3.1800\n",
      "[ 551000] D_loss: 0.1632 / G_loss: 3.1372\n",
      "[ 552000] D_loss: 0.2521 / G_loss: 3.2622\n",
      "[ 553000] D_loss: 0.2726 / G_loss: 3.3871\n",
      "[ 554000] D_loss: 0.2014 / G_loss: 3.8876\n",
      "[ 555000] D_loss: 0.2571 / G_loss: 3.0070\n",
      "[ 556000] D_loss: 0.1574 / G_loss: 3.4358\n",
      "[ 557000] D_loss: 0.2615 / G_loss: 3.2170\n",
      "[ 558000] D_loss: 0.1903 / G_loss: 3.1775\n",
      "[ 559000] D_loss: 0.2137 / G_loss: 3.6093\n",
      "[ 560000] D_loss: 0.2626 / G_loss: 3.1094\n",
      "[ 561000] D_loss: 0.2716 / G_loss: 3.5020\n",
      "[ 562000] D_loss: 0.1653 / G_loss: 3.2863\n",
      "[ 563000] D_loss: 0.2371 / G_loss: 3.4482\n",
      "[ 564000] D_loss: 0.2819 / G_loss: 3.4116\n",
      "[ 565000] D_loss: 0.2927 / G_loss: 2.9797\n",
      "[ 566000] D_loss: 0.2373 / G_loss: 3.4616\n",
      "[ 567000] D_loss: 0.2093 / G_loss: 3.6558\n",
      "[ 568000] D_loss: 0.1851 / G_loss: 3.6405\n",
      "[ 569000] D_loss: 0.2707 / G_loss: 3.5613\n",
      "[ 570000] D_loss: 0.1815 / G_loss: 3.0609\n",
      "[ 571000] D_loss: 0.4104 / G_loss: 3.3259\n",
      "[ 572000] D_loss: 0.2129 / G_loss: 3.3895\n",
      "[ 573000] D_loss: 0.2995 / G_loss: 3.8016\n",
      "[ 574000] D_loss: 0.1948 / G_loss: 3.4246\n",
      "[ 575000] D_loss: 0.1642 / G_loss: 3.3481\n",
      "[ 576000] D_loss: 0.2466 / G_loss: 3.3354\n",
      "[ 577000] D_loss: 0.2947 / G_loss: 3.4360\n",
      "[ 578000] D_loss: 0.2711 / G_loss: 2.9728\n",
      "[ 579000] D_loss: 0.2502 / G_loss: 3.2164\n",
      "[ 580000] D_loss: 0.3217 / G_loss: 3.0474\n",
      "[ 581000] D_loss: 0.1974 / G_loss: 3.0746\n",
      "[ 582000] D_loss: 0.2647 / G_loss: 3.6638\n",
      "[ 583000] D_loss: 0.2259 / G_loss: 3.0941\n",
      "[ 584000] D_loss: 0.1671 / G_loss: 3.1583\n",
      "[ 585000] D_loss: 0.2179 / G_loss: 3.3026\n",
      "[ 586000] D_loss: 0.2926 / G_loss: 3.5973\n",
      "[ 587000] D_loss: 0.2260 / G_loss: 3.4190\n",
      "[ 588000] D_loss: 0.1239 / G_loss: 3.2538\n",
      "[ 589000] D_loss: 0.1593 / G_loss: 3.8279\n",
      "[ 590000] D_loss: 0.2271 / G_loss: 3.6902\n",
      "[ 591000] D_loss: 0.1853 / G_loss: 3.6919\n",
      "[ 592000] D_loss: 0.2362 / G_loss: 3.3451\n",
      "[ 593000] D_loss: 0.1762 / G_loss: 3.8365\n",
      "[ 594000] D_loss: 0.2631 / G_loss: 3.6113\n",
      "[ 595000] D_loss: 0.2472 / G_loss: 2.9701\n",
      "[ 596000] D_loss: 0.1240 / G_loss: 3.0940\n",
      "[ 597000] D_loss: 0.1693 / G_loss: 3.1544\n",
      "[ 598000] D_loss: 0.2288 / G_loss: 3.7403\n",
      "[ 599000] D_loss: 0.3395 / G_loss: 3.5426\n",
      "[ 600000] D_loss: 0.2124 / G_loss: 3.2434\n",
      "[ 601000] D_loss: 0.2010 / G_loss: 3.2847\n",
      "[ 602000] D_loss: 0.1678 / G_loss: 3.6727\n",
      "[ 603000] D_loss: 0.1977 / G_loss: 3.8606\n",
      "[ 604000] D_loss: 0.1574 / G_loss: 3.3994\n",
      "[ 605000] D_loss: 0.2896 / G_loss: 3.2403\n",
      "[ 606000] D_loss: 0.3017 / G_loss: 3.4141\n",
      "[ 607000] D_loss: 0.1282 / G_loss: 3.0758\n",
      "[ 608000] D_loss: 0.1953 / G_loss: 3.3822\n",
      "[ 609000] D_loss: 0.2302 / G_loss: 3.6632\n",
      "[ 610000] D_loss: 0.2793 / G_loss: 3.7016\n",
      "[ 611000] D_loss: 0.1861 / G_loss: 3.3846\n",
      "[ 612000] D_loss: 0.2005 / G_loss: 3.0837\n",
      "[ 613000] D_loss: 0.1610 / G_loss: 4.2152\n",
      "[ 614000] D_loss: 0.2196 / G_loss: 3.4143\n",
      "[ 615000] D_loss: 0.1245 / G_loss: 3.1570\n",
      "[ 616000] D_loss: 0.1631 / G_loss: 3.5964\n",
      "[ 617000] D_loss: 0.3819 / G_loss: 3.3149\n",
      "[ 618000] D_loss: 0.2207 / G_loss: 3.3791\n",
      "[ 619000] D_loss: 0.1669 / G_loss: 2.8788\n",
      "[ 620000] D_loss: 0.2079 / G_loss: 3.2081\n",
      "[ 621000] D_loss: 0.1838 / G_loss: 2.9746\n",
      "[ 622000] D_loss: 0.2380 / G_loss: 3.4220\n",
      "[ 623000] D_loss: 0.2061 / G_loss: 3.2086\n",
      "[ 624000] D_loss: 0.1671 / G_loss: 3.1582\n",
      "[ 625000] D_loss: 0.2583 / G_loss: 3.2408\n",
      "[ 626000] D_loss: 0.2232 / G_loss: 3.2745\n",
      "[ 627000] D_loss: 0.1270 / G_loss: 3.3918\n",
      "[ 628000] D_loss: 0.3188 / G_loss: 3.8460\n",
      "[ 629000] D_loss: 0.2993 / G_loss: 3.9355\n",
      "[ 630000] D_loss: 0.1858 / G_loss: 3.5056\n",
      "[ 631000] D_loss: 0.1839 / G_loss: 3.1836\n",
      "[ 632000] D_loss: 0.2728 / G_loss: 3.0870\n",
      "[ 633000] D_loss: 0.2208 / G_loss: 3.6618\n",
      "[ 634000] D_loss: 0.1819 / G_loss: 3.3892\n",
      "[ 635000] D_loss: 0.0901 / G_loss: 3.2231\n",
      "[ 636000] D_loss: 0.2217 / G_loss: 3.5812\n",
      "[ 637000] D_loss: 0.2195 / G_loss: 2.9868\n",
      "[ 638000] D_loss: 0.2178 / G_loss: 3.5196\n",
      "[ 639000] D_loss: 0.2216 / G_loss: 3.1908\n",
      "[ 640000] D_loss: 0.1732 / G_loss: 3.1613\n",
      "[ 641000] D_loss: 0.2357 / G_loss: 3.1895\n",
      "[ 642000] D_loss: 0.2170 / G_loss: 3.5478\n",
      "[ 643000] D_loss: 0.1465 / G_loss: 3.1320\n",
      "[ 644000] D_loss: 0.1944 / G_loss: 3.6366\n",
      "[ 645000] D_loss: 0.0929 / G_loss: 3.4834\n",
      "[ 646000] D_loss: 0.0946 / G_loss: 3.5986\n",
      "[ 647000] D_loss: 0.2693 / G_loss: 3.2607\n",
      "[ 648000] D_loss: 0.1270 / G_loss: 3.6090\n",
      "[ 649000] D_loss: 0.1199 / G_loss: 3.5506\n",
      "[ 650000] D_loss: 0.2634 / G_loss: 3.1138\n",
      "[ 651000] D_loss: 0.1564 / G_loss: 3.0176\n",
      "[ 652000] D_loss: 0.1743 / G_loss: 3.9662\n",
      "[ 653000] D_loss: 0.1687 / G_loss: 3.4052\n",
      "[ 654000] D_loss: 0.1752 / G_loss: 3.2939\n",
      "[ 655000] D_loss: 0.2054 / G_loss: 3.5147\n",
      "[ 656000] D_loss: 0.0746 / G_loss: 3.3728\n",
      "[ 657000] D_loss: 0.2460 / G_loss: 3.2579\n",
      "[ 658000] D_loss: 0.2035 / G_loss: 3.8357\n",
      "[ 659000] D_loss: 0.2302 / G_loss: 3.1690\n",
      "[ 660000] D_loss: 0.2419 / G_loss: 3.5172\n",
      "[ 661000] D_loss: 0.2567 / G_loss: 4.1011\n",
      "[ 662000] D_loss: 0.1964 / G_loss: 3.4608\n",
      "[ 663000] D_loss: 0.2742 / G_loss: 3.6412\n",
      "[ 664000] D_loss: 0.1713 / G_loss: 3.3748\n",
      "[ 665000] D_loss: 0.0895 / G_loss: 3.7411\n",
      "[ 666000] D_loss: 0.2486 / G_loss: 3.5336\n",
      "[ 667000] D_loss: 0.1829 / G_loss: 3.2838\n",
      "[ 668000] D_loss: 0.1792 / G_loss: 3.4460\n",
      "[ 669000] D_loss: 0.1190 / G_loss: 3.7305\n",
      "[ 670000] D_loss: 0.1751 / G_loss: 3.4938\n",
      "[ 671000] D_loss: 0.1180 / G_loss: 3.5889\n",
      "[ 672000] D_loss: 0.2116 / G_loss: 3.3654\n",
      "[ 673000] D_loss: 0.2086 / G_loss: 3.6383\n",
      "[ 674000] D_loss: 0.2173 / G_loss: 4.2592\n",
      "[ 675000] D_loss: 0.2675 / G_loss: 3.2166\n",
      "[ 676000] D_loss: 0.2422 / G_loss: 3.0553\n",
      "[ 677000] D_loss: 0.2420 / G_loss: 3.2609\n",
      "[ 678000] D_loss: 0.0836 / G_loss: 3.5016\n",
      "[ 679000] D_loss: 0.2840 / G_loss: 3.1573\n",
      "[ 680000] D_loss: 0.1040 / G_loss: 3.2133\n",
      "[ 681000] D_loss: 0.1850 / G_loss: 3.5260\n",
      "[ 682000] D_loss: 0.2591 / G_loss: 3.3294\n",
      "[ 683000] D_loss: 0.2350 / G_loss: 3.3383\n",
      "[ 684000] D_loss: 0.2236 / G_loss: 3.1480\n",
      "[ 685000] D_loss: 0.1314 / G_loss: 3.9177\n",
      "[ 686000] D_loss: 0.1092 / G_loss: 3.5338\n",
      "[ 687000] D_loss: 0.2466 / G_loss: 3.4121\n",
      "[ 688000] D_loss: 0.1247 / G_loss: 3.4511\n",
      "[ 689000] D_loss: 0.2253 / G_loss: 3.7185\n",
      "[ 690000] D_loss: 0.1394 / G_loss: 3.1362\n",
      "[ 691000] D_loss: 0.2128 / G_loss: 3.1780\n",
      "[ 692000] D_loss: 0.2623 / G_loss: 4.1042\n",
      "[ 693000] D_loss: 0.1676 / G_loss: 3.3904\n",
      "[ 694000] D_loss: 0.1796 / G_loss: 3.9597\n",
      "[ 695000] D_loss: 0.2016 / G_loss: 3.4768\n",
      "[ 696000] D_loss: 0.3785 / G_loss: 3.7225\n",
      "[ 697000] D_loss: 0.1587 / G_loss: 3.4227\n",
      "[ 698000] D_loss: 0.1742 / G_loss: 3.4811\n",
      "[ 699000] D_loss: 0.2850 / G_loss: 3.5775\n",
      "[ 700000] D_loss: 0.1077 / G_loss: 3.6653\n",
      "[ 701000] D_loss: 0.3010 / G_loss: 3.3283\n",
      "[ 702000] D_loss: 0.1939 / G_loss: 3.3444\n",
      "[ 703000] D_loss: 0.1202 / G_loss: 3.5557\n",
      "[ 704000] D_loss: 0.0928 / G_loss: 3.3577\n",
      "[ 705000] D_loss: 0.1234 / G_loss: 3.6772\n",
      "[ 706000] D_loss: 0.1181 / G_loss: 3.3703\n",
      "[ 707000] D_loss: 0.1971 / G_loss: 3.6219\n",
      "[ 708000] D_loss: 0.2793 / G_loss: 3.4296\n",
      "[ 709000] D_loss: 0.1303 / G_loss: 3.7060\n",
      "[ 710000] D_loss: 0.2308 / G_loss: 3.5955\n",
      "[ 711000] D_loss: 0.1591 / G_loss: 3.2479\n",
      "[ 712000] D_loss: 0.2275 / G_loss: 3.1137\n",
      "[ 713000] D_loss: 0.2101 / G_loss: 3.7386\n",
      "[ 714000] D_loss: 0.1171 / G_loss: 3.5094\n",
      "[ 715000] D_loss: 0.1786 / G_loss: 3.4980\n",
      "[ 716000] D_loss: 0.1681 / G_loss: 3.3646\n",
      "[ 717000] D_loss: 0.1413 / G_loss: 3.2491\n",
      "[ 718000] D_loss: 0.2221 / G_loss: 3.7728\n",
      "[ 719000] D_loss: 0.1538 / G_loss: 3.5796\n",
      "[ 720000] D_loss: 0.2159 / G_loss: 3.7230\n",
      "[ 721000] D_loss: 0.0979 / G_loss: 4.0533\n",
      "[ 722000] D_loss: 0.2263 / G_loss: 3.4196\n",
      "[ 723000] D_loss: 0.2333 / G_loss: 3.4253\n",
      "[ 724000] D_loss: 0.1604 / G_loss: 3.4531\n",
      "[ 725000] D_loss: 0.2679 / G_loss: 3.6328\n",
      "[ 726000] D_loss: 0.2063 / G_loss: 3.6534\n",
      "[ 727000] D_loss: 0.1007 / G_loss: 3.7224\n",
      "[ 728000] D_loss: 0.0969 / G_loss: 4.1112\n",
      "[ 729000] D_loss: 0.1376 / G_loss: 3.4131\n",
      "[ 730000] D_loss: 0.2238 / G_loss: 3.6443\n",
      "[ 731000] D_loss: 0.1752 / G_loss: 3.6514\n",
      "[ 732000] D_loss: 0.1651 / G_loss: 3.1941\n",
      "[ 733000] D_loss: 0.2320 / G_loss: 3.1485\n",
      "[ 734000] D_loss: 0.1717 / G_loss: 3.1873\n",
      "[ 735000] D_loss: 0.2768 / G_loss: 3.4100\n",
      "[ 736000] D_loss: 0.2072 / G_loss: 3.3935\n",
      "[ 737000] D_loss: 0.2087 / G_loss: 3.1501\n",
      "[ 738000] D_loss: 0.2631 / G_loss: 3.5303\n",
      "[ 739000] D_loss: 0.3635 / G_loss: 3.7811\n",
      "[ 740000] D_loss: 0.1513 / G_loss: 3.9597\n",
      "[ 741000] D_loss: 0.1697 / G_loss: 3.2866\n",
      "[ 742000] D_loss: 0.0959 / G_loss: 3.7750\n",
      "[ 743000] D_loss: 0.1914 / G_loss: 3.6579\n",
      "[ 744000] D_loss: 0.2005 / G_loss: 3.6594\n",
      "[ 745000] D_loss: 0.1139 / G_loss: 3.4913\n",
      "[ 746000] D_loss: 0.1501 / G_loss: 3.7296\n",
      "[ 747000] D_loss: 0.1998 / G_loss: 3.4817\n",
      "[ 748000] D_loss: 0.2235 / G_loss: 3.4340\n",
      "[ 749000] D_loss: 0.1863 / G_loss: 3.5860\n",
      "[ 750000] D_loss: 0.2298 / G_loss: 3.5258\n",
      "[ 751000] D_loss: 0.1647 / G_loss: 3.5450\n",
      "[ 752000] D_loss: 0.2140 / G_loss: 3.7029\n",
      "[ 753000] D_loss: 0.2594 / G_loss: 4.1086\n",
      "[ 754000] D_loss: 0.0966 / G_loss: 3.4610\n",
      "[ 755000] D_loss: 0.1553 / G_loss: 3.5948\n",
      "[ 756000] D_loss: 0.1371 / G_loss: 3.8619\n",
      "[ 757000] D_loss: 0.3299 / G_loss: 4.3391\n",
      "[ 758000] D_loss: 0.2225 / G_loss: 3.7009\n",
      "[ 759000] D_loss: 0.3281 / G_loss: 3.1860\n",
      "[ 760000] D_loss: 0.1641 / G_loss: 3.8143\n",
      "[ 761000] D_loss: 0.1604 / G_loss: 3.4111\n",
      "[ 762000] D_loss: 0.0913 / G_loss: 3.6614\n",
      "[ 763000] D_loss: 0.1134 / G_loss: 3.1611\n",
      "[ 764000] D_loss: 0.1917 / G_loss: 3.3702\n",
      "[ 765000] D_loss: 0.2030 / G_loss: 3.5667\n",
      "[ 766000] D_loss: 0.1571 / G_loss: 3.5013\n",
      "[ 767000] D_loss: 0.2695 / G_loss: 3.3958\n",
      "[ 768000] D_loss: 0.2562 / G_loss: 3.4409\n",
      "[ 769000] D_loss: 0.1227 / G_loss: 3.7590\n",
      "[ 770000] D_loss: 0.2187 / G_loss: 3.5783\n",
      "[ 771000] D_loss: 0.1827 / G_loss: 3.4803\n",
      "[ 772000] D_loss: 0.2172 / G_loss: 3.6000\n",
      "[ 773000] D_loss: 0.2127 / G_loss: 3.6269\n",
      "[ 774000] D_loss: 0.3211 / G_loss: 4.1022\n",
      "[ 775000] D_loss: 0.2084 / G_loss: 3.9256\n",
      "[ 776000] D_loss: 0.2514 / G_loss: 3.6025\n",
      "[ 777000] D_loss: 0.2091 / G_loss: 3.7751\n",
      "[ 778000] D_loss: 0.1963 / G_loss: 3.8618\n",
      "[ 779000] D_loss: 0.1825 / G_loss: 3.3242\n",
      "[ 780000] D_loss: 0.2964 / G_loss: 3.6296\n",
      "[ 781000] D_loss: 0.1826 / G_loss: 3.5113\n",
      "[ 782000] D_loss: 0.2675 / G_loss: 3.3336\n",
      "[ 783000] D_loss: 0.1227 / G_loss: 3.4518\n",
      "[ 784000] D_loss: 0.2167 / G_loss: 3.4930\n",
      "[ 785000] D_loss: 0.0508 / G_loss: 3.5545\n",
      "[ 786000] D_loss: 0.1770 / G_loss: 3.6502\n",
      "[ 787000] D_loss: 0.2538 / G_loss: 3.6756\n",
      "[ 788000] D_loss: 0.1353 / G_loss: 3.4738\n",
      "[ 789000] D_loss: 0.2018 / G_loss: 3.5295\n",
      "[ 790000] D_loss: 0.1123 / G_loss: 3.2491\n",
      "[ 791000] D_loss: 0.0993 / G_loss: 3.8137\n",
      "[ 792000] D_loss: 0.1419 / G_loss: 3.8258\n",
      "[ 793000] D_loss: 0.1414 / G_loss: 3.1746\n",
      "[ 794000] D_loss: 0.2862 / G_loss: 3.3943\n",
      "[ 795000] D_loss: 0.1609 / G_loss: 3.8073\n",
      "[ 796000] D_loss: 0.2041 / G_loss: 3.1551\n",
      "[ 797000] D_loss: 0.1032 / G_loss: 3.5039\n",
      "[ 798000] D_loss: 0.0879 / G_loss: 3.7055\n",
      "[ 799000] D_loss: 0.2168 / G_loss: 3.1532\n",
      "[ 800000] D_loss: 0.2148 / G_loss: 3.5300\n",
      "[ 801000] D_loss: 0.2615 / G_loss: 3.6150\n",
      "[ 802000] D_loss: 0.2012 / G_loss: 3.4997\n",
      "[ 803000] D_loss: 0.2087 / G_loss: 3.5335\n",
      "[ 804000] D_loss: 0.1028 / G_loss: 3.8028\n",
      "[ 805000] D_loss: 0.3414 / G_loss: 3.8369\n",
      "[ 806000] D_loss: 0.1497 / G_loss: 3.1931\n",
      "[ 807000] D_loss: 0.1267 / G_loss: 3.8969\n",
      "[ 808000] D_loss: 0.2052 / G_loss: 3.8868\n",
      "[ 809000] D_loss: 0.1294 / G_loss: 3.6888\n",
      "[ 810000] D_loss: 0.2557 / G_loss: 3.9296\n",
      "[ 811000] D_loss: 0.1569 / G_loss: 3.5076\n",
      "[ 812000] D_loss: 0.0892 / G_loss: 3.8234\n",
      "[ 813000] D_loss: 0.1901 / G_loss: 3.3272\n",
      "[ 814000] D_loss: 0.2238 / G_loss: 3.3690\n",
      "[ 815000] D_loss: 0.1597 / G_loss: 2.9746\n",
      "[ 816000] D_loss: 0.1734 / G_loss: 3.9524\n",
      "[ 817000] D_loss: 0.1674 / G_loss: 4.1051\n",
      "[ 818000] D_loss: 0.2108 / G_loss: 4.0569\n",
      "[ 819000] D_loss: 0.1129 / G_loss: 3.3438\n",
      "[ 820000] D_loss: 0.2276 / G_loss: 3.5038\n",
      "[ 821000] D_loss: 0.1707 / G_loss: 3.3782\n",
      "[ 822000] D_loss: 0.1528 / G_loss: 3.4793\n",
      "[ 823000] D_loss: 0.0995 / G_loss: 3.6451\n",
      "[ 824000] D_loss: 0.1311 / G_loss: 3.8959\n",
      "[ 825000] D_loss: 0.2045 / G_loss: 3.5427\n",
      "[ 826000] D_loss: 0.1562 / G_loss: 3.5863\n",
      "[ 827000] D_loss: 0.2195 / G_loss: 3.5202\n",
      "[ 828000] D_loss: 0.1526 / G_loss: 3.9964\n",
      "[ 829000] D_loss: 0.1003 / G_loss: 3.4760\n",
      "[ 830000] D_loss: 0.1254 / G_loss: 3.5448\n",
      "[ 831000] D_loss: 0.1722 / G_loss: 3.5572\n",
      "[ 832000] D_loss: 0.1479 / G_loss: 3.2493\n",
      "[ 833000] D_loss: 0.1983 / G_loss: 3.6310\n",
      "[ 834000] D_loss: 0.1152 / G_loss: 3.4450\n",
      "[ 835000] D_loss: 0.1557 / G_loss: 3.5983\n",
      "[ 836000] D_loss: 0.1885 / G_loss: 3.6194\n",
      "[ 837000] D_loss: 0.2754 / G_loss: 3.2822\n",
      "[ 838000] D_loss: 0.2355 / G_loss: 3.3511\n",
      "[ 839000] D_loss: 0.1389 / G_loss: 3.6599\n",
      "[ 840000] D_loss: 0.2267 / G_loss: 3.7970\n",
      "[ 841000] D_loss: 0.1900 / G_loss: 3.9243\n",
      "[ 842000] D_loss: 0.2717 / G_loss: 3.5952\n",
      "[ 843000] D_loss: 0.1252 / G_loss: 3.5755\n",
      "[ 844000] D_loss: 0.3123 / G_loss: 3.3888\n",
      "[ 845000] D_loss: 0.0631 / G_loss: 3.7689\n",
      "[ 846000] D_loss: 0.2158 / G_loss: 3.6384\n",
      "[ 847000] D_loss: 0.0875 / G_loss: 3.7436\n",
      "[ 848000] D_loss: 0.1888 / G_loss: 3.5842\n",
      "[ 849000] D_loss: 0.1039 / G_loss: 3.5968\n",
      "[ 850000] D_loss: 0.0448 / G_loss: 3.7688\n",
      "[ 851000] D_loss: 0.1486 / G_loss: 3.6709\n",
      "[ 852000] D_loss: 0.1607 / G_loss: 3.4487\n",
      "[ 853000] D_loss: 0.1368 / G_loss: 3.9010\n",
      "[ 854000] D_loss: 0.0849 / G_loss: 3.5018\n",
      "[ 855000] D_loss: 0.1456 / G_loss: 3.5505\n",
      "[ 856000] D_loss: 0.1015 / G_loss: 3.5562\n",
      "[ 857000] D_loss: 0.2057 / G_loss: 3.2512\n",
      "[ 858000] D_loss: 0.1732 / G_loss: 3.5184\n",
      "[ 859000] D_loss: 0.0805 / G_loss: 3.6256\n",
      "[ 860000] D_loss: 0.3476 / G_loss: 3.8097\n",
      "[ 861000] D_loss: 0.2579 / G_loss: 3.8854\n",
      "[ 862000] D_loss: 0.2396 / G_loss: 4.0291\n",
      "[ 863000] D_loss: 0.1711 / G_loss: 3.6242\n",
      "[ 864000] D_loss: 0.1489 / G_loss: 3.6364\n",
      "[ 865000] D_loss: 0.1792 / G_loss: 3.6078\n",
      "[ 866000] D_loss: 0.1280 / G_loss: 3.6577\n",
      "[ 867000] D_loss: 0.1303 / G_loss: 3.9590\n",
      "[ 868000] D_loss: 0.2039 / G_loss: 3.5381\n",
      "[ 869000] D_loss: 0.1073 / G_loss: 3.3809\n",
      "[ 870000] D_loss: 0.1585 / G_loss: 3.1716\n",
      "[ 871000] D_loss: 0.2486 / G_loss: 4.0634\n",
      "[ 872000] D_loss: 0.4027 / G_loss: 3.7902\n",
      "[ 873000] D_loss: 0.1199 / G_loss: 4.1594\n",
      "[ 874000] D_loss: 0.1268 / G_loss: 3.7535\n",
      "[ 875000] D_loss: 0.3026 / G_loss: 3.6244\n",
      "[ 876000] D_loss: 0.2063 / G_loss: 3.3068\n",
      "[ 877000] D_loss: 0.0983 / G_loss: 3.4938\n",
      "[ 878000] D_loss: 0.2092 / G_loss: 3.3587\n",
      "[ 879000] D_loss: 0.1350 / G_loss: 3.9367\n",
      "[ 880000] D_loss: 0.1529 / G_loss: 3.5629\n",
      "[ 881000] D_loss: 0.1278 / G_loss: 3.3585\n",
      "[ 882000] D_loss: 0.1681 / G_loss: 3.4392\n",
      "[ 883000] D_loss: 0.2843 / G_loss: 3.6470\n",
      "[ 884000] D_loss: 0.0731 / G_loss: 3.7000\n",
      "[ 885000] D_loss: 0.1975 / G_loss: 4.0703\n",
      "[ 886000] D_loss: 0.1362 / G_loss: 3.3394\n",
      "[ 887000] D_loss: 0.1107 / G_loss: 4.0005\n",
      "[ 888000] D_loss: 0.1588 / G_loss: 3.7601\n",
      "[ 889000] D_loss: 0.2096 / G_loss: 3.9000\n",
      "[ 890000] D_loss: 0.2554 / G_loss: 3.6136\n",
      "[ 891000] D_loss: 0.0907 / G_loss: 3.6149\n",
      "[ 892000] D_loss: 0.1306 / G_loss: 3.7464\n",
      "[ 893000] D_loss: 0.1729 / G_loss: 3.3013\n",
      "[ 894000] D_loss: 0.2389 / G_loss: 3.6439\n",
      "[ 895000] D_loss: 0.0981 / G_loss: 3.5805\n",
      "[ 896000] D_loss: 0.1588 / G_loss: 3.6783\n",
      "[ 897000] D_loss: 0.2014 / G_loss: 3.6148\n",
      "[ 898000] D_loss: 0.2305 / G_loss: 3.8259\n",
      "[ 899000] D_loss: 0.1692 / G_loss: 3.3048\n",
      "[ 900000] D_loss: 0.1376 / G_loss: 3.9106\n",
      "[ 901000] D_loss: 0.3040 / G_loss: 3.9015\n",
      "[ 902000] D_loss: 0.1626 / G_loss: 3.7866\n",
      "[ 903000] D_loss: 0.2615 / G_loss: 3.7612\n",
      "[ 904000] D_loss: 0.1239 / G_loss: 3.8243\n",
      "[ 905000] D_loss: 0.1652 / G_loss: 3.6581\n",
      "[ 906000] D_loss: 0.3106 / G_loss: 3.6030\n",
      "[ 907000] D_loss: 0.1689 / G_loss: 3.8792\n",
      "[ 908000] D_loss: 0.1014 / G_loss: 3.3347\n",
      "[ 909000] D_loss: 0.1959 / G_loss: 3.3270\n",
      "[ 910000] D_loss: 0.2141 / G_loss: 4.3136\n",
      "[ 911000] D_loss: 0.1159 / G_loss: 3.7508\n",
      "[ 912000] D_loss: 0.2977 / G_loss: 3.2960\n",
      "[ 913000] D_loss: 0.0981 / G_loss: 3.9273\n",
      "[ 914000] D_loss: 0.1940 / G_loss: 4.0155\n",
      "[ 915000] D_loss: 0.0856 / G_loss: 3.8715\n",
      "[ 916000] D_loss: 0.1280 / G_loss: 4.0926\n",
      "[ 917000] D_loss: 0.3107 / G_loss: 3.7029\n",
      "[ 918000] D_loss: 0.1206 / G_loss: 3.5602\n",
      "[ 919000] D_loss: 0.1901 / G_loss: 4.5002\n",
      "[ 920000] D_loss: 0.3399 / G_loss: 3.6467\n",
      "[ 921000] D_loss: 0.0991 / G_loss: 3.7500\n",
      "[ 922000] D_loss: 0.2408 / G_loss: 3.4053\n",
      "[ 923000] D_loss: 0.2521 / G_loss: 4.2319\n",
      "[ 924000] D_loss: 0.1738 / G_loss: 3.9804\n",
      "[ 925000] D_loss: 0.2713 / G_loss: 3.6049\n",
      "[ 926000] D_loss: 0.1995 / G_loss: 3.6104\n",
      "[ 927000] D_loss: 0.1556 / G_loss: 3.9188\n",
      "[ 928000] D_loss: 0.2027 / G_loss: 3.6773\n",
      "[ 929000] D_loss: 0.1580 / G_loss: 3.9542\n",
      "[ 930000] D_loss: 0.2860 / G_loss: 3.6269\n",
      "[ 931000] D_loss: 0.2263 / G_loss: 3.8526\n",
      "[ 932000] D_loss: 0.2009 / G_loss: 3.6705\n",
      "[ 933000] D_loss: 0.1691 / G_loss: 3.8492\n",
      "[ 934000] D_loss: 0.1651 / G_loss: 4.2072\n",
      "[ 935000] D_loss: 0.2957 / G_loss: 3.7389\n",
      "[ 936000] D_loss: 0.2262 / G_loss: 3.6401\n",
      "[ 937000] D_loss: 0.2071 / G_loss: 3.6142\n",
      "[ 938000] D_loss: 0.1954 / G_loss: 3.4842\n",
      "[ 939000] D_loss: 0.1934 / G_loss: 3.7805\n",
      "[ 940000] D_loss: 0.1795 / G_loss: 3.8247\n",
      "[ 941000] D_loss: 0.1374 / G_loss: 3.9195\n",
      "[ 942000] D_loss: 0.2161 / G_loss: 3.6709\n",
      "[ 943000] D_loss: 0.1242 / G_loss: 3.5755\n",
      "[ 944000] D_loss: 0.1402 / G_loss: 3.8023\n",
      "[ 945000] D_loss: 0.1262 / G_loss: 4.4328\n",
      "[ 946000] D_loss: 0.1794 / G_loss: 4.1137\n",
      "[ 947000] D_loss: 0.1485 / G_loss: 3.6304\n",
      "[ 948000] D_loss: 0.0989 / G_loss: 3.9886\n",
      "[ 949000] D_loss: 0.1821 / G_loss: 3.5193\n",
      "[ 950000] D_loss: 0.1656 / G_loss: 4.1371\n",
      "[ 951000] D_loss: 0.1744 / G_loss: 3.6563\n",
      "[ 952000] D_loss: 0.0787 / G_loss: 3.9582\n",
      "[ 953000] D_loss: 0.1467 / G_loss: 4.3066\n",
      "[ 954000] D_loss: 0.1658 / G_loss: 3.5898\n",
      "[ 955000] D_loss: 0.1922 / G_loss: 3.3213\n",
      "[ 956000] D_loss: 0.0826 / G_loss: 4.4585\n",
      "[ 957000] D_loss: 0.1990 / G_loss: 3.7224\n",
      "[ 958000] D_loss: 0.1730 / G_loss: 3.7934\n",
      "[ 959000] D_loss: 0.1706 / G_loss: 4.0464\n",
      "[ 960000] D_loss: 0.1131 / G_loss: 4.3053\n",
      "[ 961000] D_loss: 0.1459 / G_loss: 3.9972\n",
      "[ 962000] D_loss: 0.1341 / G_loss: 3.5505\n",
      "[ 963000] D_loss: 0.2701 / G_loss: 4.5258\n",
      "[ 964000] D_loss: 0.1251 / G_loss: 3.9829\n",
      "[ 965000] D_loss: 0.1792 / G_loss: 3.7961\n",
      "[ 966000] D_loss: 0.1915 / G_loss: 3.4922\n",
      "[ 967000] D_loss: 0.0979 / G_loss: 4.1335\n",
      "[ 968000] D_loss: 0.2291 / G_loss: 3.6640\n",
      "[ 969000] D_loss: 0.1443 / G_loss: 3.7865\n",
      "[ 970000] D_loss: 0.0412 / G_loss: 3.9414\n",
      "[ 971000] D_loss: 0.2013 / G_loss: 3.6196\n",
      "[ 972000] D_loss: 0.2060 / G_loss: 3.8976\n",
      "[ 973000] D_loss: 0.1300 / G_loss: 3.8412\n",
      "[ 974000] D_loss: 0.1374 / G_loss: 3.4042\n",
      "[ 975000] D_loss: 0.1636 / G_loss: 4.2625\n",
      "[ 976000] D_loss: 0.3930 / G_loss: 3.9192\n",
      "[ 977000] D_loss: 0.2022 / G_loss: 4.2914\n",
      "[ 978000] D_loss: 0.2230 / G_loss: 3.5163\n",
      "[ 979000] D_loss: 0.4028 / G_loss: 4.1558\n",
      "[ 980000] D_loss: 0.2474 / G_loss: 3.7048\n",
      "[ 981000] D_loss: 0.1820 / G_loss: 3.6567\n",
      "[ 982000] D_loss: 0.1988 / G_loss: 3.6031\n",
      "[ 983000] D_loss: 0.1655 / G_loss: 3.8723\n",
      "[ 984000] D_loss: 0.1910 / G_loss: 3.8708\n",
      "[ 985000] D_loss: 0.2184 / G_loss: 3.5351\n",
      "[ 986000] D_loss: 0.2053 / G_loss: 3.6601\n",
      "[ 987000] D_loss: 0.1573 / G_loss: 3.6162\n",
      "[ 988000] D_loss: 0.0569 / G_loss: 3.6378\n",
      "[ 989000] D_loss: 0.1384 / G_loss: 3.6535\n",
      "[ 990000] D_loss: 0.1438 / G_loss: 4.1324\n",
      "[ 991000] D_loss: 0.1715 / G_loss: 3.8139\n",
      "[ 992000] D_loss: 0.1363 / G_loss: 3.7260\n",
      "[ 993000] D_loss: 0.1674 / G_loss: 3.9712\n",
      "[ 994000] D_loss: 0.0807 / G_loss: 3.7388\n",
      "[ 995000] D_loss: 0.1145 / G_loss: 4.0070\n",
      "[ 996000] D_loss: 0.1015 / G_loss: 4.1896\n",
      "[ 997000] D_loss: 0.0786 / G_loss: 3.6689\n",
      "[ 998000] D_loss: 0.2077 / G_loss: 4.2979\n",
      "[ 999000] D_loss: 0.2273 / G_loss: 3.5473\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "# sess.run(tf.initialize_all_variables())\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for it in range(1000000):\n",
    "    if it % 1000 == 0:\n",
    "        samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)})\n",
    "        \n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out/{:0>4d}.png'.format(idx), bbox_inches='tight')\n",
    "        idx += 1\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # _ 는 원래 Y 임.\n",
    "    X_batch, _ = mnist.train.next_batch(batch_size)\n",
    "    \n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_batch, Z: sample_Z(batch_size, Z_dim)})\n",
    "    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(batch_size, Z_dim)})\n",
    "    \n",
    "    if it % 1000 == 0:\n",
    "        print(\"[{:7d}] D_loss: {:.4f} / G_loss: {:.4f}\".format(it, D_loss_curr, G_loss_curr))\n",
    "        D_losses.append(D_loss_curr)\n",
    "        G_losses.append(G_loss_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
