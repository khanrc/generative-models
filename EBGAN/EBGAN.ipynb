{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EBGAN\n",
    "\n",
    "Zhao, Junbo, Michael Mathieu, and Yann LeCun. \"Energy-based generative adversarial network.\" arXiv preprint arXiv:1609.03126 (2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy-model\n",
    "\n",
    "* data manifold 에 가까운 지역에는 낮은 에너지를 할당하고, 다른 지역에는 높은 에너지를 할당\n",
    "* 다른 모델이 (like MLE) data manifold 를 찾는데 집중할 뿐 다른 지역에는 신경쓰지 않는것과 대조됨\n",
    "* [다만, 이 모델이 특별히 다른 모델보다 energy-based 냐 라는 비판](http://www.inference.vc/are-energy-based-gans-actually-energy-based/)도 있음 - by Ferenc\n",
    "\n",
    "$$\\begin{align}\n",
    "L_D(x,z)&=D(x)+[m-D(G(z))]^+ \\\\\n",
    "L_G(z)&=D(G(z))\n",
    "\\end{align}$$\n",
    "\n",
    "where $[\\cdot]^+ = max(0,\\cdot)$ - hinge loss (margin loss).\n",
    "\n",
    "* hinge loss 를 사용함으로써 D 는 '적당히' 만 fake sample 들에게 높은 에너지를 할당\n",
    "* G 는 그런거 없고 최대한 높은 에너지를 할당하도록 학습\n",
    "* 무조건 hinge loss 를 쓸 필요는 없고 다양한 로스가 사용가능하다고는 되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use auto-encoders as discriminator\n",
    "\n",
    "$$D(x)=\\Vert Dec(Enc(x))-x \\Vert$$\n",
    "\n",
    "![ebgan-ae](ebgan-ae.png)\n",
    "    \n",
    "* 기존의 classifier 보다 latent representation 을 학습하는 AE 를 사용함으로써 G 에게 보다 유의미한 teaching 이 가능해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repelling regularizer\n",
    "\n",
    "$$\n",
    "f_{PT}(S)={1 \\over N(N-1)} \\sum_i \\sum_{j\\neq i} \\left( \\frac{S^T_i S_j}{\\Vert S_i\\Vert \\Vert S_j\\Vert} \\right)^2\n",
    "$$\n",
    "\n",
    "S is (encoded) latent representation. i, j indicates index of data point\n",
    "\n",
    "* Pulling-away term (PT)\n",
    "* same as 'minibatch discrimination' of ImprovedGAN\n",
    "* encoded representation 들 간의 cos-sim (코사인 유사도) 를 최소화한다 => 데이터 포인트 간의 representation variance 를 높게 유지한다\n",
    "* 즉, G 가 생성하는 데이터들간의 variance 를 유지함으로써 mode collapse 를 방지함\n",
    "* 이 regularizer는 G 에만 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation - simplified\n",
    "\n",
    "* MNIST\n",
    "* use fc layers\n",
    "* but implement core things - AE and PT regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "z_dim = 64\n",
    "m = 5.\n",
    "pt_weight = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, reuse=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        net = slim.fully_connected(z, 128)\n",
    "        logits = slim.fully_connected(net, 784, activation_fn=None)\n",
    "        prob = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lf: latent features\n",
    "def pt_regularizer(lf):\n",
    "    l2_norm = tf.sqrt(tf.reduce_sum(tf.square(lf), axis=1, keep_dims=True))\n",
    "    expected_shape(l2_norm, [None, 1])\n",
    "    unit_lf = lf / l2_norm # this is unit vector?\n",
    "    cos_sim = tf.square(tf.matmul(unit_lf, unit_lf, transpose_b=True)) # [N, h_dim] x [h_dim, N] = [N, N]\n",
    "    N = tf.cast(tf.shape(lf)[0], tf.float32) # batch_size\n",
    "    pt_loss = (tf.reduce_sum(cos_sim)-N) / (N*(N-1))\n",
    "    return pt_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x, reuse=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse): # auto-encoder\n",
    "        # latent features lf\n",
    "        lf = slim.fully_connected(x, 128) # encoder\n",
    "        x_recon = slim.fully_connected(lf, 784, activation_fn=None) # decoder\n",
    "        mse = tf.losses.mean_squared_error(x, x_recon) # 데이터포인트마다 하는줄 알았는데 그냥 다합해서 하는건가봄\n",
    "    \n",
    "        return lf, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EBGAN():\n",
    "    def __init__(self, name, use_pt_regularizer=False):\n",
    "        with tf.variable_scope(name):\n",
    "            X = tf.placeholder(tf.float32, [None, 784])\n",
    "            z = tf.placeholder(tf.float32, [None, z_dim])\n",
    "\n",
    "            fake_logits, fake = generator(z)\n",
    "            D_real_lf, D_real_mse = discriminator(X)\n",
    "            D_fake_lf, D_fake_mse = discriminator(fake, reuse=True)\n",
    "            expected_shape(D_real_mse, []) # scalar\n",
    "            expected_shape(D_fake_mse, []) # scalar\n",
    "\n",
    "            D_fake_hinge = tf.reduce_mean(tf.maximum(0., m - D_fake_mse)) # hinge_loss\n",
    "            D_loss = D_real_mse + D_fake_hinge\n",
    "            G_loss = D_fake_mse\n",
    "            if use_pt_regularizer:\n",
    "                G_loss += pt_weight * pt_regularizer(D_fake_lf) # pt_loss\n",
    "\n",
    "            D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name+\"/discriminator\")\n",
    "            G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name+\"/generator\")\n",
    "\n",
    "            D_train_op = tf.train.AdamOptimizer().minimize(D_loss, var_list=D_vars)\n",
    "            G_train_op = tf.train.AdamOptimizer().minimize(G_loss, var_list=G_vars)\n",
    "\n",
    "            self.X = X\n",
    "            self.z = z\n",
    "            self.D_loss = D_loss\n",
    "            self.G_loss = G_loss\n",
    "            self.D_train_op = D_train_op\n",
    "            self.G_train_op = G_train_op\n",
    "            self.fake = fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build nets\n",
    "tf.reset_default_graph()\n",
    "\n",
    "ebgan = EBGAN('ebgan')\n",
    "reg_ebgan = EBGAN('ebgan-pt', use_pt_regularizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1000000] (non-reg) D_loss: 4.8216, G_loss: 0.3266 | (reg) D_loss: 4.8175, G_loss: 1.3232\n",
      "[10000/1000000] (non-reg) D_loss: 2.9944, G_loss: 5.4261 | (reg) D_loss: 2.6470, G_loss: 7.2877\n",
      "[20000/1000000] (non-reg) D_loss: 3.0860, G_loss: 5.5254 | (reg) D_loss: 1.9851, G_loss: 6.6089\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2c7d93351ae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mz_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     _, _, D_loss_cur1, G_loss_cur1 = sess.run([ebgan.D_train_op, ebgan.G_train_op, ebgan.D_loss, ebgan.G_loss],\n\u001b[0;32m---> 12\u001b[0;31m                                               {ebgan.X: X_batch, ebgan.z: z_batch})\n\u001b[0m\u001b[1;32m     13\u001b[0m     _, _, D_loss_cur2, G_loss_cur2 = sess.run([reg_ebgan.D_train_op, reg_ebgan.G_train_op, reg_ebgan.D_loss, reg_ebgan.G_loss],\n\u001b[1;32m     14\u001b[0m                                               {reg_ebgan.X: X_batch, reg_ebgan.z: z_batch})\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "n_iter = 1000000\n",
    "print_step = 10000\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(n_iter):\n",
    "    X_batch, _ = mnist.train.next_batch(batch_size)\n",
    "    z_batch = sample_z(batch_size, z_dim)\n",
    "    _, _, D_loss_cur1, G_loss_cur1 = sess.run([ebgan.D_train_op, ebgan.G_train_op, ebgan.D_loss, ebgan.G_loss],\n",
    "                                              {ebgan.X: X_batch, ebgan.z: z_batch})\n",
    "    _, _, D_loss_cur2, G_loss_cur2 = sess.run([reg_ebgan.D_train_op, reg_ebgan.G_train_op, reg_ebgan.D_loss, reg_ebgan.G_loss],\n",
    "                                              {reg_ebgan.X: X_batch, reg_ebgan.z: z_batch})\n",
    "    \n",
    "    if i % print_step == 0 or i == n_iter-1:\n",
    "        print('[{}/{}] (non-reg) D_loss: {:.4f}, G_loss: {:.4f} | (reg) D_loss: {:.4f}, G_loss: {:.4f}'.\n",
    "              format(i, n_iter, D_loss_cur1, G_loss_cur1, D_loss_cur2, G_loss_cur2))\n",
    "        z_ = sample_z(16, z_dim)\n",
    "        samples1 = sess.run(ebgan.fake, {ebgan.z: z_})\n",
    "        samples2 = sess.run(reg_ebgan.fake, {reg_ebgan.z: z_})\n",
    "        fig1 = plot(samples1)\n",
    "        fig2 = plot(samples2)\n",
    "        c = int((i+1) / print_step)\n",
    "        fig1.savefig('out/ebgan_{:0>4d}.png'.format(c), bbox_inches='tight')\n",
    "        fig2.savefig('out/ebgan-pt_{:0>4d}.png'.format(c), bbox_inches='tight')\n",
    "        plt.close(fig1)\n",
    "        plt.close(fig2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
