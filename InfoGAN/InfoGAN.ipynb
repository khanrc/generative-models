{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: GAN\n",
    "\n",
    "$$\n",
    "\\min_G \\max_D V(D,G) = \\mathbb{E}_{x\\sim P_\\text{data}}[\\log D(x)] + \\mathbb{E}_{z\\sim \\text{noise}} [\\log(1-D(G(z)))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InfoGAN\n",
    "\n",
    "### Main Idea: Mutual Information\n",
    "\n",
    "$$\n",
    "\\min_G \\max_D V_I(D,G) = V(D,G) - \\lambda I(c; G(z,c))\n",
    "$$\n",
    "\n",
    "### Variational Mutual Information Maximization\n",
    "\n",
    "One mathmatics:\n",
    "Mutual information term $I(c; G(z,c))$ is hard to maximize directly as it requires access to the posterior $P(c|x)$. So, we approximate posterior $P(c|x)$ to auxilliary distribution $Q(c|x)$ using variational lower bounds techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://github.com/wiseodd/generative-models/blob/master/GAN/infogan/infogan_tensorflow.py\n",
    "* https://gist.github.com/awjuliani/c9ecd8b37d33d6855cd4ed9aa16ce89f\n",
    "* https://github.com/openai/InfoGAN (Official codes but hard to understand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더이상은 수식 쓰기 귀찮으니 생략한다. 논문 참고."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop steps\n",
    "\n",
    "1. Only categorical condition [ing]\n",
    "2. One categorical + two continuous conditions\n",
    "3. Multiple + Multiple ... ?\n",
    "\n",
    "### Network architecture for MNIST\n",
    "\n",
    "![network_architecutre](infogan_network_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# z_dim: 62 noise + 2 continuous + 10 categorical = 74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dense(inputs, units, activ_fn=tf.nn.relu, use_bn=False, name=\"dense\"):\n",
    "    with tf.variable_scope(name):\n",
    "        net = tf.layers.dense(inputs, units, activation=None)\n",
    "        if use_bn:\n",
    "            net = tf.layers.batch_normalization(net, training=training)\n",
    "        net = activ_fn(net)\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(inputs, n_filters, kernel_size, strides=1, activ_fn=tf.nn.relu, use_bn=False, name=\"conv\"):\n",
    "    with tf.variable_scope(name):\n",
    "        net = tf.layers.conv2d(inputs, n_filters, kernel_size, strides, padding='same')\n",
    "        if use_bn:\n",
    "            net = tf.layers.batch_normalization(net, training=training)\n",
    "        net = activ_fn(net)\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def upconv(inputs, n_filters, kernel_size, strides=2, activ_fn=tf.nn.relu, use_bn=False, name=\"upconv\"):\n",
    "    with tf.variable_scope(name):\n",
    "        net = tf.layers.conv2d_transpose(inputs, n_filters, kernel_size, strides=strides, padding='same')\n",
    "        if use_bn:\n",
    "            net = tf.layers.batch_normalization(net, training=training)\n",
    "        net = activ_fn(net)\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# c1: categorical ~ Cat(10)\n",
    "# c2, c3: continuous ~ Uniform(-1, 1)\n",
    "def generator(z, c1, c2, c3, reuse=False):\n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        z_c = tf.concat([z, c1, c2, c3], axis=1)\n",
    "        net = dense(z_c, 1024, use_bn=True, name=\"dense1\")\n",
    "        net = dense(net, 7*7*128, use_bn=True, name=\"dense2\") # 6272\n",
    "        net = tf.reshape(net, [-1, 7, 7, 128], name=\"reshape\") # 7x7x128\n",
    "        net = upconv(net, 64, [4,4], use_bn=True, name=\"upconv1\") # 14x14x64\n",
    "        net = upconv(net, 1, [4,4], activ_fn=tf.nn.sigmoid, name=\"upconv2\") # 28x28x1\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# discfriminator D and recognition network Q share most of the network\n",
    "def D_Q_shared_nets(x, reuse=False):\n",
    "    with tf.variable_scope(\"D_Q_shared_nets\", reuse=reuse):\n",
    "        net = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        net = conv(net, 64, [4,4], strides=2, activ_fn=lrelu, name=\"conv1\") # 14x14x64\n",
    "        net = conv(net, 128, [4,4], strides=2, activ_fn=lrelu, use_bn=True, name=\"conv2\") # 7x7x128\n",
    "        net = slim.flatten(net)\n",
    "        net = dense(net, 1024, activ_fn=lrelu, use_bn=True, name=\"dense1\")\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def discriminator(x, shared_net, reuse=False):\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        prob = dense(shared_net, 1, activ_fn=tf.nn.sigmoid, name=\"D_output\")\n",
    "    \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# auxilliary distribution Q(c|x)\n",
    "# softmax for categorical, factored Gaussian for continuous\n",
    "# Q. what is factored gaussian?\n",
    "def Q(x, shared_net, reuse=False):\n",
    "    with tf.variable_scope(\"Q\", reuse=reuse):\n",
    "        net = dense(shared_net, 128, activ_fn=lrelu, use_bn=True, name=\"Q_dense\")\n",
    "        c1_prob = dense(net, 10, activ_fn=tf.nn.softmax, name=\"c1_prob\")\n",
    "        # 사실 이 c2, c3 은 dense(net, 2) 로 합쳐도 됨\n",
    "        c2_prob = dense(net, 1, activ_fn=tf.nn.tanh, name=\"c2_prob\") \n",
    "        c3_prob = dense(net, 1, activ_fn=tf.nn.tanh, name=\"c3_prob\")\n",
    "        \n",
    "        return c1_prob, c2_prob, c3_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable D_Q_shared_nets/conv1/conv2d/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-28-96b4ebc49ea3>\", line 3, in conv\n    net = tf.layers.conv2d(inputs, n_filters, kernel_size, strides, padding='same')\n  File \"<ipython-input-35-189354405110>\", line 5, in D_Q_shared_nets\n    net = conv(net, 64, [4,4], strides=2, activ_fn=lrelu, name=\"conv1\") # 14x14x64\n  File \"<ipython-input-90-e03f02ad23d5>\", line 12, in <module>\n    D_real = discriminator(X, D_Q_shared_nets(X))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-e03f02ad23d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mG_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mD_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_Q_shared_nets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mD_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_Q_shared_nets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mQ_c1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_c2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_c3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_Q_shared_nets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-189354405110>\u001b[0m in \u001b[0;36mD_Q_shared_nets\u001b[0;34m(x, reuse)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"D_Q_shared_nets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactiv_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv1\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 14x14x64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactiv_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv2\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 7x7x128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-96b4ebc49ea3>\u001b[0m in \u001b[0;36mconv\u001b[0;34m(inputs, n_filters, kernel_size, strides, activ_fn, use_bn, name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactiv_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_bn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/convolutional.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    507\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m       _scope=name)\n\u001b[0;32m--> 509\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \"\"\"\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/convolutional.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    136\u001b[0m                                   \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                                   \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                                   dtype=self.dtype)\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       self.bias = vs.get_variable('bias',\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    986\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    989\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m    990\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    888\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    342\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m       return _true_getter(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc\u001b[0m in \u001b[0;36mvariable_getter\u001b[0;34m(getter, name, shape, dtype, initializer, regularizer, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m           variable_getter=functools.partial(getter, **kwargs))\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# Build (if necessary) and call the layer, inside a variable scope.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc\u001b[0m in \u001b[0;36m_add_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, variable_getter)\u001b[0m\n\u001b[1;32m    198\u001b[0m                                \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                                \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                                trainable=trainable and self.trainable)\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0;31m# TODO(sguada) fix name = variable.op.name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    637\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 639\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    640\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable D_Q_shared_nets/conv1/conv2d/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-28-96b4ebc49ea3>\", line 3, in conv\n    net = tf.layers.conv2d(inputs, n_filters, kernel_size, strides, padding='same')\n  File \"<ipython-input-35-189354405110>\", line 5, in D_Q_shared_nets\n    net = conv(net, 64, [4,4], strides=2, activ_fn=lrelu, name=\"conv1\") # 14x14x64\n  File \"<ipython-input-90-e03f02ad23d5>\", line 12, in <module>\n    D_real = discriminator(X, D_Q_shared_nets(X))\n"
     ]
    }
   ],
   "source": [
    "# build nets\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "training = tf.placeholder(tf.bool)\n",
    "z = tf.placeholder(tf.float32, [None, 62])\n",
    "c1 = tf.placeholder(tf.float32, [None, 10]) # tensor 는 타입이 동일해야 함. 전체 latent vector type 이 float 이므로 얘도 float.\n",
    "c2 = tf.placeholder(tf.float32, [None, 1])\n",
    "c3 = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "G_sample = generator(z, c1, c2, c3)\n",
    "D_real = discriminator(X, D_Q_shared_nets(X))\n",
    "D_fake = discriminator(X, D_Q_shared_nets(G_sample, reuse=True), reuse=True)\n",
    "Q_c1, Q_c2, Q_c3 = Q(G_sample, D_Q_shared_nets(G_sample, reuse=True))\n",
    "\n",
    "D_loss = -tf.reduce_mean(tf.log(D_real + 1e-8) + tf.log(1 - D_fake + 1e-8))\n",
    "G_loss = -tf.reduce_mean(tf.log(D_fake + 1e-8)) # heuristic non-saturation loss\n",
    "# categorical loss\n",
    "Q_c1_loss = tf.reduce_sum(tf.log(Q_c1 + 1e-8) * c1, axis=1)\n",
    "# continuous loss - 사실 c2, c3 은 각각 1개라 reduce_sum 안해도 됨\n",
    "Q_c2_loss = tf.reduce_sum(0.5 * (Q_c2 - c2)**2, axis=1)\n",
    "Q_c3_loss = tf.reduce_sum(0.5 * (Q_c3 - c3)**2, axis=1)\n",
    "Q_loss = -tf.reduce_mean(Q_c1_loss + Q_c2_loss + Q_c3_loss)\n",
    "\n",
    "var_D_Q_shared = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"D_Q_shared_nets\")\n",
    "var_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"discriminator\")\n",
    "var_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"generator\")\n",
    "var_Q = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Q\")\n",
    "\n",
    "# lambda = 1\n",
    "D_train_op = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5).minimize(D_loss, var_list=var_D_Q_shared + var_D)\n",
    "G_train_op = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.5).minimize(G_loss, var_list=var_G)\n",
    "Q_train_op = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5).minimize(Q_loss, var_list=var_D_Q_shared + var_Q + var_G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
