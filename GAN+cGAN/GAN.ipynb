{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784], name=\"X\")\n",
    "D_W1 = tf.Variable(xavier_init([784, 128]), name=\"D_W1\")\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]), name=\"D_b1\")\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]), name=\"D_W2\")\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]), name=\"D_b2\")\n",
    "D_params = [D_W1, D_b1, D_W2, D_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Z is noised input\n",
    "# G(z) => generated new x\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100], name=\"Z\")\n",
    "G_W1 = tf.Variable(xavier_init([100, 128]), name=\"G_W1\")\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]), name=\"G_b1\")\n",
    "G_W2 = tf.Variable(xavier_init([128, 784]), name=\"G_W2\")\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]), name=\"G_b2\")\n",
    "G_params = [G_W1, G_b1, G_W2, G_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x, reuse=False):\n",
    "    # 여기서는 variable 을 따로 생성해서 박아주기 때문에 reuse 가 의미가 없음.\n",
    "    # reuse 가 있는 경우는 get_variable 과 같이 쓰일때. get_variable 을 사용하면,\n",
    "    # 처음에는 변수를 생성해주고, reuse 가 on 된 다음부터는 해당 scope 의 variable 을 리턴한다.\n",
    "    # 또한, 동일한 namespace 의 variable 을 중복 생성할 수 없다.\n",
    "    # 이 코드와 같이 해도 되지만 캡슐화와 구조화를 위해서 이러한 기능을 지원한다고 함.\n",
    "    # https://www.tensorflow.org/programmers_guide/variable_scope\n",
    "    with tf.variable_scope(\"discriminator\") as scope:\n",
    "#         if reuse:\n",
    "#             scope.reuse_variables()\n",
    "        D_a1 = tf.matmul(x, D_W1) + D_b1\n",
    "        D_h1 = tf.nn.relu(D_a1)\n",
    "        # what is logit?\n",
    "        # logit is inverse function of sigmoid function\n",
    "        # therefore, D_logit = logit(D_prob).\n",
    "        # 즉, sigmoid 함수에 들어가는 값이 logit 이라고 할 수 있음.\n",
    "        D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "        D_prob = tf.nn.sigmoid(D_logit)\n",
    "    \n",
    "    return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        G_a1 = tf.matmul(z, G_W1) + G_b1\n",
    "        G_h1 = tf.nn.relu(G_a1)\n",
    "        # what is differ between logit <> log_prob ?\n",
    "        # 이것도 걍 로짓이 맞는거 같은데? logit_prob 의 약자일것 같음.\n",
    "        G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "        G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "    \n",
    "    return G_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "* 왜 D_loss_fake != 0 인가?  \n",
    "* CE 인데 정답 레이블이 전부 0이면 loss 가 0이 되어야 하는 게 아닌가?\n",
    "\n",
    "### Answer\n",
    "\n",
    "* 내가 CE 를 잘못 이해하고 있었음.\n",
    "* CE 의 식은 C = -1/n \\sum [yln(a) + (1-y)ln(1-a)]\n",
    "* 즉, 정답레이블이 1이든 0이든 정답과 예측값과의 차이를 계산하여 로스로 사용함.\n",
    "    * 이 값은 ln(x) 의 [0,1] 구간을 사용하여 계산하므로, 정답에서 크게 멀어질수록 로스값이 기하급수적으로 증가한다.\n",
    "* one-hot vector 에 CE 를 적용할때랑은 다른가...?\n",
    "    * pset 1 다시 보면서 체크해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "G_sample = generator(Z)\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample, reuse=True)\n",
    "\n",
    "# V1. paper-based learning (다른 코드를 보면 logit 을 이용해서 CE로 계산을 함. 이게 더 성능이 잘 나오나봄)\n",
    "# http://bamos.github.io/2016/08/09/deep-completion/ 참고\n",
    "# 아래 함수자체가 CE기 때문에, 알고리즘은 동일하고, 다만 연산속도가 logits 으로 계산하는게 더 빠른가보다.\n",
    "\n",
    "# maximize x => minimize -x\n",
    "#D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1-D_fake))\n",
    "#G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "\n",
    "# V2.\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=D_params)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=G_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "Z_dim = 100\n",
    "N = mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0] D_loss: 0.0039 / G_loss: 9.1304\n",
      "[      1] D_loss: 0.0134 / G_loss: 7.1805\n",
      "[      2] D_loss: 0.1067 / G_loss: 4.2016\n",
      "[      3] D_loss: 0.0755 / G_loss: 5.7283\n",
      "[      4] D_loss: 0.2402 / G_loss: 5.0259\n",
      "[      5] D_loss: 0.1957 / G_loss: 4.9974\n",
      "[      6] D_loss: 0.2148 / G_loss: 4.3867\n",
      "[      7] D_loss: 0.4680 / G_loss: 3.7835\n",
      "[      8] D_loss: 0.3944 / G_loss: 3.0099\n",
      "[      9] D_loss: 0.3505 / G_loss: 3.4984\n",
      "[     10] D_loss: 0.3929 / G_loss: 2.6362\n",
      "[     11] D_loss: 0.4463 / G_loss: 2.2785\n",
      "[     12] D_loss: 0.5120 / G_loss: 3.0658\n",
      "[     13] D_loss: 0.5303 / G_loss: 2.7237\n",
      "[     14] D_loss: 0.8039 / G_loss: 2.5436\n",
      "[     15] D_loss: 0.6394 / G_loss: 2.2838\n",
      "[     16] D_loss: 0.5588 / G_loss: 1.9623\n",
      "[     17] D_loss: 0.5544 / G_loss: 1.8894\n",
      "[     18] D_loss: 0.6899 / G_loss: 1.8406\n",
      "[     19] D_loss: 0.6192 / G_loss: 1.9803\n",
      "[     20] D_loss: 0.7252 / G_loss: 2.2254\n",
      "[     21] D_loss: 0.7548 / G_loss: 2.1413\n",
      "[     22] D_loss: 0.6380 / G_loss: 2.4090\n",
      "[     23] D_loss: 0.7454 / G_loss: 2.2652\n",
      "[     24] D_loss: 0.7230 / G_loss: 1.9798\n",
      "[     25] D_loss: 0.8463 / G_loss: 2.2679\n",
      "[     26] D_loss: 0.6531 / G_loss: 2.0986\n",
      "[     27] D_loss: 0.6504 / G_loss: 2.5558\n",
      "[     28] D_loss: 0.7746 / G_loss: 1.9995\n",
      "[     29] D_loss: 0.9106 / G_loss: 2.0595\n",
      "[     30] D_loss: 0.8480 / G_loss: 1.8497\n",
      "[     31] D_loss: 0.4851 / G_loss: 2.7897\n",
      "[     32] D_loss: 0.6465 / G_loss: 2.5867\n",
      "[     33] D_loss: 0.7834 / G_loss: 2.5329\n",
      "[     34] D_loss: 0.6597 / G_loss: 2.1229\n",
      "[     35] D_loss: 0.5351 / G_loss: 2.6720\n",
      "[     36] D_loss: 0.4833 / G_loss: 2.4018\n",
      "[     37] D_loss: 0.7934 / G_loss: 2.2284\n",
      "[     38] D_loss: 0.6915 / G_loss: 2.1870\n",
      "[     39] D_loss: 0.5881 / G_loss: 2.2395\n",
      "[     40] D_loss: 0.6652 / G_loss: 2.6758\n",
      "[     41] D_loss: 0.7743 / G_loss: 2.4892\n",
      "[     42] D_loss: 0.6884 / G_loss: 2.8933\n",
      "[     43] D_loss: 0.6515 / G_loss: 2.2665\n",
      "[     44] D_loss: 0.5496 / G_loss: 2.3554\n",
      "[     45] D_loss: 0.5965 / G_loss: 2.1085\n",
      "[     46] D_loss: 0.7447 / G_loss: 1.9390\n",
      "[     47] D_loss: 0.4309 / G_loss: 2.1714\n",
      "[     48] D_loss: 0.5074 / G_loss: 2.5083\n",
      "[     49] D_loss: 0.5007 / G_loss: 2.4612\n",
      "[     50] D_loss: 0.6519 / G_loss: 2.0801\n",
      "[     51] D_loss: 0.7435 / G_loss: 2.2760\n",
      "[     52] D_loss: 0.5737 / G_loss: 2.4004\n",
      "[     53] D_loss: 0.6198 / G_loss: 2.3606\n",
      "[     54] D_loss: 0.4203 / G_loss: 2.6536\n",
      "[     55] D_loss: 0.6683 / G_loss: 1.9516\n",
      "[     56] D_loss: 0.7735 / G_loss: 2.5866\n",
      "[     57] D_loss: 0.5906 / G_loss: 2.2025\n",
      "[     58] D_loss: 0.7663 / G_loss: 2.3053\n",
      "[     59] D_loss: 0.6159 / G_loss: 2.1027\n",
      "[     60] D_loss: 0.6832 / G_loss: 2.1470\n",
      "[     61] D_loss: 0.4449 / G_loss: 2.4227\n",
      "[     62] D_loss: 0.6970 / G_loss: 2.0208\n",
      "[     63] D_loss: 0.5893 / G_loss: 1.9986\n",
      "[     64] D_loss: 0.7228 / G_loss: 2.5081\n",
      "[     65] D_loss: 0.6516 / G_loss: 2.1058\n",
      "[     66] D_loss: 0.6122 / G_loss: 2.3392\n",
      "[     67] D_loss: 0.7027 / G_loss: 2.4240\n",
      "[     68] D_loss: 0.6070 / G_loss: 2.5835\n",
      "[     69] D_loss: 0.7248 / G_loss: 3.0190\n",
      "[     70] D_loss: 0.6694 / G_loss: 2.5125\n",
      "[     71] D_loss: 0.8125 / G_loss: 2.4867\n",
      "[     72] D_loss: 0.4650 / G_loss: 2.4988\n",
      "[     73] D_loss: 0.6271 / G_loss: 2.1846\n",
      "[     74] D_loss: 0.5565 / G_loss: 2.2588\n",
      "[     75] D_loss: 0.4827 / G_loss: 2.5101\n",
      "[     76] D_loss: 0.6425 / G_loss: 2.5125\n",
      "[     77] D_loss: 0.6166 / G_loss: 2.0052\n",
      "[     78] D_loss: 0.5454 / G_loss: 2.5398\n",
      "[     79] D_loss: 0.7879 / G_loss: 2.2220\n",
      "[     80] D_loss: 0.6288 / G_loss: 2.4935\n",
      "[     81] D_loss: 0.6491 / G_loss: 2.5614\n",
      "[     82] D_loss: 0.6303 / G_loss: 2.7451\n",
      "[     83] D_loss: 0.4736 / G_loss: 2.1968\n",
      "[     84] D_loss: 0.7949 / G_loss: 2.3993\n",
      "[     85] D_loss: 0.5810 / G_loss: 2.3386\n",
      "[     86] D_loss: 0.6427 / G_loss: 2.8315\n",
      "[     87] D_loss: 0.6513 / G_loss: 2.1640\n",
      "[     88] D_loss: 0.6276 / G_loss: 2.0986\n",
      "[     89] D_loss: 0.5038 / G_loss: 2.2305\n",
      "[     90] D_loss: 0.6431 / G_loss: 2.3626\n",
      "[     91] D_loss: 0.4502 / G_loss: 2.6319\n",
      "[     92] D_loss: 0.6722 / G_loss: 2.7174\n",
      "[     93] D_loss: 0.6420 / G_loss: 2.4703\n",
      "[     94] D_loss: 0.6383 / G_loss: 2.5536\n",
      "[     95] D_loss: 0.7059 / G_loss: 2.3792\n",
      "[     96] D_loss: 0.6307 / G_loss: 2.3072\n",
      "[     97] D_loss: 0.6394 / G_loss: 2.7956\n",
      "[     98] D_loss: 0.5480 / G_loss: 2.8374\n",
      "[     99] D_loss: 0.5286 / G_loss: 2.2821\n",
      "[    100] D_loss: 0.4548 / G_loss: 2.1956\n",
      "[    101] D_loss: 0.8342 / G_loss: 2.7549\n",
      "[    102] D_loss: 0.4295 / G_loss: 3.0180\n",
      "[    103] D_loss: 0.8772 / G_loss: 2.7017\n",
      "[    104] D_loss: 0.6154 / G_loss: 2.7206\n",
      "[    105] D_loss: 0.9687 / G_loss: 2.3669\n",
      "[    106] D_loss: 0.5969 / G_loss: 3.2250\n",
      "[    107] D_loss: 0.4563 / G_loss: 2.5671\n",
      "[    108] D_loss: 0.5776 / G_loss: 2.6795\n",
      "[    109] D_loss: 0.4480 / G_loss: 2.4660\n",
      "[    110] D_loss: 0.4732 / G_loss: 2.5752\n",
      "[    111] D_loss: 0.6518 / G_loss: 2.3991\n",
      "[    112] D_loss: 0.5457 / G_loss: 2.1867\n",
      "[    113] D_loss: 0.5329 / G_loss: 2.5891\n",
      "[    114] D_loss: 0.6279 / G_loss: 2.4135\n",
      "[    115] D_loss: 0.7021 / G_loss: 2.4992\n",
      "[    116] D_loss: 0.6228 / G_loss: 2.8325\n",
      "[    117] D_loss: 0.6023 / G_loss: 2.1909\n",
      "[    118] D_loss: 0.5427 / G_loss: 2.3913\n",
      "[    119] D_loss: 0.4463 / G_loss: 2.6210\n",
      "[    120] D_loss: 0.6458 / G_loss: 2.7924\n",
      "[    121] D_loss: 0.3958 / G_loss: 2.5483\n",
      "[    122] D_loss: 0.6018 / G_loss: 2.3684\n",
      "[    123] D_loss: 0.5084 / G_loss: 2.4596\n",
      "[    124] D_loss: 0.3672 / G_loss: 2.7326\n",
      "[    125] D_loss: 0.5623 / G_loss: 2.5711\n",
      "[    126] D_loss: 0.5976 / G_loss: 2.5023\n",
      "[    127] D_loss: 0.5267 / G_loss: 2.4876\n",
      "[    128] D_loss: 0.5637 / G_loss: 2.3559\n",
      "[    129] D_loss: 0.7050 / G_loss: 2.8349\n",
      "[    130] D_loss: 0.6206 / G_loss: 2.4741\n",
      "[    131] D_loss: 0.5402 / G_loss: 2.7103\n",
      "[    132] D_loss: 0.5728 / G_loss: 2.8832\n",
      "[    133] D_loss: 0.5516 / G_loss: 2.4576\n",
      "[    134] D_loss: 0.5874 / G_loss: 2.7225\n",
      "[    135] D_loss: 0.7423 / G_loss: 2.8218\n",
      "[    136] D_loss: 0.5652 / G_loss: 2.8089\n",
      "[    137] D_loss: 0.4781 / G_loss: 2.5646\n",
      "[    138] D_loss: 0.5241 / G_loss: 2.5052\n",
      "[    139] D_loss: 0.4518 / G_loss: 2.8794\n",
      "[    140] D_loss: 0.6127 / G_loss: 2.8722\n",
      "[    141] D_loss: 0.4993 / G_loss: 2.4577\n",
      "[    142] D_loss: 0.4694 / G_loss: 2.7571\n",
      "[    143] D_loss: 0.5700 / G_loss: 2.0114\n",
      "[    144] D_loss: 0.4777 / G_loss: 2.5953\n",
      "[    145] D_loss: 0.4177 / G_loss: 2.5115\n",
      "[    146] D_loss: 0.4706 / G_loss: 2.4472\n",
      "[    147] D_loss: 0.7982 / G_loss: 3.2243\n",
      "[    148] D_loss: 0.5315 / G_loss: 2.9075\n",
      "[    149] D_loss: 0.4901 / G_loss: 2.4221\n",
      "[    150] D_loss: 0.5068 / G_loss: 2.9470\n",
      "[    151] D_loss: 0.5032 / G_loss: 2.6804\n",
      "[    152] D_loss: 0.5244 / G_loss: 2.5893\n",
      "[    153] D_loss: 0.4508 / G_loss: 2.7033\n",
      "[    154] D_loss: 0.4027 / G_loss: 2.4857\n",
      "[    155] D_loss: 0.6099 / G_loss: 3.0279\n",
      "[    156] D_loss: 0.4784 / G_loss: 3.0152\n",
      "[    157] D_loss: 0.4686 / G_loss: 2.5518\n",
      "[    158] D_loss: 0.5700 / G_loss: 2.4066\n",
      "[    159] D_loss: 0.5326 / G_loss: 2.2896\n",
      "[    160] D_loss: 0.3746 / G_loss: 3.0039\n",
      "[    161] D_loss: 0.4706 / G_loss: 2.0615\n",
      "[    162] D_loss: 0.5749 / G_loss: 2.6577\n",
      "[    163] D_loss: 0.4454 / G_loss: 2.4897\n",
      "[    164] D_loss: 0.4114 / G_loss: 2.8980\n",
      "[    165] D_loss: 0.5800 / G_loss: 2.6276\n",
      "[    166] D_loss: 0.6163 / G_loss: 2.8167\n",
      "[    167] D_loss: 0.5576 / G_loss: 2.6946\n",
      "[    168] D_loss: 0.7086 / G_loss: 2.2807\n",
      "[    169] D_loss: 0.5443 / G_loss: 3.2073\n",
      "[    170] D_loss: 0.5386 / G_loss: 2.3605\n",
      "[    171] D_loss: 0.5592 / G_loss: 2.6199\n",
      "[    172] D_loss: 0.4054 / G_loss: 2.7624\n",
      "[    173] D_loss: 0.6040 / G_loss: 2.4565\n",
      "[    174] D_loss: 0.4306 / G_loss: 2.6059\n",
      "[    175] D_loss: 0.3860 / G_loss: 2.9116\n",
      "[    176] D_loss: 0.4641 / G_loss: 2.8534\n",
      "[    177] D_loss: 0.5801 / G_loss: 3.0549\n",
      "[    178] D_loss: 0.4255 / G_loss: 2.5312\n",
      "[    179] D_loss: 0.4623 / G_loss: 2.3039\n",
      "[    180] D_loss: 0.5577 / G_loss: 2.7849\n",
      "[    181] D_loss: 0.4454 / G_loss: 2.6801\n",
      "[    182] D_loss: 0.4898 / G_loss: 2.3209\n",
      "[    183] D_loss: 0.4550 / G_loss: 2.5365\n",
      "[    184] D_loss: 0.4438 / G_loss: 2.8528\n",
      "[    185] D_loss: 0.5333 / G_loss: 2.3227\n",
      "[    186] D_loss: 0.5748 / G_loss: 2.6469\n",
      "[    187] D_loss: 0.6129 / G_loss: 2.3894\n",
      "[    188] D_loss: 0.6461 / G_loss: 2.6691\n",
      "[    189] D_loss: 0.3968 / G_loss: 2.7388\n",
      "[    190] D_loss: 0.5420 / G_loss: 2.6875\n",
      "[    191] D_loss: 0.4896 / G_loss: 2.6564\n",
      "[    192] D_loss: 0.4621 / G_loss: 2.7431\n",
      "[    193] D_loss: 0.6122 / G_loss: 2.4888\n",
      "[    194] D_loss: 0.6117 / G_loss: 2.5246\n",
      "[    195] D_loss: 0.7083 / G_loss: 2.9140\n",
      "[    196] D_loss: 0.5579 / G_loss: 2.5399\n",
      "[    197] D_loss: 0.7271 / G_loss: 2.4486\n",
      "[    198] D_loss: 0.4958 / G_loss: 2.4237\n",
      "[    199] D_loss: 0.5559 / G_loss: 2.6037\n",
      "[    200] D_loss: 0.5067 / G_loss: 3.5418\n",
      "[    201] D_loss: 0.5233 / G_loss: 2.8507\n",
      "[    202] D_loss: 0.5045 / G_loss: 2.2482\n",
      "[    203] D_loss: 0.4158 / G_loss: 3.2278\n",
      "[    204] D_loss: 0.5591 / G_loss: 2.6753\n",
      "[    205] D_loss: 0.6099 / G_loss: 2.8342\n",
      "[    206] D_loss: 0.7468 / G_loss: 2.5490\n",
      "[    207] D_loss: 0.5016 / G_loss: 2.9537\n",
      "[    208] D_loss: 0.5575 / G_loss: 2.7484\n",
      "[    209] D_loss: 0.3727 / G_loss: 2.5932\n",
      "[    210] D_loss: 0.5162 / G_loss: 2.8920\n",
      "[    211] D_loss: 0.4717 / G_loss: 2.7632\n",
      "[    212] D_loss: 0.4396 / G_loss: 2.9384\n",
      "[    213] D_loss: 0.4475 / G_loss: 2.5183\n",
      "[    214] D_loss: 0.5055 / G_loss: 3.0669\n",
      "[    215] D_loss: 0.3903 / G_loss: 2.8887\n",
      "[    216] D_loss: 0.3922 / G_loss: 2.7376\n",
      "[    217] D_loss: 0.4105 / G_loss: 3.0718\n",
      "[    218] D_loss: 0.3203 / G_loss: 3.0233\n",
      "[    219] D_loss: 0.5294 / G_loss: 2.7205\n",
      "[    220] D_loss: 0.5364 / G_loss: 2.6856\n",
      "[    221] D_loss: 0.4688 / G_loss: 2.4229\n",
      "[    222] D_loss: 0.5613 / G_loss: 2.8788\n",
      "[    223] D_loss: 0.4073 / G_loss: 2.4159\n",
      "[    224] D_loss: 0.3570 / G_loss: 2.8459\n",
      "[    225] D_loss: 0.4275 / G_loss: 2.6350\n",
      "[    226] D_loss: 0.4476 / G_loss: 2.6291\n",
      "[    227] D_loss: 0.6179 / G_loss: 2.5235\n",
      "[    228] D_loss: 0.6478 / G_loss: 2.6547\n",
      "[    229] D_loss: 0.4618 / G_loss: 2.5577\n",
      "[    230] D_loss: 0.4566 / G_loss: 2.5857\n",
      "[    231] D_loss: 0.4374 / G_loss: 2.4108\n",
      "[    232] D_loss: 0.5285 / G_loss: 3.0827\n",
      "[    233] D_loss: 0.5857 / G_loss: 2.6059\n",
      "[    234] D_loss: 0.6439 / G_loss: 2.7842\n",
      "[    235] D_loss: 0.4552 / G_loss: 2.3844\n",
      "[    236] D_loss: 0.5451 / G_loss: 2.3160\n",
      "[    237] D_loss: 0.5677 / G_loss: 2.7964\n",
      "[    238] D_loss: 0.4099 / G_loss: 2.5251\n",
      "[    239] D_loss: 0.4874 / G_loss: 2.5813\n",
      "[    240] D_loss: 0.4271 / G_loss: 2.6527\n",
      "[    241] D_loss: 0.4343 / G_loss: 2.7105\n",
      "[    242] D_loss: 0.5057 / G_loss: 2.6490\n",
      "[    243] D_loss: 0.4611 / G_loss: 2.9124\n",
      "[    244] D_loss: 0.5651 / G_loss: 2.6312\n",
      "[    245] D_loss: 0.4937 / G_loss: 2.8138\n",
      "[    246] D_loss: 0.4490 / G_loss: 2.5217\n",
      "[    247] D_loss: 0.5370 / G_loss: 2.2215\n",
      "[    248] D_loss: 0.6002 / G_loss: 2.7069\n",
      "[    249] D_loss: 0.5925 / G_loss: 2.7592\n",
      "[    250] D_loss: 0.5082 / G_loss: 2.6989\n",
      "[    251] D_loss: 0.7923 / G_loss: 2.9889\n",
      "[    252] D_loss: 0.3923 / G_loss: 2.8345\n",
      "[    253] D_loss: 0.3771 / G_loss: 2.7006\n",
      "[    254] D_loss: 0.4842 / G_loss: 3.1272\n",
      "[    255] D_loss: 0.3603 / G_loss: 2.2604\n",
      "[    256] D_loss: 0.5263 / G_loss: 2.7123\n",
      "[    257] D_loss: 0.4082 / G_loss: 2.5187\n",
      "[    258] D_loss: 0.4268 / G_loss: 2.8570\n",
      "[    259] D_loss: 0.5183 / G_loss: 2.9001\n",
      "[    260] D_loss: 0.5525 / G_loss: 2.3525\n",
      "[    261] D_loss: 0.3196 / G_loss: 2.6355\n",
      "[    262] D_loss: 0.4147 / G_loss: 2.8800\n",
      "[    263] D_loss: 0.5174 / G_loss: 2.9462\n",
      "[    264] D_loss: 0.4569 / G_loss: 2.6208\n",
      "[    265] D_loss: 0.4935 / G_loss: 3.4316\n",
      "[    266] D_loss: 0.6394 / G_loss: 2.5000\n",
      "[    267] D_loss: 0.3805 / G_loss: 2.5807\n",
      "[    268] D_loss: 0.4184 / G_loss: 2.9833\n",
      "[    269] D_loss: 0.5744 / G_loss: 2.9702\n",
      "[    270] D_loss: 0.5373 / G_loss: 2.7511\n",
      "[    271] D_loss: 0.4591 / G_loss: 2.8304\n",
      "[    272] D_loss: 0.3906 / G_loss: 3.3711\n",
      "[    273] D_loss: 0.3601 / G_loss: 2.1003\n",
      "[    274] D_loss: 0.5394 / G_loss: 2.4945\n",
      "[    275] D_loss: 0.3481 / G_loss: 3.3146\n",
      "[    276] D_loss: 0.4951 / G_loss: 2.5102\n",
      "[    277] D_loss: 0.3037 / G_loss: 2.5820\n",
      "[    278] D_loss: 0.4012 / G_loss: 3.2730\n",
      "[    279] D_loss: 0.4324 / G_loss: 2.8010\n",
      "[    280] D_loss: 0.4127 / G_loss: 2.5740\n",
      "[    281] D_loss: 0.3305 / G_loss: 2.8172\n",
      "[    282] D_loss: 0.5784 / G_loss: 2.6751\n",
      "[    283] D_loss: 0.5627 / G_loss: 3.1180\n",
      "[    284] D_loss: 0.2776 / G_loss: 2.6717\n",
      "[    285] D_loss: 0.4323 / G_loss: 2.8859\n",
      "[    286] D_loss: 0.6260 / G_loss: 3.0081\n",
      "[    287] D_loss: 0.5272 / G_loss: 3.4702\n",
      "[    288] D_loss: 0.5062 / G_loss: 2.7543\n",
      "[    289] D_loss: 0.4293 / G_loss: 2.6659\n",
      "[    290] D_loss: 0.3674 / G_loss: 3.0826\n",
      "[    291] D_loss: 0.3896 / G_loss: 2.7640\n",
      "[    292] D_loss: 0.4322 / G_loss: 2.5426\n",
      "[    293] D_loss: 0.4725 / G_loss: 2.8075\n",
      "[    294] D_loss: 0.3704 / G_loss: 3.0280\n",
      "[    295] D_loss: 0.3897 / G_loss: 2.8556\n",
      "[    296] D_loss: 0.3955 / G_loss: 2.9338\n",
      "[    297] D_loss: 0.3495 / G_loss: 2.5887\n",
      "[    298] D_loss: 0.4507 / G_loss: 2.6510\n",
      "[    299] D_loss: 0.5506 / G_loss: 2.3432\n",
      "[    300] D_loss: 0.3931 / G_loss: 2.5395\n",
      "[    301] D_loss: 0.4216 / G_loss: 2.4960\n",
      "[    302] D_loss: 0.5616 / G_loss: 2.8874\n",
      "[    303] D_loss: 0.3486 / G_loss: 2.6938\n",
      "[    304] D_loss: 0.4944 / G_loss: 2.4211\n",
      "[    305] D_loss: 0.5919 / G_loss: 2.8445\n",
      "[    306] D_loss: 0.3865 / G_loss: 2.4352\n",
      "[    307] D_loss: 0.4721 / G_loss: 2.4652\n",
      "[    308] D_loss: 0.5537 / G_loss: 2.6430\n",
      "[    309] D_loss: 0.4201 / G_loss: 3.1212\n",
      "[    310] D_loss: 0.4636 / G_loss: 2.8470\n",
      "[    311] D_loss: 0.3783 / G_loss: 2.4638\n",
      "[    312] D_loss: 0.3348 / G_loss: 2.6273\n",
      "[    313] D_loss: 0.4669 / G_loss: 2.8243\n",
      "[    314] D_loss: 0.4857 / G_loss: 3.3585\n",
      "[    315] D_loss: 0.3884 / G_loss: 2.6917\n",
      "[    316] D_loss: 0.3048 / G_loss: 2.5990\n",
      "[    317] D_loss: 0.4080 / G_loss: 3.5294\n",
      "[    318] D_loss: 0.4164 / G_loss: 3.0164\n",
      "[    319] D_loss: 0.4658 / G_loss: 2.8365\n",
      "[    320] D_loss: 0.5412 / G_loss: 3.2881\n",
      "[    321] D_loss: 0.3566 / G_loss: 3.3203\n",
      "[    322] D_loss: 0.5095 / G_loss: 2.5442\n",
      "[    323] D_loss: 0.4213 / G_loss: 3.0996\n",
      "[    324] D_loss: 0.5573 / G_loss: 2.3018\n",
      "[    325] D_loss: 0.6019 / G_loss: 3.3471\n",
      "[    326] D_loss: 0.5457 / G_loss: 2.8999\n",
      "[    327] D_loss: 0.4320 / G_loss: 2.7247\n",
      "[    328] D_loss: 0.4661 / G_loss: 3.2379\n",
      "[    329] D_loss: 0.4568 / G_loss: 2.7956\n",
      "[    330] D_loss: 0.3250 / G_loss: 2.8822\n",
      "[    331] D_loss: 0.7167 / G_loss: 3.1470\n",
      "[    332] D_loss: 0.3246 / G_loss: 2.7964\n",
      "[    333] D_loss: 0.3797 / G_loss: 2.8994\n",
      "[    334] D_loss: 0.3811 / G_loss: 2.7682\n",
      "[    335] D_loss: 0.6170 / G_loss: 3.1966\n",
      "[    336] D_loss: 0.4159 / G_loss: 3.2221\n",
      "[    337] D_loss: 0.4823 / G_loss: 2.4738\n",
      "[    338] D_loss: 0.2337 / G_loss: 2.8458\n",
      "[    339] D_loss: 0.4835 / G_loss: 2.6502\n",
      "[    340] D_loss: 0.4236 / G_loss: 2.6802\n",
      "[    341] D_loss: 0.3348 / G_loss: 3.0147\n",
      "[    342] D_loss: 0.4359 / G_loss: 2.7508\n",
      "[    343] D_loss: 0.4867 / G_loss: 2.8342\n",
      "[    344] D_loss: 0.4767 / G_loss: 2.7457\n",
      "[    345] D_loss: 0.2406 / G_loss: 3.2952\n",
      "[    346] D_loss: 0.2966 / G_loss: 2.9538\n",
      "[    347] D_loss: 0.5379 / G_loss: 2.9940\n",
      "[    348] D_loss: 0.2910 / G_loss: 2.8657\n",
      "[    349] D_loss: 0.2849 / G_loss: 2.3474\n",
      "[    350] D_loss: 0.4218 / G_loss: 3.2997\n",
      "[    351] D_loss: 0.4742 / G_loss: 2.7881\n",
      "[    352] D_loss: 0.3330 / G_loss: 2.6343\n",
      "[    353] D_loss: 0.3757 / G_loss: 3.3417\n",
      "[    354] D_loss: 0.3453 / G_loss: 3.1556\n",
      "[    355] D_loss: 0.3276 / G_loss: 2.5492\n",
      "[    356] D_loss: 0.4248 / G_loss: 2.7095\n",
      "[    357] D_loss: 0.4651 / G_loss: 2.9847\n",
      "[    358] D_loss: 0.3293 / G_loss: 3.1041\n",
      "[    359] D_loss: 0.4252 / G_loss: 3.1769\n",
      "[    360] D_loss: 0.3965 / G_loss: 2.7553\n",
      "[    361] D_loss: 0.3472 / G_loss: 2.8044\n",
      "[    362] D_loss: 0.4454 / G_loss: 3.4275\n",
      "[    363] D_loss: 0.4116 / G_loss: 2.7426\n",
      "[    364] D_loss: 0.4114 / G_loss: 2.8486\n",
      "[    365] D_loss: 0.4146 / G_loss: 3.3047\n",
      "[    366] D_loss: 0.2703 / G_loss: 2.6730\n",
      "[    367] D_loss: 0.3533 / G_loss: 2.8728\n",
      "[    368] D_loss: 0.6515 / G_loss: 2.8860\n",
      "[    369] D_loss: 0.4286 / G_loss: 3.0930\n",
      "[    370] D_loss: 0.4934 / G_loss: 2.5830\n",
      "[    371] D_loss: 0.3524 / G_loss: 2.9563\n",
      "[    372] D_loss: 0.2891 / G_loss: 2.9443\n",
      "[    373] D_loss: 0.4099 / G_loss: 3.2936\n",
      "[    374] D_loss: 0.4301 / G_loss: 2.5357\n",
      "[    375] D_loss: 0.3959 / G_loss: 2.9068\n",
      "[    376] D_loss: 0.3587 / G_loss: 3.2164\n",
      "[    377] D_loss: 0.3260 / G_loss: 2.9326\n",
      "[    378] D_loss: 0.2377 / G_loss: 2.8388\n",
      "[    379] D_loss: 0.3774 / G_loss: 3.0133\n",
      "[    380] D_loss: 0.3753 / G_loss: 2.9118\n",
      "[    381] D_loss: 0.2724 / G_loss: 2.5851\n",
      "[    382] D_loss: 0.3024 / G_loss: 3.2850\n",
      "[    383] D_loss: 0.3594 / G_loss: 2.9311\n",
      "[    384] D_loss: 0.4427 / G_loss: 2.9010\n",
      "[    385] D_loss: 0.3608 / G_loss: 2.8712\n",
      "[    386] D_loss: 0.2866 / G_loss: 2.3189\n",
      "[    387] D_loss: 0.2590 / G_loss: 2.7511\n",
      "[    388] D_loss: 0.3903 / G_loss: 3.6783\n",
      "[    389] D_loss: 0.2345 / G_loss: 3.2271\n",
      "[    390] D_loss: 0.4394 / G_loss: 2.6391\n",
      "[    391] D_loss: 0.4200 / G_loss: 3.0711\n",
      "[    392] D_loss: 0.3629 / G_loss: 3.0198\n",
      "[    393] D_loss: 0.5093 / G_loss: 2.7587\n",
      "[    394] D_loss: 0.3795 / G_loss: 2.3595\n",
      "[    395] D_loss: 0.5515 / G_loss: 2.6366\n",
      "[    396] D_loss: 0.4762 / G_loss: 3.0047\n",
      "[    397] D_loss: 0.3538 / G_loss: 3.0283\n",
      "[    398] D_loss: 0.6750 / G_loss: 2.7513\n",
      "[    399] D_loss: 0.3888 / G_loss: 2.7216\n",
      "[    400] D_loss: 0.3416 / G_loss: 3.1235\n",
      "[    401] D_loss: 0.6013 / G_loss: 2.5838\n",
      "[    402] D_loss: 0.3632 / G_loss: 2.6961\n",
      "[    403] D_loss: 0.3254 / G_loss: 3.0536\n",
      "[    404] D_loss: 0.6051 / G_loss: 2.8490\n",
      "[    405] D_loss: 0.3883 / G_loss: 2.3278\n",
      "[    406] D_loss: 0.5057 / G_loss: 2.8915\n",
      "[    407] D_loss: 0.3585 / G_loss: 2.8383\n",
      "[    408] D_loss: 0.4034 / G_loss: 3.2562\n",
      "[    409] D_loss: 0.5235 / G_loss: 2.5503\n",
      "[    410] D_loss: 0.2862 / G_loss: 2.6637\n",
      "[    411] D_loss: 0.4008 / G_loss: 3.1162\n",
      "[    412] D_loss: 0.3578 / G_loss: 2.9803\n",
      "[    413] D_loss: 0.2750 / G_loss: 3.3366\n",
      "[    414] D_loss: 0.3284 / G_loss: 3.5603\n",
      "[    415] D_loss: 0.2727 / G_loss: 2.7795\n",
      "[    416] D_loss: 0.2919 / G_loss: 2.9285\n",
      "[    417] D_loss: 0.5049 / G_loss: 2.9151\n",
      "[    418] D_loss: 0.3674 / G_loss: 2.7880\n",
      "[    419] D_loss: 0.4196 / G_loss: 3.0118\n",
      "[    420] D_loss: 0.6157 / G_loss: 3.0931\n",
      "[    421] D_loss: 0.4155 / G_loss: 2.9779\n",
      "[    422] D_loss: 0.2808 / G_loss: 2.8180\n",
      "[    423] D_loss: 0.3880 / G_loss: 2.9087\n",
      "[    424] D_loss: 0.3051 / G_loss: 2.8722\n",
      "[    425] D_loss: 0.3748 / G_loss: 2.5574\n",
      "[    426] D_loss: 0.3537 / G_loss: 3.1081\n",
      "[    427] D_loss: 0.5729 / G_loss: 3.1324\n",
      "[    428] D_loss: 0.4708 / G_loss: 2.6893\n",
      "[    429] D_loss: 0.5349 / G_loss: 2.4658\n",
      "[    430] D_loss: 0.4215 / G_loss: 3.1293\n",
      "[    431] D_loss: 0.4024 / G_loss: 2.8744\n",
      "[    432] D_loss: 0.4455 / G_loss: 3.4595\n",
      "[    433] D_loss: 0.4138 / G_loss: 2.5595\n",
      "[    434] D_loss: 0.3347 / G_loss: 2.7846\n",
      "[    435] D_loss: 0.4274 / G_loss: 2.7575\n",
      "[    436] D_loss: 0.3552 / G_loss: 2.7056\n",
      "[    437] D_loss: 0.4587 / G_loss: 3.0547\n",
      "[    438] D_loss: 0.3564 / G_loss: 3.6390\n",
      "[    439] D_loss: 0.4309 / G_loss: 2.7981\n",
      "[    440] D_loss: 0.2925 / G_loss: 2.7769\n",
      "[    441] D_loss: 0.3927 / G_loss: 2.9339\n",
      "[    442] D_loss: 0.4809 / G_loss: 3.2837\n",
      "[    443] D_loss: 0.3907 / G_loss: 2.7255\n",
      "[    444] D_loss: 0.5262 / G_loss: 2.6240\n",
      "[    445] D_loss: 0.4301 / G_loss: 3.1872\n",
      "[    446] D_loss: 0.5057 / G_loss: 2.6915\n",
      "[    447] D_loss: 0.3381 / G_loss: 2.8440\n",
      "[    448] D_loss: 0.4437 / G_loss: 2.7996\n",
      "[    449] D_loss: 0.3357 / G_loss: 3.2096\n",
      "[    450] D_loss: 0.3148 / G_loss: 2.9368\n",
      "[    451] D_loss: 0.2138 / G_loss: 3.0087\n",
      "[    452] D_loss: 0.2368 / G_loss: 3.2641\n",
      "[    453] D_loss: 0.2012 / G_loss: 2.8398\n",
      "[    454] D_loss: 0.2846 / G_loss: 3.3801\n",
      "[    455] D_loss: 0.2482 / G_loss: 3.0793\n",
      "[    456] D_loss: 0.3082 / G_loss: 3.2589\n",
      "[    457] D_loss: 0.3031 / G_loss: 2.7871\n",
      "[    458] D_loss: 0.3689 / G_loss: 3.0221\n",
      "[    459] D_loss: 0.3484 / G_loss: 3.2486\n",
      "[    460] D_loss: 0.3487 / G_loss: 3.1025\n",
      "[    461] D_loss: 0.4308 / G_loss: 2.6585\n",
      "[    462] D_loss: 0.5439 / G_loss: 3.3747\n",
      "[    463] D_loss: 0.3897 / G_loss: 2.6753\n",
      "[    464] D_loss: 0.3101 / G_loss: 2.8007\n",
      "[    465] D_loss: 0.2597 / G_loss: 2.5141\n",
      "[    466] D_loss: 0.2913 / G_loss: 2.8539\n",
      "[    467] D_loss: 0.4615 / G_loss: 2.6433\n",
      "[    468] D_loss: 0.4154 / G_loss: 2.8149\n",
      "[    469] D_loss: 0.2721 / G_loss: 2.9516\n",
      "[    470] D_loss: 0.3808 / G_loss: 3.5581\n",
      "[    471] D_loss: 0.4380 / G_loss: 2.6773\n",
      "[    472] D_loss: 0.4148 / G_loss: 3.0838\n",
      "[    473] D_loss: 0.2720 / G_loss: 2.6632\n",
      "[    474] D_loss: 0.2960 / G_loss: 2.6022\n",
      "[    475] D_loss: 0.3123 / G_loss: 2.8052\n",
      "[    476] D_loss: 0.3924 / G_loss: 2.7622\n",
      "[    477] D_loss: 0.3202 / G_loss: 3.2860\n",
      "[    478] D_loss: 0.4079 / G_loss: 2.6127\n",
      "[    479] D_loss: 0.4160 / G_loss: 2.9265\n",
      "[    480] D_loss: 0.5929 / G_loss: 3.2458\n",
      "[    481] D_loss: 0.3675 / G_loss: 2.8017\n",
      "[    482] D_loss: 0.3485 / G_loss: 2.6887\n",
      "[    483] D_loss: 0.3621 / G_loss: 3.0874\n",
      "[    484] D_loss: 0.3137 / G_loss: 2.9776\n",
      "[    485] D_loss: 0.2803 / G_loss: 3.1219\n",
      "[    486] D_loss: 0.2831 / G_loss: 3.4222\n",
      "[    487] D_loss: 0.2459 / G_loss: 2.5737\n",
      "[    488] D_loss: 0.2837 / G_loss: 3.1061\n",
      "[    489] D_loss: 0.3301 / G_loss: 3.3986\n",
      "[    490] D_loss: 0.2933 / G_loss: 3.0424\n",
      "[    491] D_loss: 0.4123 / G_loss: 3.7570\n",
      "[    492] D_loss: 0.3762 / G_loss: 2.8207\n",
      "[    493] D_loss: 0.3826 / G_loss: 2.5881\n",
      "[    494] D_loss: 0.4255 / G_loss: 3.0092\n",
      "[    495] D_loss: 0.3953 / G_loss: 2.9330\n",
      "[    496] D_loss: 0.3284 / G_loss: 2.6480\n",
      "[    497] D_loss: 0.4709 / G_loss: 3.4449\n",
      "[    498] D_loss: 0.3034 / G_loss: 2.7895\n",
      "[    499] D_loss: 0.2988 / G_loss: 3.5581\n",
      "[    500] D_loss: 0.3557 / G_loss: 3.0272\n",
      "[    501] D_loss: 0.4720 / G_loss: 2.7561\n",
      "[    502] D_loss: 0.3428 / G_loss: 3.0356\n",
      "[    503] D_loss: 0.2511 / G_loss: 2.8677\n",
      "[    504] D_loss: 0.4564 / G_loss: 3.0282\n",
      "[    505] D_loss: 0.2964 / G_loss: 2.6182\n",
      "[    506] D_loss: 0.2278 / G_loss: 2.7202\n",
      "[    507] D_loss: 0.4875 / G_loss: 2.9661\n",
      "[    508] D_loss: 0.4053 / G_loss: 2.7534\n",
      "[    509] D_loss: 0.4515 / G_loss: 3.3380\n",
      "[    510] D_loss: 0.3038 / G_loss: 2.9823\n",
      "[    511] D_loss: 0.4135 / G_loss: 2.7061\n",
      "[    512] D_loss: 0.3637 / G_loss: 2.8360\n",
      "[    513] D_loss: 0.4589 / G_loss: 2.8666\n",
      "[    514] D_loss: 0.4067 / G_loss: 3.0799\n",
      "[    515] D_loss: 0.3171 / G_loss: 2.9303\n",
      "[    516] D_loss: 0.1697 / G_loss: 3.3676\n",
      "[    517] D_loss: 0.3289 / G_loss: 2.5493\n",
      "[    518] D_loss: 0.2592 / G_loss: 2.4329\n",
      "[    519] D_loss: 0.2933 / G_loss: 3.0314\n",
      "[    520] D_loss: 0.4556 / G_loss: 2.7343\n",
      "[    521] D_loss: 0.4026 / G_loss: 3.1008\n",
      "[    522] D_loss: 0.3149 / G_loss: 3.2097\n",
      "[    523] D_loss: 0.1680 / G_loss: 3.3151\n",
      "[    524] D_loss: 0.4375 / G_loss: 3.0448\n",
      "[    525] D_loss: 0.3662 / G_loss: 3.1544\n",
      "[    526] D_loss: 0.2669 / G_loss: 3.2112\n",
      "[    527] D_loss: 0.4501 / G_loss: 2.5622\n",
      "[    528] D_loss: 0.3873 / G_loss: 2.8839\n",
      "[    529] D_loss: 0.3304 / G_loss: 2.5875\n",
      "[    530] D_loss: 0.2217 / G_loss: 2.9087\n",
      "[    531] D_loss: 0.1548 / G_loss: 3.3532\n",
      "[    532] D_loss: 0.4374 / G_loss: 3.3660\n",
      "[    533] D_loss: 0.3720 / G_loss: 3.0002\n",
      "[    534] D_loss: 0.2140 / G_loss: 3.3715\n",
      "[    535] D_loss: 0.5821 / G_loss: 3.5318\n",
      "[    536] D_loss: 0.3722 / G_loss: 3.1378\n",
      "[    537] D_loss: 0.3638 / G_loss: 3.1388\n",
      "[    538] D_loss: 0.3585 / G_loss: 2.7866\n",
      "[    539] D_loss: 0.1957 / G_loss: 2.9711\n",
      "[    540] D_loss: 0.2883 / G_loss: 2.6546\n",
      "[    541] D_loss: 0.3664 / G_loss: 3.0299\n",
      "[    542] D_loss: 0.2766 / G_loss: 3.3526\n",
      "[    543] D_loss: 0.1743 / G_loss: 3.0419\n",
      "[    544] D_loss: 0.3557 / G_loss: 3.2385\n",
      "[    545] D_loss: 0.3711 / G_loss: 2.8464\n",
      "[    546] D_loss: 0.2327 / G_loss: 2.9379\n",
      "[    547] D_loss: 0.3412 / G_loss: 3.0998\n",
      "[    548] D_loss: 0.2941 / G_loss: 3.0669\n",
      "[    549] D_loss: 0.3304 / G_loss: 3.3110\n",
      "[    550] D_loss: 0.4043 / G_loss: 3.4603\n",
      "[    551] D_loss: 0.3164 / G_loss: 3.2220\n",
      "[    552] D_loss: 0.2947 / G_loss: 2.9280\n",
      "[    553] D_loss: 0.3921 / G_loss: 2.9709\n",
      "[    554] D_loss: 0.3623 / G_loss: 2.9488\n",
      "[    555] D_loss: 0.3844 / G_loss: 2.8588\n",
      "[    556] D_loss: 0.2650 / G_loss: 3.2968\n",
      "[    557] D_loss: 0.4704 / G_loss: 2.6703\n",
      "[    558] D_loss: 0.1914 / G_loss: 3.0734\n",
      "[    559] D_loss: 0.2227 / G_loss: 3.1514\n",
      "[    560] D_loss: 0.2960 / G_loss: 2.8317\n",
      "[    561] D_loss: 0.3896 / G_loss: 3.1250\n",
      "[    562] D_loss: 0.3333 / G_loss: 2.7376\n",
      "[    563] D_loss: 0.3035 / G_loss: 3.0416\n",
      "[    564] D_loss: 0.6261 / G_loss: 2.9057\n",
      "[    565] D_loss: 0.3510 / G_loss: 2.8407\n",
      "[    566] D_loss: 0.3321 / G_loss: 3.0252\n",
      "[    567] D_loss: 0.4628 / G_loss: 2.5559\n",
      "[    568] D_loss: 0.3802 / G_loss: 2.8240\n",
      "[    569] D_loss: 0.2930 / G_loss: 3.1600\n",
      "[    570] D_loss: 0.2594 / G_loss: 3.3069\n",
      "[    571] D_loss: 0.2400 / G_loss: 3.2447\n",
      "[    572] D_loss: 0.2538 / G_loss: 3.2709\n",
      "[    573] D_loss: 0.4019 / G_loss: 3.2363\n",
      "[    574] D_loss: 0.1958 / G_loss: 3.5262\n",
      "[    575] D_loss: 0.3289 / G_loss: 3.1198\n",
      "[    576] D_loss: 0.1767 / G_loss: 2.5875\n",
      "[    577] D_loss: 0.3942 / G_loss: 3.0920\n",
      "[    578] D_loss: 0.3134 / G_loss: 3.1369\n",
      "[    579] D_loss: 0.3506 / G_loss: 3.0484\n",
      "[    580] D_loss: 0.4247 / G_loss: 2.5152\n",
      "[    581] D_loss: 0.2599 / G_loss: 3.2474\n",
      "[    582] D_loss: 0.2987 / G_loss: 3.1481\n",
      "[    583] D_loss: 0.3743 / G_loss: 3.2563\n",
      "[    584] D_loss: 0.3411 / G_loss: 3.2442\n",
      "[    585] D_loss: 0.2105 / G_loss: 3.1171\n",
      "[    586] D_loss: 0.1477 / G_loss: 2.7613\n",
      "[    587] D_loss: 0.2453 / G_loss: 3.1820\n",
      "[    588] D_loss: 0.2521 / G_loss: 2.5692\n",
      "[    589] D_loss: 0.1065 / G_loss: 3.7698\n",
      "[    590] D_loss: 0.4585 / G_loss: 2.8434\n",
      "[    591] D_loss: 0.2272 / G_loss: 2.9325\n",
      "[    592] D_loss: 0.1358 / G_loss: 3.2098\n",
      "[    593] D_loss: 0.2690 / G_loss: 3.1135\n",
      "[    594] D_loss: 0.3366 / G_loss: 2.9396\n",
      "[    595] D_loss: 0.2297 / G_loss: 2.9558\n",
      "[    596] D_loss: 0.4526 / G_loss: 3.2731\n",
      "[    597] D_loss: 0.3060 / G_loss: 3.2546\n",
      "[    598] D_loss: 0.1999 / G_loss: 3.1196\n",
      "[    599] D_loss: 0.2782 / G_loss: 3.3656\n",
      "[    600] D_loss: 0.2659 / G_loss: 3.3992\n",
      "[    601] D_loss: 0.2964 / G_loss: 3.9050\n",
      "[    602] D_loss: 0.3008 / G_loss: 3.4529\n",
      "[    603] D_loss: 0.2365 / G_loss: 2.8983\n",
      "[    604] D_loss: 0.3756 / G_loss: 2.8115\n",
      "[    605] D_loss: 0.3521 / G_loss: 3.4019\n",
      "[    606] D_loss: 0.3229 / G_loss: 3.0222\n",
      "[    607] D_loss: 0.1609 / G_loss: 3.2271\n",
      "[    608] D_loss: 0.3231 / G_loss: 2.8832\n",
      "[    609] D_loss: 0.2200 / G_loss: 3.4510\n",
      "[    610] D_loss: 0.3466 / G_loss: 3.2421\n",
      "[    611] D_loss: 0.1948 / G_loss: 3.2519\n",
      "[    612] D_loss: 0.2322 / G_loss: 3.4909\n",
      "[    613] D_loss: 0.3569 / G_loss: 3.0271\n",
      "[    614] D_loss: 0.2649 / G_loss: 3.7235\n",
      "[    615] D_loss: 0.3202 / G_loss: 3.1499\n",
      "[    616] D_loss: 0.2235 / G_loss: 2.8874\n",
      "[    617] D_loss: 0.2686 / G_loss: 3.2401\n",
      "[    618] D_loss: 0.4231 / G_loss: 3.1207\n",
      "[    619] D_loss: 0.3133 / G_loss: 3.6511\n",
      "[    620] D_loss: 0.1536 / G_loss: 2.9264\n",
      "[    621] D_loss: 0.2971 / G_loss: 2.5628\n",
      "[    622] D_loss: 0.2836 / G_loss: 3.5272\n",
      "[    623] D_loss: 0.3099 / G_loss: 2.9663\n",
      "[    624] D_loss: 0.6387 / G_loss: 3.6701\n",
      "[    625] D_loss: 0.2718 / G_loss: 3.4055\n",
      "[    626] D_loss: 0.4511 / G_loss: 3.2547\n",
      "[    627] D_loss: 0.2131 / G_loss: 2.8773\n",
      "[    628] D_loss: 0.2386 / G_loss: 3.1298\n",
      "[    629] D_loss: 0.1822 / G_loss: 3.2641\n",
      "[    630] D_loss: 0.2869 / G_loss: 2.5919\n",
      "[    631] D_loss: 0.2532 / G_loss: 2.7230\n",
      "[    632] D_loss: 0.3460 / G_loss: 2.4974\n",
      "[    633] D_loss: 0.2170 / G_loss: 3.0481\n",
      "[    634] D_loss: 0.3442 / G_loss: 2.9375\n",
      "[    635] D_loss: 0.4708 / G_loss: 3.1626\n",
      "[    636] D_loss: 0.3680 / G_loss: 3.2448\n",
      "[    637] D_loss: 0.3138 / G_loss: 3.1643\n",
      "[    638] D_loss: 0.3161 / G_loss: 3.5284\n",
      "[    639] D_loss: 0.1449 / G_loss: 3.2123\n",
      "[    640] D_loss: 0.2243 / G_loss: 3.3031\n",
      "[    641] D_loss: 0.3659 / G_loss: 3.2009\n",
      "[    642] D_loss: 0.2110 / G_loss: 2.7937\n",
      "[    643] D_loss: 0.2246 / G_loss: 3.0816\n",
      "[    644] D_loss: 0.2618 / G_loss: 2.5527\n",
      "[    645] D_loss: 0.1714 / G_loss: 3.5290\n",
      "[    646] D_loss: 0.2206 / G_loss: 2.8725\n",
      "[    647] D_loss: 0.3755 / G_loss: 2.8508\n",
      "[    648] D_loss: 0.2160 / G_loss: 2.8353\n",
      "[    649] D_loss: 0.2414 / G_loss: 2.8471\n",
      "[    650] D_loss: 0.3620 / G_loss: 3.2444\n",
      "[    651] D_loss: 0.1097 / G_loss: 3.3531\n",
      "[    652] D_loss: 0.2121 / G_loss: 2.9481\n",
      "[    653] D_loss: 0.2000 / G_loss: 3.1853\n",
      "[    654] D_loss: 0.1470 / G_loss: 3.5365\n",
      "[    655] D_loss: 0.1308 / G_loss: 3.1224\n",
      "[    656] D_loss: 0.3167 / G_loss: 3.3085\n",
      "[    657] D_loss: 0.4226 / G_loss: 3.1308\n",
      "[    658] D_loss: 0.2761 / G_loss: 3.0871\n",
      "[    659] D_loss: 0.2093 / G_loss: 3.2151\n",
      "[    660] D_loss: 0.2729 / G_loss: 3.2526\n",
      "[    661] D_loss: 0.2471 / G_loss: 3.0535\n",
      "[    662] D_loss: 0.1933 / G_loss: 3.3553\n",
      "[    663] D_loss: 0.2551 / G_loss: 3.2031\n",
      "[    664] D_loss: 0.4419 / G_loss: 3.1090\n",
      "[    665] D_loss: 0.2128 / G_loss: 3.1808\n",
      "[    666] D_loss: 0.3269 / G_loss: 3.2685\n",
      "[    667] D_loss: 0.1897 / G_loss: 3.7569\n",
      "[    668] D_loss: 0.3115 / G_loss: 2.7158\n",
      "[    669] D_loss: 0.2791 / G_loss: 3.1321\n",
      "[    670] D_loss: 0.2399 / G_loss: 2.8173\n",
      "[    671] D_loss: 0.3755 / G_loss: 3.1893\n",
      "[    672] D_loss: 0.1305 / G_loss: 2.9632\n",
      "[    673] D_loss: 0.3037 / G_loss: 3.1751\n",
      "[    674] D_loss: 0.2780 / G_loss: 2.9064\n",
      "[    675] D_loss: 0.2421 / G_loss: 2.9217\n",
      "[    676] D_loss: 0.2416 / G_loss: 2.8290\n",
      "[    677] D_loss: 0.4458 / G_loss: 3.4095\n",
      "[    678] D_loss: 0.3318 / G_loss: 2.8033\n",
      "[    679] D_loss: 0.1506 / G_loss: 3.8901\n",
      "[    680] D_loss: 0.2702 / G_loss: 3.2874\n",
      "[    681] D_loss: 0.3637 / G_loss: 3.1806\n",
      "[    682] D_loss: 0.3606 / G_loss: 2.6015\n",
      "[    683] D_loss: 0.3415 / G_loss: 3.2044\n",
      "[    684] D_loss: 0.2704 / G_loss: 2.7672\n",
      "[    685] D_loss: 0.3271 / G_loss: 3.7780\n",
      "[    686] D_loss: 0.2316 / G_loss: 3.1818\n",
      "[    687] D_loss: 0.2136 / G_loss: 3.1732\n",
      "[    688] D_loss: 0.1440 / G_loss: 3.1098\n",
      "[    689] D_loss: 0.2246 / G_loss: 3.1676\n",
      "[    690] D_loss: 0.2401 / G_loss: 2.7043\n",
      "[    691] D_loss: 0.1678 / G_loss: 2.7975\n",
      "[    692] D_loss: 0.1483 / G_loss: 3.3203\n",
      "[    693] D_loss: 0.2731 / G_loss: 3.3524\n",
      "[    694] D_loss: 0.2169 / G_loss: 3.1150\n",
      "[    695] D_loss: 0.2401 / G_loss: 3.3071\n",
      "[    696] D_loss: 0.3141 / G_loss: 3.0814\n",
      "[    697] D_loss: 0.2957 / G_loss: 3.2005\n",
      "[    698] D_loss: 0.2783 / G_loss: 3.0090\n",
      "[    699] D_loss: 0.1079 / G_loss: 2.6902\n",
      "[    700] D_loss: 0.3957 / G_loss: 2.9711\n",
      "[    701] D_loss: 0.3471 / G_loss: 3.2202\n",
      "[    702] D_loss: 0.4333 / G_loss: 3.3154\n",
      "[    703] D_loss: 0.1253 / G_loss: 3.4450\n",
      "[    704] D_loss: 0.2400 / G_loss: 2.9881\n",
      "[    705] D_loss: 0.2485 / G_loss: 3.2666\n",
      "[    706] D_loss: 0.2809 / G_loss: 2.9096\n",
      "[    707] D_loss: 0.2647 / G_loss: 3.2677\n",
      "[    708] D_loss: 0.2558 / G_loss: 3.0795\n",
      "[    709] D_loss: 0.1068 / G_loss: 3.1034\n",
      "[    710] D_loss: 0.2016 / G_loss: 3.2641\n",
      "[    711] D_loss: 0.2557 / G_loss: 3.1695\n",
      "[    712] D_loss: 0.1639 / G_loss: 2.7475\n",
      "[    713] D_loss: 0.3868 / G_loss: 2.9230\n",
      "[    714] D_loss: 0.1877 / G_loss: 3.4048\n",
      "[    715] D_loss: 0.1206 / G_loss: 3.1715\n",
      "[    716] D_loss: 0.1939 / G_loss: 3.4065\n",
      "[    717] D_loss: 0.3915 / G_loss: 3.5488\n",
      "[    718] D_loss: 0.3065 / G_loss: 3.1200\n",
      "[    719] D_loss: 0.3070 / G_loss: 2.7805\n",
      "[    720] D_loss: 0.1279 / G_loss: 3.3940\n",
      "[    721] D_loss: 0.1989 / G_loss: 3.3127\n",
      "[    722] D_loss: 0.1587 / G_loss: 3.0156\n",
      "[    723] D_loss: 0.2390 / G_loss: 3.1155\n",
      "[    724] D_loss: 0.1559 / G_loss: 3.1612\n",
      "[    725] D_loss: 0.2056 / G_loss: 2.7658\n",
      "[    726] D_loss: 0.3717 / G_loss: 2.8121\n",
      "[    727] D_loss: 0.1756 / G_loss: 2.8702\n",
      "[    728] D_loss: 0.4029 / G_loss: 2.7300\n",
      "[    729] D_loss: 0.2475 / G_loss: 2.6433\n",
      "[    730] D_loss: 0.4196 / G_loss: 3.0186\n",
      "[    731] D_loss: 0.3513 / G_loss: 3.0559\n",
      "[    732] D_loss: 0.4029 / G_loss: 2.9837\n",
      "[    733] D_loss: 0.2033 / G_loss: 3.2534\n",
      "[    734] D_loss: 0.1666 / G_loss: 2.8715\n",
      "[    735] D_loss: 0.3051 / G_loss: 2.8985\n",
      "[    736] D_loss: 0.1258 / G_loss: 3.1275\n",
      "[    737] D_loss: 0.3421 / G_loss: 2.7514\n",
      "[    738] D_loss: 0.2082 / G_loss: 3.4593\n",
      "[    739] D_loss: 0.0781 / G_loss: 3.7905\n",
      "[    740] D_loss: 0.3236 / G_loss: 2.7939\n",
      "[    741] D_loss: 0.3195 / G_loss: 3.2074\n",
      "[    742] D_loss: 0.2027 / G_loss: 3.2352\n",
      "[    743] D_loss: 0.2375 / G_loss: 3.0880\n",
      "[    744] D_loss: 0.2839 / G_loss: 3.2124\n",
      "[    745] D_loss: 0.1369 / G_loss: 3.6113\n",
      "[    746] D_loss: 0.1618 / G_loss: 3.4519\n",
      "[    747] D_loss: 0.2359 / G_loss: 2.9374\n",
      "[    748] D_loss: 0.1785 / G_loss: 3.0732\n",
      "[    749] D_loss: 0.4623 / G_loss: 3.2291\n",
      "[    750] D_loss: 0.1436 / G_loss: 2.8997\n",
      "[    751] D_loss: 0.1800 / G_loss: 3.4315\n",
      "[    752] D_loss: 0.2882 / G_loss: 3.0370\n",
      "[    753] D_loss: 0.2712 / G_loss: 3.3743\n",
      "[    754] D_loss: 0.3453 / G_loss: 3.5161\n",
      "[    755] D_loss: 0.1142 / G_loss: 3.3166\n",
      "[    756] D_loss: 0.2290 / G_loss: 3.0607\n",
      "[    757] D_loss: 0.3007 / G_loss: 3.2902\n",
      "[    758] D_loss: 0.2354 / G_loss: 3.2758\n",
      "[    759] D_loss: 0.3103 / G_loss: 3.1381\n",
      "[    760] D_loss: 0.4609 / G_loss: 2.8583\n",
      "[    761] D_loss: 0.2372 / G_loss: 2.9992\n",
      "[    762] D_loss: 0.2520 / G_loss: 3.1564\n",
      "[    763] D_loss: 0.4845 / G_loss: 3.3197\n",
      "[    764] D_loss: 0.1672 / G_loss: 3.3347\n",
      "[    765] D_loss: 0.4881 / G_loss: 3.1653\n",
      "[    766] D_loss: 0.2294 / G_loss: 2.8108\n",
      "[    767] D_loss: 0.1776 / G_loss: 3.1811\n",
      "[    768] D_loss: 0.3720 / G_loss: 3.2627\n",
      "[    769] D_loss: 0.3536 / G_loss: 3.0125\n",
      "[    770] D_loss: 0.2435 / G_loss: 3.1435\n",
      "[    771] D_loss: 0.2156 / G_loss: 3.7320\n",
      "[    772] D_loss: 0.2928 / G_loss: 3.0607\n",
      "[    773] D_loss: 0.2166 / G_loss: 3.2465\n",
      "[    774] D_loss: 0.1972 / G_loss: 3.3271\n",
      "[    775] D_loss: 0.2191 / G_loss: 3.5083\n",
      "[    776] D_loss: 0.2024 / G_loss: 3.0715\n",
      "[    777] D_loss: 0.2455 / G_loss: 3.3560\n",
      "[    778] D_loss: 0.2420 / G_loss: 2.8281\n",
      "[    779] D_loss: 0.1388 / G_loss: 3.1374\n",
      "[    780] D_loss: 0.2653 / G_loss: 3.3471\n",
      "[    781] D_loss: 0.3106 / G_loss: 2.7475\n",
      "[    782] D_loss: 0.2340 / G_loss: 2.8260\n",
      "[    783] D_loss: 0.1658 / G_loss: 3.6023\n",
      "[    784] D_loss: 0.1747 / G_loss: 3.7343\n",
      "[    785] D_loss: 0.2349 / G_loss: 3.2957\n",
      "[    786] D_loss: 0.2076 / G_loss: 2.8517\n",
      "[    787] D_loss: 0.3109 / G_loss: 3.2048\n",
      "[    788] D_loss: 0.3752 / G_loss: 3.6883\n",
      "[    789] D_loss: 0.2145 / G_loss: 2.8449\n",
      "[    790] D_loss: 0.2616 / G_loss: 2.9701\n",
      "[    791] D_loss: 0.0980 / G_loss: 3.7563\n",
      "[    792] D_loss: 0.3061 / G_loss: 3.1322\n",
      "[    793] D_loss: 0.2428 / G_loss: 3.3083\n",
      "[    794] D_loss: 0.2353 / G_loss: 3.7506\n",
      "[    795] D_loss: 0.3335 / G_loss: 3.0907\n",
      "[    796] D_loss: 0.1796 / G_loss: 3.2856\n",
      "[    797] D_loss: 0.2622 / G_loss: 3.2640\n",
      "[    798] D_loss: 0.1446 / G_loss: 2.9324\n",
      "[    799] D_loss: 0.1854 / G_loss: 3.3870\n",
      "[    800] D_loss: 0.3846 / G_loss: 3.3724\n",
      "[    801] D_loss: 0.1810 / G_loss: 3.7012\n",
      "[    802] D_loss: 0.1267 / G_loss: 3.3008\n",
      "[    803] D_loss: 0.2966 / G_loss: 3.0850\n",
      "[    804] D_loss: 0.0761 / G_loss: 3.2822\n",
      "[    805] D_loss: 0.1268 / G_loss: 3.5220\n",
      "[    806] D_loss: 0.1906 / G_loss: 3.2340\n",
      "[    807] D_loss: 0.1543 / G_loss: 3.0791\n",
      "[    808] D_loss: 0.2104 / G_loss: 3.0007\n",
      "[    809] D_loss: 0.4096 / G_loss: 3.3625\n",
      "[    810] D_loss: 0.2703 / G_loss: 3.1642\n",
      "[    811] D_loss: 0.1831 / G_loss: 2.9192\n",
      "[    812] D_loss: 0.2318 / G_loss: 2.8625\n",
      "[    813] D_loss: 0.2523 / G_loss: 2.7529\n",
      "[    814] D_loss: 0.2360 / G_loss: 3.2005\n",
      "[    815] D_loss: 0.2889 / G_loss: 2.9867\n",
      "[    816] D_loss: 0.2769 / G_loss: 3.4875\n",
      "[    817] D_loss: 0.2921 / G_loss: 2.7222\n",
      "[    818] D_loss: 0.2117 / G_loss: 3.0200\n",
      "[    819] D_loss: 0.2617 / G_loss: 3.2622\n",
      "[    820] D_loss: 0.2673 / G_loss: 3.5138\n",
      "[    821] D_loss: 0.0952 / G_loss: 3.4077\n",
      "[    822] D_loss: 0.2253 / G_loss: 3.2824\n",
      "[    823] D_loss: 0.2484 / G_loss: 3.4671\n",
      "[    824] D_loss: 0.1723 / G_loss: 3.1351\n",
      "[    825] D_loss: 0.2415 / G_loss: 3.5834\n",
      "[    826] D_loss: 0.1785 / G_loss: 3.3941\n",
      "[    827] D_loss: 0.3196 / G_loss: 3.5849\n",
      "[    828] D_loss: 0.1431 / G_loss: 3.2342\n",
      "[    829] D_loss: 0.3938 / G_loss: 3.0199\n",
      "[    830] D_loss: 0.3654 / G_loss: 3.2846\n",
      "[    831] D_loss: 0.2201 / G_loss: 2.8545\n",
      "[    832] D_loss: 0.2004 / G_loss: 2.8898\n",
      "[    833] D_loss: 0.2737 / G_loss: 2.9327\n",
      "[    834] D_loss: 0.2231 / G_loss: 2.9589\n",
      "[    835] D_loss: 0.3463 / G_loss: 3.0789\n",
      "[    836] D_loss: 0.2055 / G_loss: 3.1390\n",
      "[    837] D_loss: 0.4397 / G_loss: 2.8483\n",
      "[    838] D_loss: 0.1173 / G_loss: 2.9447\n",
      "[    839] D_loss: 0.2250 / G_loss: 2.8714\n",
      "[    840] D_loss: 0.2532 / G_loss: 2.7888\n",
      "[    841] D_loss: 0.2212 / G_loss: 3.2449\n",
      "[    842] D_loss: 0.2484 / G_loss: 3.0901\n",
      "[    843] D_loss: 0.2182 / G_loss: 2.8600\n",
      "[    844] D_loss: 0.5439 / G_loss: 3.0530\n",
      "[    845] D_loss: 0.3346 / G_loss: 3.3074\n",
      "[    846] D_loss: 0.3625 / G_loss: 3.0514\n",
      "[    847] D_loss: 0.0939 / G_loss: 3.2142\n",
      "[    848] D_loss: 0.1816 / G_loss: 3.1477\n",
      "[    849] D_loss: 0.2456 / G_loss: 3.1261\n",
      "[    850] D_loss: 0.4826 / G_loss: 3.5754\n",
      "[    851] D_loss: 0.2050 / G_loss: 2.9887\n",
      "[    852] D_loss: 0.1871 / G_loss: 2.7661\n",
      "[    853] D_loss: 0.2376 / G_loss: 2.9213\n",
      "[    854] D_loss: 0.3034 / G_loss: 3.2420\n",
      "[    855] D_loss: 0.2803 / G_loss: 3.0192\n",
      "[    856] D_loss: 0.1405 / G_loss: 2.9926\n",
      "[    857] D_loss: 0.1781 / G_loss: 2.8104\n",
      "[    858] D_loss: 0.1875 / G_loss: 3.2020\n",
      "[    859] D_loss: 0.1917 / G_loss: 3.2686\n",
      "[    860] D_loss: 0.2925 / G_loss: 2.9776\n",
      "[    861] D_loss: 0.2320 / G_loss: 2.9231\n",
      "[    862] D_loss: 0.2413 / G_loss: 3.4396\n",
      "[    863] D_loss: 0.2392 / G_loss: 3.2003\n",
      "[    864] D_loss: 0.1515 / G_loss: 2.6590\n",
      "[    865] D_loss: 0.2241 / G_loss: 3.0167\n",
      "[    866] D_loss: 0.2072 / G_loss: 3.1064\n",
      "[    867] D_loss: 0.2210 / G_loss: 2.8411\n",
      "[    868] D_loss: 0.2053 / G_loss: 3.1588\n",
      "[    869] D_loss: 0.3584 / G_loss: 3.5759\n",
      "[    870] D_loss: 0.3191 / G_loss: 3.3934\n",
      "[    871] D_loss: 0.2049 / G_loss: 3.3764\n",
      "[    872] D_loss: 0.2028 / G_loss: 3.2140\n",
      "[    873] D_loss: 0.2195 / G_loss: 3.2213\n",
      "[    874] D_loss: 0.1737 / G_loss: 3.2453\n",
      "[    875] D_loss: 0.2745 / G_loss: 3.3406\n",
      "[    876] D_loss: 0.1130 / G_loss: 3.1065\n",
      "[    877] D_loss: 0.1495 / G_loss: 3.1168\n",
      "[    878] D_loss: 0.4022 / G_loss: 2.8918\n",
      "[    879] D_loss: 0.2084 / G_loss: 3.2030\n",
      "[    880] D_loss: 0.2284 / G_loss: 3.7288\n",
      "[    881] D_loss: 0.1588 / G_loss: 3.2184\n",
      "[    882] D_loss: 0.3211 / G_loss: 3.0031\n",
      "[    883] D_loss: 0.1719 / G_loss: 3.2459\n",
      "[    884] D_loss: 0.3050 / G_loss: 3.2378\n",
      "[    885] D_loss: 0.1829 / G_loss: 3.1232\n",
      "[    886] D_loss: 0.1077 / G_loss: 3.5488\n",
      "[    887] D_loss: 0.2513 / G_loss: 3.5263\n",
      "[    888] D_loss: 0.2047 / G_loss: 3.4247\n",
      "[    889] D_loss: 0.3857 / G_loss: 2.9944\n",
      "[    890] D_loss: 0.2112 / G_loss: 3.4221\n",
      "[    891] D_loss: 0.2535 / G_loss: 3.4147\n",
      "[    892] D_loss: 0.1329 / G_loss: 3.4174\n",
      "[    893] D_loss: 0.1285 / G_loss: 3.1232\n",
      "[    894] D_loss: 0.1041 / G_loss: 3.5872\n",
      "[    895] D_loss: 0.3966 / G_loss: 4.5756\n",
      "[    896] D_loss: 0.3648 / G_loss: 2.9732\n",
      "[    897] D_loss: 0.2343 / G_loss: 3.2328\n",
      "[    898] D_loss: 0.3404 / G_loss: 2.8835\n",
      "[    899] D_loss: 0.1689 / G_loss: 2.9228\n",
      "[    900] D_loss: 0.2121 / G_loss: 3.2551\n",
      "[    901] D_loss: 0.2245 / G_loss: 2.6591\n",
      "[    902] D_loss: 0.2604 / G_loss: 3.1370\n",
      "[    903] D_loss: 0.2300 / G_loss: 3.1241\n",
      "[    904] D_loss: 0.2222 / G_loss: 3.0949\n",
      "[    905] D_loss: 0.1479 / G_loss: 3.1061\n",
      "[    906] D_loss: 0.2038 / G_loss: 3.3696\n",
      "[    907] D_loss: 0.1334 / G_loss: 3.4627\n",
      "[    908] D_loss: 0.1916 / G_loss: 3.0981\n",
      "[    909] D_loss: 0.1738 / G_loss: 3.2425\n",
      "[    910] D_loss: 0.1744 / G_loss: 3.1119\n",
      "[    911] D_loss: 0.2890 / G_loss: 2.8608\n",
      "[    912] D_loss: 0.2380 / G_loss: 3.3021\n",
      "[    913] D_loss: 0.1739 / G_loss: 3.3999\n",
      "[    914] D_loss: 0.2836 / G_loss: 3.0912\n",
      "[    915] D_loss: 0.1064 / G_loss: 3.1407\n",
      "[    916] D_loss: 0.2867 / G_loss: 3.1752\n",
      "[    917] D_loss: 0.1960 / G_loss: 3.5639\n",
      "[    918] D_loss: 0.3375 / G_loss: 3.6700\n",
      "[    919] D_loss: 0.1692 / G_loss: 3.4306\n",
      "[    920] D_loss: 0.1756 / G_loss: 3.4016\n",
      "[    921] D_loss: 0.1273 / G_loss: 3.4771\n",
      "[    922] D_loss: 0.2134 / G_loss: 3.1574\n",
      "[    923] D_loss: 0.1610 / G_loss: 3.2021\n",
      "[    924] D_loss: 0.1469 / G_loss: 3.2270\n",
      "[    925] D_loss: 0.2852 / G_loss: 3.1310\n",
      "[    926] D_loss: 0.3131 / G_loss: 3.2453\n",
      "[    927] D_loss: 0.2262 / G_loss: 3.0956\n",
      "[    928] D_loss: 0.1516 / G_loss: 3.4329\n",
      "[    929] D_loss: 0.2951 / G_loss: 3.3421\n",
      "[    930] D_loss: 0.2620 / G_loss: 3.6040\n",
      "[    931] D_loss: 0.2930 / G_loss: 2.9670\n",
      "[    932] D_loss: 0.3993 / G_loss: 2.9896\n",
      "[    933] D_loss: 0.2955 / G_loss: 3.1019\n",
      "[    934] D_loss: 0.2926 / G_loss: 3.0532\n",
      "[    935] D_loss: 0.2372 / G_loss: 3.4521\n",
      "[    936] D_loss: 0.2917 / G_loss: 3.2291\n",
      "[    937] D_loss: 0.3555 / G_loss: 3.3872\n",
      "[    938] D_loss: 0.2501 / G_loss: 2.6952\n",
      "[    939] D_loss: 0.2511 / G_loss: 3.5279\n",
      "[    940] D_loss: 0.2218 / G_loss: 3.2469\n",
      "[    941] D_loss: 0.1741 / G_loss: 2.8152\n",
      "[    942] D_loss: 0.2699 / G_loss: 3.0215\n",
      "[    943] D_loss: 0.2639 / G_loss: 3.1826\n",
      "[    944] D_loss: 0.3122 / G_loss: 3.2570\n",
      "[    945] D_loss: 0.1724 / G_loss: 3.1857\n",
      "[    946] D_loss: 0.3185 / G_loss: 3.2382\n",
      "[    947] D_loss: 0.2021 / G_loss: 3.0812\n",
      "[    948] D_loss: 0.3031 / G_loss: 3.0121\n",
      "[    949] D_loss: 0.1584 / G_loss: 3.2583\n",
      "[    950] D_loss: 0.2165 / G_loss: 2.9471\n",
      "[    951] D_loss: 0.2456 / G_loss: 3.0786\n",
      "[    952] D_loss: 0.2070 / G_loss: 3.2051\n",
      "[    953] D_loss: 0.2852 / G_loss: 3.0910\n",
      "[    954] D_loss: 0.0947 / G_loss: 3.3509\n",
      "[    955] D_loss: 0.2393 / G_loss: 3.5116\n",
      "[    956] D_loss: 0.3293 / G_loss: 3.3395\n",
      "[    957] D_loss: 0.3606 / G_loss: 3.8086\n",
      "[    958] D_loss: 0.1983 / G_loss: 3.3048\n",
      "[    959] D_loss: 0.2521 / G_loss: 3.0743\n",
      "[    960] D_loss: 0.1510 / G_loss: 3.5766\n",
      "[    961] D_loss: 0.1617 / G_loss: 3.5538\n",
      "[    962] D_loss: 0.2975 / G_loss: 3.2651\n",
      "[    963] D_loss: 0.2774 / G_loss: 3.3787\n",
      "[    964] D_loss: 0.1819 / G_loss: 3.2447\n",
      "[    965] D_loss: 0.3798 / G_loss: 2.9482\n",
      "[    966] D_loss: 0.2629 / G_loss: 3.2748\n",
      "[    967] D_loss: 0.1789 / G_loss: 3.3713\n",
      "[    968] D_loss: 0.1902 / G_loss: 3.1656\n",
      "[    969] D_loss: 0.4567 / G_loss: 3.4866\n",
      "[    970] D_loss: 0.1946 / G_loss: 3.1133\n",
      "[    971] D_loss: 0.4614 / G_loss: 3.4239\n",
      "[    972] D_loss: 0.1783 / G_loss: 3.4489\n",
      "[    973] D_loss: 0.2112 / G_loss: 3.1331\n",
      "[    974] D_loss: 0.1204 / G_loss: 3.2447\n",
      "[    975] D_loss: 0.3662 / G_loss: 3.6818\n",
      "[    976] D_loss: 0.1474 / G_loss: 3.1791\n",
      "[    977] D_loss: 0.1462 / G_loss: 3.0551\n",
      "[    978] D_loss: 0.2995 / G_loss: 3.1486\n",
      "[    979] D_loss: 0.3432 / G_loss: 3.1603\n",
      "[    980] D_loss: 0.2436 / G_loss: 3.5484\n",
      "[    981] D_loss: 0.1933 / G_loss: 3.1569\n",
      "[    982] D_loss: 0.2667 / G_loss: 3.2891\n",
      "[    983] D_loss: 0.2136 / G_loss: 3.6800\n",
      "[    984] D_loss: 0.2529 / G_loss: 3.2474\n",
      "[    985] D_loss: 0.1599 / G_loss: 3.2096\n",
      "[    986] D_loss: 0.1810 / G_loss: 3.1790\n",
      "[    987] D_loss: 0.3316 / G_loss: 3.0371\n",
      "[    988] D_loss: 0.2230 / G_loss: 3.5147\n",
      "[    989] D_loss: 0.1931 / G_loss: 3.1937\n",
      "[    990] D_loss: 0.3826 / G_loss: 3.1616\n",
      "[    991] D_loss: 0.1179 / G_loss: 3.0695\n",
      "[    992] D_loss: 0.3793 / G_loss: 3.0701\n",
      "[    993] D_loss: 0.2248 / G_loss: 2.9706\n",
      "[    994] D_loss: 0.3336 / G_loss: 3.0433\n",
      "[    995] D_loss: 0.1850 / G_loss: 3.0917\n",
      "[    996] D_loss: 0.2562 / G_loss: 3.3985\n",
      "[    997] D_loss: 0.4205 / G_loss: 3.1127\n",
      "[    998] D_loss: 0.2521 / G_loss: 3.2377\n",
      "[    999] D_loss: 0.1371 / G_loss: 3.0917\n",
      "[   1000] D_loss: 0.2637 / G_loss: 3.1681\n",
      "[   1001] D_loss: 0.2194 / G_loss: 3.6946\n",
      "[   1002] D_loss: 0.1985 / G_loss: 3.3793\n",
      "[   1003] D_loss: 0.1601 / G_loss: 3.4912\n",
      "[   1004] D_loss: 0.2721 / G_loss: 3.1859\n",
      "[   1005] D_loss: 0.1513 / G_loss: 3.6533\n",
      "[   1006] D_loss: 0.3877 / G_loss: 3.0433\n",
      "[   1007] D_loss: 0.2069 / G_loss: 3.1057\n",
      "[   1008] D_loss: 0.1484 / G_loss: 3.0963\n",
      "[   1009] D_loss: 0.2186 / G_loss: 3.3654\n",
      "[   1010] D_loss: 0.2938 / G_loss: 3.1482\n",
      "[   1011] D_loss: 0.2095 / G_loss: 3.4775\n",
      "[   1012] D_loss: 0.2121 / G_loss: 3.1452\n",
      "[   1013] D_loss: 0.1779 / G_loss: 3.1358\n",
      "[   1014] D_loss: 0.2376 / G_loss: 2.9303\n",
      "[   1015] D_loss: 0.1524 / G_loss: 3.4060\n",
      "[   1016] D_loss: 0.2283 / G_loss: 3.5630\n",
      "[   1017] D_loss: 0.2407 / G_loss: 3.1704\n",
      "[   1018] D_loss: 0.1454 / G_loss: 3.0812\n",
      "[   1019] D_loss: 0.1766 / G_loss: 3.2057\n",
      "[   1020] D_loss: 0.0844 / G_loss: 3.4311\n",
      "[   1021] D_loss: 0.1807 / G_loss: 3.0584\n",
      "[   1022] D_loss: 0.1307 / G_loss: 3.0303\n",
      "[   1023] D_loss: 0.2007 / G_loss: 3.7352\n",
      "[   1024] D_loss: 0.0339 / G_loss: 4.0511\n",
      "[   1025] D_loss: 0.1359 / G_loss: 3.5714\n",
      "[   1026] D_loss: 0.3231 / G_loss: 3.3806\n",
      "[   1027] D_loss: 0.3527 / G_loss: 3.2152\n",
      "[   1028] D_loss: 0.0546 / G_loss: 3.1619\n",
      "[   1029] D_loss: 0.2282 / G_loss: 3.4902\n",
      "[   1030] D_loss: 0.2361 / G_loss: 3.1811\n",
      "[   1031] D_loss: 0.1728 / G_loss: 3.2448\n",
      "[   1032] D_loss: 0.1621 / G_loss: 3.5698\n",
      "[   1033] D_loss: 0.2022 / G_loss: 3.0029\n",
      "[   1034] D_loss: 0.3131 / G_loss: 3.5086\n",
      "[   1035] D_loss: 0.1777 / G_loss: 3.5638\n",
      "[   1036] D_loss: 0.1331 / G_loss: 3.5799\n",
      "[   1037] D_loss: 0.2275 / G_loss: 3.2116\n",
      "[   1038] D_loss: 0.1998 / G_loss: 3.4988\n",
      "[   1039] D_loss: 0.1938 / G_loss: 3.5226\n",
      "[   1040] D_loss: 0.2321 / G_loss: 3.2690\n",
      "[   1041] D_loss: 0.3656 / G_loss: 3.6459\n",
      "[   1042] D_loss: 0.2226 / G_loss: 2.9536\n",
      "[   1043] D_loss: 0.4117 / G_loss: 3.0399\n",
      "[   1044] D_loss: 0.1007 / G_loss: 3.5411\n",
      "[   1045] D_loss: 0.2374 / G_loss: 3.0622\n",
      "[   1046] D_loss: 0.1115 / G_loss: 3.0402\n",
      "[   1047] D_loss: 0.2886 / G_loss: 3.2926\n",
      "[   1048] D_loss: 0.1390 / G_loss: 3.1628\n",
      "[   1049] D_loss: 0.1904 / G_loss: 3.4437\n",
      "[   1050] D_loss: 0.1696 / G_loss: 3.6286\n",
      "[   1051] D_loss: 0.1696 / G_loss: 3.1327\n",
      "[   1052] D_loss: 0.2397 / G_loss: 3.4990\n",
      "[   1053] D_loss: 0.1484 / G_loss: 3.8948\n",
      "[   1054] D_loss: 0.2681 / G_loss: 3.6162\n",
      "[   1055] D_loss: 0.2337 / G_loss: 3.3379\n",
      "[   1056] D_loss: 0.2221 / G_loss: 3.2832\n",
      "[   1057] D_loss: 0.1185 / G_loss: 2.9954\n",
      "[   1058] D_loss: 0.3541 / G_loss: 3.4204\n",
      "[   1059] D_loss: 0.2878 / G_loss: 3.4515\n",
      "[   1060] D_loss: 0.1517 / G_loss: 3.5877\n",
      "[   1061] D_loss: 0.1710 / G_loss: 3.2869\n",
      "[   1062] D_loss: 0.1256 / G_loss: 3.3123\n",
      "[   1063] D_loss: 0.2273 / G_loss: 3.4172\n",
      "[   1064] D_loss: 0.0733 / G_loss: 3.5943\n",
      "[   1065] D_loss: 0.1165 / G_loss: 2.8530\n",
      "[   1066] D_loss: 0.2677 / G_loss: 3.3255\n",
      "[   1067] D_loss: 0.2428 / G_loss: 3.6088\n",
      "[   1068] D_loss: 0.2735 / G_loss: 3.3153\n",
      "[   1069] D_loss: 0.2310 / G_loss: 3.5303\n",
      "[   1070] D_loss: 0.1762 / G_loss: 3.3398\n",
      "[   1071] D_loss: 0.4781 / G_loss: 3.1854\n",
      "[   1072] D_loss: 0.1564 / G_loss: 3.6977\n",
      "[   1073] D_loss: 0.2724 / G_loss: 3.1008\n",
      "[   1074] D_loss: 0.2152 / G_loss: 3.6335\n",
      "[   1075] D_loss: 0.1414 / G_loss: 3.3190\n",
      "[   1076] D_loss: 0.2647 / G_loss: 3.2989\n",
      "[   1077] D_loss: 0.2274 / G_loss: 3.6174\n",
      "[   1078] D_loss: 0.1344 / G_loss: 3.3264\n",
      "[   1079] D_loss: 0.1453 / G_loss: 3.8304\n",
      "[   1080] D_loss: 0.1634 / G_loss: 3.0976\n",
      "[   1081] D_loss: 0.3380 / G_loss: 3.6383\n",
      "[   1082] D_loss: 0.2626 / G_loss: 3.1378\n",
      "[   1083] D_loss: 0.1943 / G_loss: 3.5330\n",
      "[   1084] D_loss: 0.1874 / G_loss: 3.7187\n",
      "[   1085] D_loss: 0.3594 / G_loss: 3.1475\n",
      "[   1086] D_loss: 0.2420 / G_loss: 3.3703\n",
      "[   1087] D_loss: 0.1055 / G_loss: 3.5334\n",
      "[   1088] D_loss: 0.2347 / G_loss: 3.5474\n",
      "[   1089] D_loss: 0.2230 / G_loss: 3.5280\n",
      "[   1090] D_loss: 0.1083 / G_loss: 3.5503\n",
      "[   1091] D_loss: 0.1509 / G_loss: 3.3960\n",
      "[   1092] D_loss: 0.1780 / G_loss: 3.3169\n",
      "[   1093] D_loss: 0.1375 / G_loss: 3.5260\n",
      "[   1094] D_loss: 0.1392 / G_loss: 3.4445\n",
      "[   1095] D_loss: 0.3890 / G_loss: 3.8428\n",
      "[   1096] D_loss: 0.2518 / G_loss: 3.3286\n",
      "[   1097] D_loss: 0.2611 / G_loss: 3.6335\n",
      "[   1098] D_loss: 0.1752 / G_loss: 3.3779\n",
      "[   1099] D_loss: 0.2744 / G_loss: 3.2560\n",
      "[   1100] D_loss: 0.1757 / G_loss: 3.4365\n",
      "[   1101] D_loss: 0.1697 / G_loss: 3.2902\n",
      "[   1102] D_loss: 0.1698 / G_loss: 3.8928\n",
      "[   1103] D_loss: 0.1281 / G_loss: 3.3674\n",
      "[   1104] D_loss: 0.1444 / G_loss: 3.4761\n",
      "[   1105] D_loss: 0.2994 / G_loss: 3.5315\n",
      "[   1106] D_loss: 0.3658 / G_loss: 3.3791\n",
      "[   1107] D_loss: 0.1837 / G_loss: 3.3237\n",
      "[   1108] D_loss: 0.1737 / G_loss: 3.2456\n",
      "[   1109] D_loss: 0.1017 / G_loss: 3.1405\n",
      "[   1110] D_loss: 0.2241 / G_loss: 3.0701\n",
      "[   1111] D_loss: 0.1603 / G_loss: 3.1887\n",
      "[   1112] D_loss: 0.1802 / G_loss: 3.4484\n",
      "[   1113] D_loss: 0.1892 / G_loss: 3.0910\n",
      "[   1114] D_loss: 0.3157 / G_loss: 3.4818\n",
      "[   1115] D_loss: 0.2929 / G_loss: 3.2721\n",
      "[   1116] D_loss: 0.3180 / G_loss: 3.4638\n",
      "[   1117] D_loss: 0.1535 / G_loss: 3.4888\n",
      "[   1118] D_loss: 0.1937 / G_loss: 3.5209\n",
      "[   1119] D_loss: 0.2727 / G_loss: 3.6339\n",
      "[   1120] D_loss: 0.1413 / G_loss: 3.2951\n",
      "[   1121] D_loss: 0.4347 / G_loss: 3.5161\n",
      "[   1122] D_loss: 0.1068 / G_loss: 3.3657\n",
      "[   1123] D_loss: 0.1418 / G_loss: 3.4348\n",
      "[   1124] D_loss: 0.3654 / G_loss: 3.5498\n",
      "[   1125] D_loss: 0.2274 / G_loss: 3.4210\n",
      "[   1126] D_loss: 0.0763 / G_loss: 3.0535\n",
      "[   1127] D_loss: 0.1178 / G_loss: 3.5585\n",
      "[   1128] D_loss: 0.3967 / G_loss: 3.6754\n",
      "[   1129] D_loss: 0.1578 / G_loss: 3.5246\n",
      "[   1130] D_loss: 0.1174 / G_loss: 4.0421\n",
      "[   1131] D_loss: 0.1259 / G_loss: 3.4086\n",
      "[   1132] D_loss: 0.2101 / G_loss: 3.5497\n",
      "[   1133] D_loss: 0.1674 / G_loss: 3.6116\n",
      "[   1134] D_loss: 0.2422 / G_loss: 4.0948\n",
      "[   1135] D_loss: 0.1933 / G_loss: 3.2877\n",
      "[   1136] D_loss: 0.2086 / G_loss: 3.4645\n",
      "[   1137] D_loss: 0.0722 / G_loss: 3.3953\n",
      "[   1138] D_loss: 0.2535 / G_loss: 3.5500\n",
      "[   1139] D_loss: 0.0940 / G_loss: 3.6073\n",
      "[   1140] D_loss: 0.1575 / G_loss: 3.8361\n",
      "[   1141] D_loss: 0.1648 / G_loss: 3.4171\n",
      "[   1142] D_loss: 0.1973 / G_loss: 3.1701\n",
      "[   1143] D_loss: 0.1255 / G_loss: 2.8468\n",
      "[   1144] D_loss: 0.2990 / G_loss: 3.5245\n",
      "[   1145] D_loss: 0.1460 / G_loss: 3.5368\n",
      "[   1146] D_loss: 0.1973 / G_loss: 3.5946\n",
      "[   1147] D_loss: 0.3788 / G_loss: 3.1590\n",
      "[   1148] D_loss: 0.2914 / G_loss: 3.4907\n",
      "[   1149] D_loss: 0.2258 / G_loss: 4.0841\n",
      "[   1150] D_loss: 0.1488 / G_loss: 2.9594\n",
      "[   1151] D_loss: 0.1525 / G_loss: 3.5994\n",
      "[   1152] D_loss: 0.2094 / G_loss: 3.5816\n",
      "[   1153] D_loss: 0.1363 / G_loss: 3.7303\n",
      "[   1154] D_loss: 0.1331 / G_loss: 4.0596\n",
      "[   1155] D_loss: 0.1649 / G_loss: 3.6810\n",
      "[   1156] D_loss: 0.2340 / G_loss: 3.8599\n",
      "[   1157] D_loss: 0.0840 / G_loss: 4.2671\n",
      "[   1158] D_loss: 0.2324 / G_loss: 4.1594\n",
      "[   1159] D_loss: 0.1333 / G_loss: 3.6610\n",
      "[   1160] D_loss: 0.1734 / G_loss: 3.5602\n",
      "[   1161] D_loss: 0.2592 / G_loss: 3.2329\n",
      "[   1162] D_loss: 0.2172 / G_loss: 3.5412\n",
      "[   1163] D_loss: 0.2150 / G_loss: 3.5342\n",
      "[   1164] D_loss: 0.0589 / G_loss: 4.2738\n",
      "[   1165] D_loss: 0.1526 / G_loss: 3.5719\n",
      "[   1166] D_loss: 0.0809 / G_loss: 5.0656\n",
      "[   1167] D_loss: 0.1939 / G_loss: 3.7274\n",
      "[   1168] D_loss: 0.2941 / G_loss: 3.8456\n",
      "[   1169] D_loss: 0.2063 / G_loss: 3.6829\n",
      "[   1170] D_loss: 0.2251 / G_loss: 3.1839\n",
      "[   1171] D_loss: 0.1062 / G_loss: 3.7713\n",
      "[   1172] D_loss: 0.0894 / G_loss: 3.3635\n",
      "[   1173] D_loss: 0.2498 / G_loss: 3.1049\n",
      "[   1174] D_loss: 0.1495 / G_loss: 3.2632\n",
      "[   1175] D_loss: 0.1784 / G_loss: 3.1163\n",
      "[   1176] D_loss: 0.3305 / G_loss: 3.3058\n",
      "[   1177] D_loss: 0.2484 / G_loss: 3.4966\n",
      "[   1178] D_loss: 0.3282 / G_loss: 3.3397\n",
      "[   1179] D_loss: 0.1266 / G_loss: 3.7676\n",
      "[   1180] D_loss: 0.0891 / G_loss: 3.4478\n",
      "[   1181] D_loss: 0.1570 / G_loss: 3.2799\n",
      "[   1182] D_loss: 0.1989 / G_loss: 3.2424\n",
      "[   1183] D_loss: 0.3627 / G_loss: 3.6306\n",
      "[   1184] D_loss: 0.1781 / G_loss: 3.6678\n",
      "[   1185] D_loss: 0.1490 / G_loss: 3.6753\n",
      "[   1186] D_loss: 0.1018 / G_loss: 3.0670\n",
      "[   1187] D_loss: 0.2681 / G_loss: 4.0392\n",
      "[   1188] D_loss: 0.2468 / G_loss: 2.9740\n",
      "[   1189] D_loss: 0.1817 / G_loss: 3.1574\n",
      "[   1190] D_loss: 0.2939 / G_loss: 3.4245\n",
      "[   1191] D_loss: 0.1298 / G_loss: 3.6019\n",
      "[   1192] D_loss: 0.4011 / G_loss: 3.3832\n",
      "[   1193] D_loss: 0.0949 / G_loss: 3.2794\n",
      "[   1194] D_loss: 0.1111 / G_loss: 3.6652\n",
      "[   1195] D_loss: 0.1372 / G_loss: 3.7525\n",
      "[   1196] D_loss: 0.1706 / G_loss: 3.9756\n",
      "[   1197] D_loss: 0.1295 / G_loss: 3.3241\n",
      "[   1198] D_loss: 0.1799 / G_loss: 3.4333\n",
      "[   1199] D_loss: 0.2345 / G_loss: 3.4693\n",
      "[   1200] D_loss: 0.2083 / G_loss: 3.5049\n",
      "[   1201] D_loss: 0.1290 / G_loss: 3.8090\n",
      "[   1202] D_loss: 0.2139 / G_loss: 3.7145\n",
      "[   1203] D_loss: 0.1481 / G_loss: 3.9515\n",
      "[   1204] D_loss: 0.1935 / G_loss: 3.4822\n",
      "[   1205] D_loss: 0.2657 / G_loss: 3.3767\n",
      "[   1206] D_loss: 0.1839 / G_loss: 3.4066\n",
      "[   1207] D_loss: 0.2542 / G_loss: 3.8703\n",
      "[   1208] D_loss: 0.2291 / G_loss: 3.8593\n",
      "[   1209] D_loss: 0.4677 / G_loss: 3.4607\n",
      "[   1210] D_loss: 0.0985 / G_loss: 3.2651\n",
      "[   1211] D_loss: 0.0914 / G_loss: 3.4409\n",
      "[   1212] D_loss: 0.3146 / G_loss: 3.2823\n",
      "[   1213] D_loss: 0.2408 / G_loss: 3.9395\n",
      "[   1214] D_loss: 0.1564 / G_loss: 3.5769\n",
      "[   1215] D_loss: 0.3234 / G_loss: 3.0070\n",
      "[   1216] D_loss: 0.1813 / G_loss: 3.5812\n",
      "[   1217] D_loss: 0.2181 / G_loss: 2.9907\n",
      "[   1218] D_loss: 0.0889 / G_loss: 3.2897\n",
      "[   1219] D_loss: 0.2884 / G_loss: 3.9849\n",
      "[   1220] D_loss: 0.3228 / G_loss: 3.5336\n",
      "[   1221] D_loss: 0.1010 / G_loss: 3.3554\n",
      "[   1222] D_loss: 0.2637 / G_loss: 3.9549\n",
      "[   1223] D_loss: 0.0752 / G_loss: 3.3916\n",
      "[   1224] D_loss: 0.1614 / G_loss: 3.5493\n",
      "[   1225] D_loss: 0.1997 / G_loss: 3.5303\n",
      "[   1226] D_loss: 0.1990 / G_loss: 3.5457\n",
      "[   1227] D_loss: 0.1932 / G_loss: 3.7799\n",
      "[   1228] D_loss: 0.0692 / G_loss: 4.4268\n",
      "[   1229] D_loss: 0.3208 / G_loss: 3.6809\n",
      "[   1230] D_loss: 0.1358 / G_loss: 3.1540\n",
      "[   1231] D_loss: 0.1569 / G_loss: 3.6548\n",
      "[   1232] D_loss: 0.2365 / G_loss: 3.8202\n",
      "[   1233] D_loss: 0.1348 / G_loss: 3.4067\n",
      "[   1234] D_loss: 0.3042 / G_loss: 3.5592\n",
      "[   1235] D_loss: 0.0815 / G_loss: 3.4761\n",
      "[   1236] D_loss: 0.2183 / G_loss: 3.6183\n",
      "[   1237] D_loss: 0.1243 / G_loss: 4.0321\n",
      "[   1238] D_loss: 0.1508 / G_loss: 3.6442\n",
      "[   1239] D_loss: 0.1413 / G_loss: 3.5956\n",
      "[   1240] D_loss: 0.2612 / G_loss: 3.3477\n",
      "[   1241] D_loss: 0.0865 / G_loss: 4.1690\n",
      "[   1242] D_loss: 0.0777 / G_loss: 3.3021\n",
      "[   1243] D_loss: 0.2698 / G_loss: 3.9417\n",
      "[   1244] D_loss: 0.0547 / G_loss: 3.6658\n",
      "[   1245] D_loss: 0.0873 / G_loss: 3.6942\n",
      "[   1246] D_loss: 0.1813 / G_loss: 3.7044\n",
      "[   1247] D_loss: 0.1370 / G_loss: 3.5366\n",
      "[   1248] D_loss: 0.1033 / G_loss: 3.5612\n",
      "[   1249] D_loss: 0.1649 / G_loss: 3.5170\n",
      "[   1250] D_loss: 0.0514 / G_loss: 3.8013\n",
      "[   1251] D_loss: 0.2138 / G_loss: 3.5778\n",
      "[   1252] D_loss: 0.2037 / G_loss: 3.7770\n",
      "[   1253] D_loss: 0.1740 / G_loss: 3.8513\n",
      "[   1254] D_loss: 0.1462 / G_loss: 3.6380\n",
      "[   1255] D_loss: 0.2394 / G_loss: 3.8262\n",
      "[   1256] D_loss: 0.2541 / G_loss: 3.5787\n",
      "[   1257] D_loss: 0.0770 / G_loss: 3.5644\n",
      "[   1258] D_loss: 0.1914 / G_loss: 3.6657\n",
      "[   1259] D_loss: 0.0402 / G_loss: 3.3576\n",
      "[   1260] D_loss: 0.2300 / G_loss: 3.6090\n",
      "[   1261] D_loss: 0.2724 / G_loss: 3.8727\n",
      "[   1262] D_loss: 0.0869 / G_loss: 3.7955\n",
      "[   1263] D_loss: 0.1759 / G_loss: 4.3891\n",
      "[   1264] D_loss: 0.2412 / G_loss: 3.6597\n",
      "[   1265] D_loss: 0.1277 / G_loss: 3.4022\n",
      "[   1266] D_loss: 0.1649 / G_loss: 4.2882\n",
      "[   1267] D_loss: 0.3670 / G_loss: 5.2109\n",
      "[   1268] D_loss: 0.3426 / G_loss: 3.5779\n",
      "[   1269] D_loss: 0.2964 / G_loss: 3.1139\n",
      "[   1270] D_loss: 0.3834 / G_loss: 3.0283\n",
      "[   1271] D_loss: 0.0387 / G_loss: 3.2105\n",
      "[   1272] D_loss: 0.1542 / G_loss: 3.3710\n",
      "[   1273] D_loss: 0.1773 / G_loss: 3.4266\n",
      "[   1274] D_loss: 0.1105 / G_loss: 3.4364\n",
      "[   1275] D_loss: 0.2149 / G_loss: 3.2643\n",
      "[   1276] D_loss: 0.1786 / G_loss: 3.2885\n",
      "[   1277] D_loss: 0.0822 / G_loss: 3.6066\n",
      "[   1278] D_loss: 0.4356 / G_loss: 3.3058\n",
      "[   1279] D_loss: 0.1424 / G_loss: 3.5545\n",
      "[   1280] D_loss: 0.1945 / G_loss: 3.3854\n",
      "[   1281] D_loss: 0.1368 / G_loss: 3.8039\n",
      "[   1282] D_loss: 0.2451 / G_loss: 3.5015\n",
      "[   1283] D_loss: 0.1649 / G_loss: 3.7177\n",
      "[   1284] D_loss: 0.1240 / G_loss: 3.6241\n",
      "[   1285] D_loss: 0.1433 / G_loss: 3.5420\n",
      "[   1286] D_loss: 0.2681 / G_loss: 3.3245\n",
      "[   1287] D_loss: 0.3419 / G_loss: 3.2245\n",
      "[   1288] D_loss: 0.1507 / G_loss: 3.1818\n",
      "[   1289] D_loss: 0.1418 / G_loss: 3.6870\n",
      "[   1290] D_loss: 0.1010 / G_loss: 3.5595\n",
      "[   1291] D_loss: 0.1596 / G_loss: 3.5418\n",
      "[   1292] D_loss: 0.2775 / G_loss: 3.4051\n",
      "[   1293] D_loss: 0.1175 / G_loss: 4.1088\n",
      "[   1294] D_loss: 0.2646 / G_loss: 3.5971\n",
      "[   1295] D_loss: 0.2106 / G_loss: 3.1993\n",
      "[   1296] D_loss: 0.3432 / G_loss: 3.2148\n",
      "[   1297] D_loss: 0.1165 / G_loss: 3.6772\n",
      "[   1298] D_loss: 0.0869 / G_loss: 3.4179\n",
      "[   1299] D_loss: 0.1027 / G_loss: 3.4503\n",
      "[   1300] D_loss: 0.1340 / G_loss: 3.7371\n",
      "[   1301] D_loss: 0.0785 / G_loss: 3.7577\n",
      "[   1302] D_loss: 0.0795 / G_loss: 3.4259\n",
      "[   1303] D_loss: 0.2980 / G_loss: 3.4059\n",
      "[   1304] D_loss: 0.1091 / G_loss: 3.4941\n",
      "[   1305] D_loss: 0.1871 / G_loss: 3.7003\n",
      "[   1306] D_loss: 0.1659 / G_loss: 3.3498\n",
      "[   1307] D_loss: 0.1206 / G_loss: 3.6210\n",
      "[   1308] D_loss: 0.2084 / G_loss: 3.1510\n",
      "[   1309] D_loss: 0.3559 / G_loss: 3.3147\n",
      "[   1310] D_loss: 0.2387 / G_loss: 3.8960\n",
      "[   1311] D_loss: 0.1920 / G_loss: 3.7763\n",
      "[   1312] D_loss: 0.0858 / G_loss: 3.2635\n",
      "[   1313] D_loss: 0.2404 / G_loss: 3.2135\n",
      "[   1314] D_loss: 0.3518 / G_loss: 3.8603\n",
      "[   1315] D_loss: 0.0368 / G_loss: 3.4500\n",
      "[   1316] D_loss: 0.2289 / G_loss: 3.8231\n",
      "[   1317] D_loss: 0.1831 / G_loss: 3.5610\n",
      "[   1318] D_loss: 0.1475 / G_loss: 3.4987\n",
      "[   1319] D_loss: 0.2644 / G_loss: 3.8377\n",
      "[   1320] D_loss: 0.0828 / G_loss: 3.3439\n",
      "[   1321] D_loss: 0.4449 / G_loss: 3.7422\n",
      "[   1322] D_loss: 0.2883 / G_loss: 3.4843\n",
      "[   1323] D_loss: 0.2403 / G_loss: 3.1611\n",
      "[   1324] D_loss: 0.2077 / G_loss: 3.6866\n",
      "[   1325] D_loss: 0.2800 / G_loss: 3.6598\n",
      "[   1326] D_loss: 0.0847 / G_loss: 3.6207\n",
      "[   1327] D_loss: 0.2130 / G_loss: 3.2649\n",
      "[   1328] D_loss: 0.1017 / G_loss: 3.8147\n",
      "[   1329] D_loss: 0.1473 / G_loss: 3.4623\n",
      "[   1330] D_loss: 0.1861 / G_loss: 3.5954\n",
      "[   1331] D_loss: 0.2308 / G_loss: 3.5668\n",
      "[   1332] D_loss: 0.1607 / G_loss: 3.4572\n",
      "[   1333] D_loss: 0.1130 / G_loss: 3.4417\n",
      "[   1334] D_loss: 0.1080 / G_loss: 3.5331\n",
      "[   1335] D_loss: 0.1402 / G_loss: 3.3051\n",
      "[   1336] D_loss: 0.1964 / G_loss: 3.7648\n",
      "[   1337] D_loss: 0.2170 / G_loss: 3.3921\n",
      "[   1338] D_loss: 0.0820 / G_loss: 3.5170\n",
      "[   1339] D_loss: 0.2175 / G_loss: 3.6798\n",
      "[   1340] D_loss: 0.1212 / G_loss: 3.4765\n",
      "[   1341] D_loss: 0.1319 / G_loss: 3.8071\n",
      "[   1342] D_loss: 0.1186 / G_loss: 3.3322\n",
      "[   1343] D_loss: 0.1917 / G_loss: 3.5639\n",
      "[   1344] D_loss: 0.1564 / G_loss: 3.4066\n",
      "[   1345] D_loss: 0.1128 / G_loss: 3.5252\n",
      "[   1346] D_loss: 0.2839 / G_loss: 3.2140\n",
      "[   1347] D_loss: 0.1187 / G_loss: 3.7304\n",
      "[   1348] D_loss: 0.2562 / G_loss: 3.1553\n",
      "[   1349] D_loss: 0.2468 / G_loss: 3.8833\n",
      "[   1350] D_loss: 0.1578 / G_loss: 3.5145\n",
      "[   1351] D_loss: 0.3340 / G_loss: 3.6672\n",
      "[   1352] D_loss: 0.2552 / G_loss: 3.7184\n",
      "[   1353] D_loss: 0.1676 / G_loss: 3.8865\n",
      "[   1354] D_loss: 0.2709 / G_loss: 3.7269\n",
      "[   1355] D_loss: 0.1059 / G_loss: 3.6059\n",
      "[   1356] D_loss: 0.2488 / G_loss: 4.1491\n",
      "[   1357] D_loss: 0.2633 / G_loss: 3.5608\n",
      "[   1358] D_loss: 0.2616 / G_loss: 3.6703\n",
      "[   1359] D_loss: 0.1816 / G_loss: 3.4324\n",
      "[   1360] D_loss: 0.2801 / G_loss: 3.6893\n",
      "[   1361] D_loss: 0.1965 / G_loss: 3.5491\n",
      "[   1362] D_loss: 0.1397 / G_loss: 3.6692\n",
      "[   1363] D_loss: 0.0440 / G_loss: 3.6576\n",
      "[   1364] D_loss: 0.3446 / G_loss: 3.4851\n",
      "[   1365] D_loss: 0.1713 / G_loss: 3.3631\n",
      "[   1366] D_loss: 0.2738 / G_loss: 3.4007\n",
      "[   1367] D_loss: 0.1673 / G_loss: 3.5454\n",
      "[   1368] D_loss: 0.1008 / G_loss: 4.0488\n",
      "[   1369] D_loss: 0.1883 / G_loss: 3.0964\n",
      "[   1370] D_loss: 0.0700 / G_loss: 3.6891\n",
      "[   1371] D_loss: 0.1129 / G_loss: 3.5073\n",
      "[   1372] D_loss: 0.2166 / G_loss: 3.6521\n",
      "[   1373] D_loss: 0.2224 / G_loss: 3.7682\n",
      "[   1374] D_loss: 0.2637 / G_loss: 3.9202\n",
      "[   1375] D_loss: 0.2764 / G_loss: 4.1769\n",
      "[   1376] D_loss: 0.0341 / G_loss: 3.5830\n",
      "[   1377] D_loss: 0.2038 / G_loss: 3.4490\n",
      "[   1378] D_loss: 0.1345 / G_loss: 4.2757\n",
      "[   1379] D_loss: 0.1413 / G_loss: 3.8360\n",
      "[   1380] D_loss: 0.2054 / G_loss: 3.8083\n",
      "[   1381] D_loss: 0.0509 / G_loss: 4.2812\n",
      "[   1382] D_loss: 0.1463 / G_loss: 3.1511\n",
      "[   1383] D_loss: 0.1737 / G_loss: 3.6160\n",
      "[   1384] D_loss: 0.2218 / G_loss: 3.3624\n",
      "[   1385] D_loss: 0.1750 / G_loss: 3.6952\n",
      "[   1386] D_loss: 0.2655 / G_loss: 3.9603\n",
      "[   1387] D_loss: 0.1972 / G_loss: 3.3522\n",
      "[   1388] D_loss: 0.2234 / G_loss: 3.4665\n",
      "[   1389] D_loss: 0.2937 / G_loss: 3.5111\n",
      "[   1390] D_loss: 0.3285 / G_loss: 3.5768\n",
      "[   1391] D_loss: 0.1145 / G_loss: 4.0132\n",
      "[   1392] D_loss: 0.0457 / G_loss: 3.4305\n",
      "[   1393] D_loss: 0.1816 / G_loss: 3.8415\n",
      "[   1394] D_loss: 0.2476 / G_loss: 3.9771\n",
      "[   1395] D_loss: 0.1870 / G_loss: 3.6419\n",
      "[   1396] D_loss: 0.0983 / G_loss: 3.3609\n",
      "[   1397] D_loss: 0.0674 / G_loss: 3.8768\n",
      "[   1398] D_loss: 0.1402 / G_loss: 3.8978\n",
      "[   1399] D_loss: 0.1525 / G_loss: 3.4832\n",
      "[   1400] D_loss: 0.1300 / G_loss: 3.5549\n",
      "[   1401] D_loss: 0.0822 / G_loss: 4.1093\n",
      "[   1402] D_loss: 0.2022 / G_loss: 3.8072\n",
      "[   1403] D_loss: 0.2338 / G_loss: 3.6779\n",
      "[   1404] D_loss: 0.1545 / G_loss: 3.8158\n",
      "[   1405] D_loss: 0.1092 / G_loss: 3.7755\n",
      "[   1406] D_loss: 0.0524 / G_loss: 3.4888\n",
      "[   1407] D_loss: 0.3630 / G_loss: 3.6839\n",
      "[   1408] D_loss: 0.0758 / G_loss: 3.0993\n",
      "[   1409] D_loss: 0.3250 / G_loss: 3.5026\n",
      "[   1410] D_loss: 0.1683 / G_loss: 3.5058\n",
      "[   1411] D_loss: 0.2616 / G_loss: 3.2530\n",
      "[   1412] D_loss: 0.2120 / G_loss: 3.4545\n",
      "[   1413] D_loss: 0.0472 / G_loss: 3.6712\n",
      "[   1414] D_loss: 0.0999 / G_loss: 3.4735\n",
      "[   1415] D_loss: 0.4509 / G_loss: 3.8502\n",
      "[   1416] D_loss: 0.1588 / G_loss: 3.9160\n",
      "[   1417] D_loss: 0.1927 / G_loss: 3.6245\n",
      "[   1418] D_loss: 0.2811 / G_loss: 4.1085\n",
      "[   1419] D_loss: 0.3255 / G_loss: 3.7420\n",
      "[   1420] D_loss: 0.2743 / G_loss: 3.7366\n",
      "[   1421] D_loss: 0.1636 / G_loss: 3.7687\n",
      "[   1422] D_loss: 0.0845 / G_loss: 3.6594\n",
      "[   1423] D_loss: 0.2386 / G_loss: 3.6311\n",
      "[   1424] D_loss: 0.2804 / G_loss: 3.4169\n",
      "[   1425] D_loss: 0.1155 / G_loss: 3.7292\n",
      "[   1426] D_loss: 0.1219 / G_loss: 3.9442\n",
      "[   1427] D_loss: 0.2128 / G_loss: 3.6963\n",
      "[   1428] D_loss: 0.2178 / G_loss: 3.6763\n",
      "[   1429] D_loss: 0.2003 / G_loss: 3.9105\n",
      "[   1430] D_loss: 0.1429 / G_loss: 3.3371\n",
      "[   1431] D_loss: 0.1184 / G_loss: 3.6505\n",
      "[   1432] D_loss: 0.1652 / G_loss: 3.5289\n",
      "[   1433] D_loss: 0.1884 / G_loss: 3.7368\n",
      "[   1434] D_loss: 0.1007 / G_loss: 3.7906\n",
      "[   1435] D_loss: 0.1331 / G_loss: 3.4723\n",
      "[   1436] D_loss: 0.0861 / G_loss: 3.6026\n",
      "[   1437] D_loss: 0.1268 / G_loss: 4.0107\n",
      "[   1438] D_loss: 0.1479 / G_loss: 3.6861\n",
      "[   1439] D_loss: 0.1848 / G_loss: 3.4664\n",
      "[   1440] D_loss: 0.2168 / G_loss: 3.8247\n",
      "[   1441] D_loss: 0.2167 / G_loss: 3.8496\n",
      "[   1442] D_loss: 0.1597 / G_loss: 3.1660\n",
      "[   1443] D_loss: 0.0963 / G_loss: 3.6989\n",
      "[   1444] D_loss: 0.3092 / G_loss: 3.4972\n",
      "[   1445] D_loss: 0.2249 / G_loss: 3.8592\n",
      "[   1446] D_loss: 0.3053 / G_loss: 4.1560\n",
      "[   1447] D_loss: 0.1935 / G_loss: 4.0125\n",
      "[   1448] D_loss: 0.1596 / G_loss: 3.4016\n",
      "[   1449] D_loss: 0.2136 / G_loss: 3.9230\n",
      "[   1450] D_loss: 0.1422 / G_loss: 3.9699\n",
      "[   1451] D_loss: 0.1755 / G_loss: 3.9255\n",
      "[   1452] D_loss: 0.1562 / G_loss: 3.7828\n",
      "[   1453] D_loss: 0.1037 / G_loss: 3.4402\n",
      "[   1454] D_loss: 0.2037 / G_loss: 3.7742\n",
      "[   1455] D_loss: 0.1653 / G_loss: 3.6338\n",
      "[   1456] D_loss: 0.1278 / G_loss: 3.9489\n",
      "[   1457] D_loss: 0.1458 / G_loss: 3.9149\n",
      "[   1458] D_loss: 0.1885 / G_loss: 4.2370\n",
      "[   1459] D_loss: 0.1938 / G_loss: 3.9767\n",
      "[   1460] D_loss: 0.1513 / G_loss: 3.4282\n",
      "[   1461] D_loss: 0.1188 / G_loss: 3.9797\n",
      "[   1462] D_loss: 0.2375 / G_loss: 3.8364\n",
      "[   1463] D_loss: 0.2317 / G_loss: 3.4241\n",
      "[   1464] D_loss: 0.2083 / G_loss: 3.5673\n",
      "[   1465] D_loss: 0.1259 / G_loss: 3.8207\n",
      "[   1466] D_loss: 0.1038 / G_loss: 3.7213\n",
      "[   1467] D_loss: 0.0973 / G_loss: 3.5220\n",
      "[   1468] D_loss: 0.1080 / G_loss: 4.0472\n",
      "[   1469] D_loss: 0.2613 / G_loss: 3.7591\n",
      "[   1470] D_loss: 0.1680 / G_loss: 3.8614\n",
      "[   1471] D_loss: 0.2512 / G_loss: 3.2092\n",
      "[   1472] D_loss: 0.0642 / G_loss: 3.4591\n",
      "[   1473] D_loss: 0.0642 / G_loss: 3.2806\n",
      "[   1474] D_loss: 0.1163 / G_loss: 3.8389\n",
      "[   1475] D_loss: 0.1070 / G_loss: 3.8632\n",
      "[   1476] D_loss: 0.1814 / G_loss: 3.5294\n",
      "[   1477] D_loss: 0.0819 / G_loss: 3.2639\n",
      "[   1478] D_loss: 0.1432 / G_loss: 3.6039\n",
      "[   1479] D_loss: 0.3550 / G_loss: 3.6417\n",
      "[   1480] D_loss: 0.0905 / G_loss: 3.1631\n",
      "[   1481] D_loss: 0.3091 / G_loss: 3.7226\n",
      "[   1482] D_loss: 0.1406 / G_loss: 3.4403\n",
      "[   1483] D_loss: 0.1737 / G_loss: 3.8865\n",
      "[   1484] D_loss: 0.1682 / G_loss: 3.6482\n",
      "[   1485] D_loss: 0.1153 / G_loss: 3.5148\n",
      "[   1486] D_loss: 0.2147 / G_loss: 3.8931\n",
      "[   1487] D_loss: 0.2737 / G_loss: 3.7085\n",
      "[   1488] D_loss: 0.5905 / G_loss: 4.3460\n",
      "[   1489] D_loss: 0.1825 / G_loss: 3.3569\n",
      "[   1490] D_loss: 0.2524 / G_loss: 3.6781\n",
      "[   1491] D_loss: 0.1448 / G_loss: 3.6609\n",
      "[   1492] D_loss: 0.2206 / G_loss: 3.7383\n",
      "[   1493] D_loss: 0.1900 / G_loss: 3.3431\n",
      "[   1494] D_loss: 0.2567 / G_loss: 3.4482\n",
      "[   1495] D_loss: 0.2590 / G_loss: 3.5989\n",
      "[   1496] D_loss: 0.1261 / G_loss: 3.6084\n",
      "[   1497] D_loss: 0.1227 / G_loss: 3.4403\n",
      "[   1498] D_loss: 0.0298 / G_loss: 4.1260\n",
      "[   1499] D_loss: 0.2851 / G_loss: 3.8211\n",
      "[   1500] D_loss: 0.1750 / G_loss: 3.8832\n",
      "[   1501] D_loss: 0.1657 / G_loss: 3.7058\n",
      "[   1502] D_loss: 0.0540 / G_loss: 4.0033\n",
      "[   1503] D_loss: 0.1221 / G_loss: 3.7005\n",
      "[   1504] D_loss: 0.1391 / G_loss: 3.7042\n",
      "[   1505] D_loss: 0.1088 / G_loss: 3.2214\n",
      "[   1506] D_loss: 0.0997 / G_loss: 3.6601\n",
      "[   1507] D_loss: 0.1099 / G_loss: 3.5856\n",
      "[   1508] D_loss: 0.3256 / G_loss: 3.7621\n",
      "[   1509] D_loss: 0.2149 / G_loss: 3.8601\n",
      "[   1510] D_loss: 0.2545 / G_loss: 4.0855\n",
      "[   1511] D_loss: 0.2057 / G_loss: 3.4787\n",
      "[   1512] D_loss: 0.3385 / G_loss: 3.7110\n",
      "[   1513] D_loss: 0.3242 / G_loss: 3.3060\n",
      "[   1514] D_loss: 0.1270 / G_loss: 3.4661\n",
      "[   1515] D_loss: 0.2762 / G_loss: 3.7276\n",
      "[   1516] D_loss: 0.1890 / G_loss: 3.7171\n",
      "[   1517] D_loss: 0.2133 / G_loss: 3.6803\n",
      "[   1518] D_loss: 0.1330 / G_loss: 3.3913\n",
      "[   1519] D_loss: 0.1718 / G_loss: 3.8112\n",
      "[   1520] D_loss: 0.1185 / G_loss: 3.6593\n",
      "[   1521] D_loss: 0.0838 / G_loss: 3.9683\n",
      "[   1522] D_loss: 0.2005 / G_loss: 3.3430\n",
      "[   1523] D_loss: 0.3305 / G_loss: 3.9548\n",
      "[   1524] D_loss: 0.1037 / G_loss: 3.6557\n",
      "[   1525] D_loss: 0.2036 / G_loss: 4.3677\n",
      "[   1526] D_loss: 0.1750 / G_loss: 3.6502\n",
      "[   1527] D_loss: 0.3079 / G_loss: 3.6347\n",
      "[   1528] D_loss: 0.2961 / G_loss: 3.5084\n",
      "[   1529] D_loss: 0.3559 / G_loss: 3.4124\n",
      "[   1530] D_loss: 0.1084 / G_loss: 3.3709\n",
      "[   1531] D_loss: 0.1900 / G_loss: 3.7576\n",
      "[   1532] D_loss: 0.1933 / G_loss: 3.9882\n",
      "[   1533] D_loss: 0.0966 / G_loss: 3.8580\n",
      "[   1534] D_loss: 0.2602 / G_loss: 3.9159\n",
      "[   1535] D_loss: 0.1427 / G_loss: 3.8357\n",
      "[   1536] D_loss: 0.2680 / G_loss: 3.4231\n",
      "[   1537] D_loss: 0.1020 / G_loss: 3.6945\n",
      "[   1538] D_loss: 0.1170 / G_loss: 3.5451\n",
      "[   1539] D_loss: 0.1834 / G_loss: 3.6568\n",
      "[   1540] D_loss: 0.2052 / G_loss: 4.7606\n",
      "[   1541] D_loss: 0.0837 / G_loss: 3.5907\n",
      "[   1542] D_loss: 0.0776 / G_loss: 3.4934\n",
      "[   1543] D_loss: 0.1606 / G_loss: 3.4561\n",
      "[   1544] D_loss: 0.1958 / G_loss: 3.3489\n",
      "[   1545] D_loss: 0.2903 / G_loss: 3.4603\n",
      "[   1546] D_loss: 0.2688 / G_loss: 2.9938\n",
      "[   1547] D_loss: 0.2398 / G_loss: 3.7383\n",
      "[   1548] D_loss: 0.2206 / G_loss: 3.5192\n",
      "[   1549] D_loss: 0.3236 / G_loss: 3.5956\n",
      "[   1550] D_loss: 0.2710 / G_loss: 3.7680\n",
      "[   1551] D_loss: 0.0402 / G_loss: 3.5118\n",
      "[   1552] D_loss: 0.1427 / G_loss: 3.4077\n",
      "[   1553] D_loss: 0.2346 / G_loss: 3.5823\n",
      "[   1554] D_loss: 0.1227 / G_loss: 3.2791\n",
      "[   1555] D_loss: 0.1632 / G_loss: 3.4281\n",
      "[   1556] D_loss: 0.1251 / G_loss: 3.6855\n",
      "[   1557] D_loss: 0.2132 / G_loss: 3.6115\n",
      "[   1558] D_loss: 0.1376 / G_loss: 3.3339\n",
      "[   1559] D_loss: 0.2712 / G_loss: 3.6895\n",
      "[   1560] D_loss: 0.1580 / G_loss: 3.7537\n",
      "[   1561] D_loss: 0.1705 / G_loss: 3.5597\n",
      "[   1562] D_loss: 0.1651 / G_loss: 3.6479\n",
      "[   1563] D_loss: 0.1265 / G_loss: 3.4498\n",
      "[   1564] D_loss: 0.2385 / G_loss: 3.2152\n",
      "[   1565] D_loss: 0.1396 / G_loss: 3.4365\n",
      "[   1566] D_loss: 0.1868 / G_loss: 3.7824\n",
      "[   1567] D_loss: 0.2210 / G_loss: 3.2512\n",
      "[   1568] D_loss: 0.1472 / G_loss: 3.9153\n",
      "[   1569] D_loss: 0.1764 / G_loss: 4.1028\n",
      "[   1570] D_loss: 0.2016 / G_loss: 3.9440\n",
      "[   1571] D_loss: 0.2303 / G_loss: 3.2822\n",
      "[   1572] D_loss: 0.3244 / G_loss: 3.7376\n",
      "[   1573] D_loss: 0.2028 / G_loss: 3.5558\n",
      "[   1574] D_loss: 0.2293 / G_loss: 3.4162\n",
      "[   1575] D_loss: 0.2069 / G_loss: 3.1186\n",
      "[   1576] D_loss: 0.1662 / G_loss: 3.4352\n",
      "[   1577] D_loss: 0.2396 / G_loss: 3.9111\n",
      "[   1578] D_loss: 0.2896 / G_loss: 3.3943\n",
      "[   1579] D_loss: 0.2139 / G_loss: 3.7780\n",
      "[   1580] D_loss: 0.1662 / G_loss: 3.6480\n",
      "[   1581] D_loss: 0.2220 / G_loss: 3.3883\n",
      "[   1582] D_loss: 0.2874 / G_loss: 3.5951\n",
      "[   1583] D_loss: 0.1793 / G_loss: 3.2955\n",
      "[   1584] D_loss: 0.2542 / G_loss: 3.2203\n",
      "[   1585] D_loss: 0.1927 / G_loss: 3.4167\n",
      "[   1586] D_loss: 0.2362 / G_loss: 2.9356\n",
      "[   1587] D_loss: 0.3071 / G_loss: 3.1539\n",
      "[   1588] D_loss: 0.1881 / G_loss: 3.3106\n",
      "[   1589] D_loss: 0.3609 / G_loss: 3.5982\n",
      "[   1590] D_loss: 0.3037 / G_loss: 3.2935\n",
      "[   1591] D_loss: 0.1684 / G_loss: 3.4572\n",
      "[   1592] D_loss: 0.2719 / G_loss: 3.2363\n",
      "[   1593] D_loss: 0.4909 / G_loss: 3.2511\n",
      "[   1594] D_loss: 0.0826 / G_loss: 3.5425\n",
      "[   1595] D_loss: 0.2813 / G_loss: 3.3889\n",
      "[   1596] D_loss: 0.2662 / G_loss: 3.2617\n",
      "[   1597] D_loss: 0.3210 / G_loss: 3.3436\n",
      "[   1598] D_loss: 0.1767 / G_loss: 3.1727\n",
      "[   1599] D_loss: 0.2085 / G_loss: 3.3234\n",
      "[   1600] D_loss: 0.0301 / G_loss: 3.5986\n",
      "[   1601] D_loss: 0.2092 / G_loss: 3.4606\n",
      "[   1602] D_loss: 0.1203 / G_loss: 3.3031\n",
      "[   1603] D_loss: 0.2557 / G_loss: 3.2680\n",
      "[   1604] D_loss: 0.2273 / G_loss: 3.4747\n",
      "[   1605] D_loss: 0.3240 / G_loss: 3.1783\n",
      "[   1606] D_loss: 0.2049 / G_loss: 3.4515\n",
      "[   1607] D_loss: 0.4559 / G_loss: 3.4880\n",
      "[   1608] D_loss: 0.2903 / G_loss: 3.3811\n",
      "[   1609] D_loss: 0.1890 / G_loss: 3.4890\n",
      "[   1610] D_loss: 0.1205 / G_loss: 3.2562\n",
      "[   1611] D_loss: 0.2143 / G_loss: 3.3933\n",
      "[   1612] D_loss: 0.3134 / G_loss: 3.1676\n",
      "[   1613] D_loss: 0.2257 / G_loss: 3.1282\n",
      "[   1614] D_loss: 0.1991 / G_loss: 3.4191\n",
      "[   1615] D_loss: 0.1485 / G_loss: 3.3694\n",
      "[   1616] D_loss: 0.4891 / G_loss: 3.5067\n",
      "[   1617] D_loss: 0.1844 / G_loss: 3.1761\n",
      "[   1618] D_loss: 0.1575 / G_loss: 3.1399\n",
      "[   1619] D_loss: 0.1940 / G_loss: 3.3108\n",
      "[   1620] D_loss: 0.1256 / G_loss: 3.2040\n",
      "[   1621] D_loss: 0.1141 / G_loss: 3.3886\n",
      "[   1622] D_loss: 0.1897 / G_loss: 3.2494\n",
      "[   1623] D_loss: 0.2726 / G_loss: 3.4871\n",
      "[   1624] D_loss: 0.2246 / G_loss: 3.2152\n",
      "[   1625] D_loss: 0.1904 / G_loss: 3.4453\n",
      "[   1626] D_loss: 0.1826 / G_loss: 3.7031\n",
      "[   1627] D_loss: 0.1830 / G_loss: 3.6219\n",
      "[   1628] D_loss: 0.2516 / G_loss: 3.4753\n",
      "[   1629] D_loss: 0.2101 / G_loss: 3.4774\n",
      "[   1630] D_loss: 0.3451 / G_loss: 3.3310\n",
      "[   1631] D_loss: 0.1419 / G_loss: 3.4680\n",
      "[   1632] D_loss: 0.2973 / G_loss: 3.1408\n",
      "[   1633] D_loss: 0.2075 / G_loss: 3.6366\n",
      "[   1634] D_loss: 0.1076 / G_loss: 3.3218\n",
      "[   1635] D_loss: 0.0949 / G_loss: 3.9055\n",
      "[   1636] D_loss: 0.0708 / G_loss: 3.5679\n",
      "[   1637] D_loss: 0.2905 / G_loss: 3.7777\n",
      "[   1638] D_loss: 0.3122 / G_loss: 3.3892\n",
      "[   1639] D_loss: 0.2662 / G_loss: 3.4851\n",
      "[   1640] D_loss: 0.1206 / G_loss: 3.2353\n",
      "[   1641] D_loss: 0.1522 / G_loss: 3.5337\n",
      "[   1642] D_loss: 0.2339 / G_loss: 3.7577\n",
      "[   1643] D_loss: 0.1207 / G_loss: 3.3370\n",
      "[   1644] D_loss: 0.1688 / G_loss: 3.3641\n",
      "[   1645] D_loss: 0.2059 / G_loss: 3.1167\n",
      "[   1646] D_loss: 0.2411 / G_loss: 3.3116\n",
      "[   1647] D_loss: 0.2273 / G_loss: 3.4363\n",
      "[   1648] D_loss: 0.3356 / G_loss: 3.5294\n",
      "[   1649] D_loss: 0.2329 / G_loss: 3.3085\n",
      "[   1650] D_loss: 0.2146 / G_loss: 3.4901\n",
      "[   1651] D_loss: 0.2205 / G_loss: 3.1908\n",
      "[   1652] D_loss: 0.3185 / G_loss: 3.2444\n",
      "[   1653] D_loss: 0.1715 / G_loss: 3.3512\n",
      "[   1654] D_loss: 0.3118 / G_loss: 3.2278\n",
      "[   1655] D_loss: 0.1553 / G_loss: 3.3926\n",
      "[   1656] D_loss: 0.1405 / G_loss: 3.2655\n",
      "[   1657] D_loss: 0.2438 / G_loss: 3.1049\n",
      "[   1658] D_loss: 0.1867 / G_loss: 3.2089\n",
      "[   1659] D_loss: 0.1573 / G_loss: 3.2081\n",
      "[   1660] D_loss: 0.2055 / G_loss: 3.3877\n",
      "[   1661] D_loss: 0.0901 / G_loss: 3.5318\n",
      "[   1662] D_loss: 0.1155 / G_loss: 3.3747\n",
      "[   1663] D_loss: 0.1432 / G_loss: 3.4053\n",
      "[   1664] D_loss: 0.1274 / G_loss: 3.3652\n",
      "[   1665] D_loss: 0.2001 / G_loss: 3.3566\n",
      "[   1666] D_loss: 0.2671 / G_loss: 3.1523\n",
      "[   1667] D_loss: 0.1709 / G_loss: 3.4429\n",
      "[   1668] D_loss: 0.2809 / G_loss: 3.6692\n",
      "[   1669] D_loss: 0.1237 / G_loss: 3.3523\n",
      "[   1670] D_loss: 0.2163 / G_loss: 3.4215\n",
      "[   1671] D_loss: 0.1983 / G_loss: 3.1954\n",
      "[   1672] D_loss: 0.1491 / G_loss: 3.4172\n",
      "[   1673] D_loss: 0.2346 / G_loss: 3.6028\n",
      "[   1674] D_loss: 0.2559 / G_loss: 3.4590\n",
      "[   1675] D_loss: 0.2180 / G_loss: 4.0582\n",
      "[   1676] D_loss: 0.2795 / G_loss: 3.5216\n",
      "[   1677] D_loss: 0.1066 / G_loss: 3.5654\n",
      "[   1678] D_loss: 0.0779 / G_loss: 3.5360\n",
      "[   1679] D_loss: 0.1882 / G_loss: 3.8655\n",
      "[   1680] D_loss: 0.1109 / G_loss: 3.3930\n",
      "[   1681] D_loss: 0.1041 / G_loss: 3.5601\n",
      "[   1682] D_loss: 0.2303 / G_loss: 3.2068\n",
      "[   1683] D_loss: 0.1347 / G_loss: 3.4313\n",
      "[   1684] D_loss: 0.2051 / G_loss: 3.5787\n",
      "[   1685] D_loss: 0.0722 / G_loss: 3.2575\n",
      "[   1686] D_loss: 0.1024 / G_loss: 3.4856\n",
      "[   1687] D_loss: 0.2090 / G_loss: 3.5009\n",
      "[   1688] D_loss: 0.0620 / G_loss: 3.3638\n",
      "[   1689] D_loss: 0.2078 / G_loss: 3.5622\n",
      "[   1690] D_loss: 0.3117 / G_loss: 3.5414\n",
      "[   1691] D_loss: 0.2424 / G_loss: 3.2933\n",
      "[   1692] D_loss: 0.0983 / G_loss: 3.3986\n",
      "[   1693] D_loss: 0.2136 / G_loss: 3.4206\n",
      "[   1694] D_loss: 0.1890 / G_loss: 3.1473\n",
      "[   1695] D_loss: 0.0892 / G_loss: 3.3424\n",
      "[   1696] D_loss: 0.2884 / G_loss: 3.1580\n",
      "[   1697] D_loss: 0.2438 / G_loss: 3.5017\n",
      "[   1698] D_loss: 0.2600 / G_loss: 3.9505\n",
      "[   1699] D_loss: 0.3214 / G_loss: 3.3572\n",
      "[   1700] D_loss: 0.1636 / G_loss: 2.9889\n",
      "[   1701] D_loss: 0.1312 / G_loss: 3.4891\n",
      "[   1702] D_loss: 0.1064 / G_loss: 3.4312\n",
      "[   1703] D_loss: 0.1915 / G_loss: 3.4686\n",
      "[   1704] D_loss: 0.2305 / G_loss: 3.4388\n",
      "[   1705] D_loss: 0.2225 / G_loss: 3.3718\n",
      "[   1706] D_loss: 0.2512 / G_loss: 3.4904\n",
      "[   1707] D_loss: 0.1931 / G_loss: 3.9392\n",
      "[   1708] D_loss: 0.1781 / G_loss: 3.6182\n",
      "[   1709] D_loss: 0.1644 / G_loss: 3.6013\n",
      "[   1710] D_loss: 0.3080 / G_loss: 3.1656\n",
      "[   1711] D_loss: 0.1053 / G_loss: 3.3519\n",
      "[   1712] D_loss: 0.2560 / G_loss: 3.1968\n",
      "[   1713] D_loss: 0.2554 / G_loss: 3.2717\n",
      "[   1714] D_loss: 0.2296 / G_loss: 3.3032\n",
      "[   1715] D_loss: 0.0860 / G_loss: 3.5484\n",
      "[   1716] D_loss: 0.2492 / G_loss: 3.2977\n",
      "[   1717] D_loss: 0.2554 / G_loss: 3.2329\n",
      "[   1718] D_loss: 0.1127 / G_loss: 3.4606\n",
      "[   1719] D_loss: 0.1426 / G_loss: 3.3387\n",
      "[   1720] D_loss: 0.1483 / G_loss: 3.4015\n",
      "[   1721] D_loss: 0.2171 / G_loss: 3.3094\n",
      "[   1722] D_loss: 0.2503 / G_loss: 3.9091\n",
      "[   1723] D_loss: 0.0674 / G_loss: 3.7050\n",
      "[   1724] D_loss: 0.3131 / G_loss: 3.4046\n",
      "[   1725] D_loss: 0.3030 / G_loss: 3.2749\n",
      "[   1726] D_loss: 0.2298 / G_loss: 3.6406\n",
      "[   1727] D_loss: 0.1766 / G_loss: 3.3845\n",
      "[   1728] D_loss: 0.2569 / G_loss: 3.4379\n",
      "[   1729] D_loss: 0.1597 / G_loss: 3.2939\n",
      "[   1730] D_loss: 0.2171 / G_loss: 3.4079\n",
      "[   1731] D_loss: 0.2281 / G_loss: 3.4690\n",
      "[   1732] D_loss: 0.1975 / G_loss: 3.5437\n",
      "[   1733] D_loss: 0.1932 / G_loss: 3.3447\n",
      "[   1734] D_loss: 0.3466 / G_loss: 3.4100\n",
      "[   1735] D_loss: 0.2508 / G_loss: 3.5229\n",
      "[   1736] D_loss: 0.1779 / G_loss: 3.5061\n",
      "[   1737] D_loss: 0.1070 / G_loss: 3.4559\n",
      "[   1738] D_loss: 0.0496 / G_loss: 3.1806\n",
      "[   1739] D_loss: 0.1628 / G_loss: 3.4658\n",
      "[   1740] D_loss: 0.1384 / G_loss: 3.4436\n",
      "[   1741] D_loss: 0.2265 / G_loss: 3.5276\n",
      "[   1742] D_loss: 0.1971 / G_loss: 3.5242\n",
      "[   1743] D_loss: 0.1847 / G_loss: 3.3095\n",
      "[   1744] D_loss: 0.2162 / G_loss: 3.5651\n",
      "[   1745] D_loss: 0.1136 / G_loss: 3.4935\n",
      "[   1746] D_loss: 0.0779 / G_loss: 3.3135\n",
      "[   1747] D_loss: 0.2418 / G_loss: 3.4679\n",
      "[   1748] D_loss: 0.2569 / G_loss: 3.7823\n",
      "[   1749] D_loss: 0.1104 / G_loss: 3.7101\n",
      "[   1750] D_loss: 0.3574 / G_loss: 3.3889\n",
      "[   1751] D_loss: 0.1650 / G_loss: 3.1886\n",
      "[   1752] D_loss: 0.2249 / G_loss: 3.6800\n",
      "[   1753] D_loss: 0.1750 / G_loss: 3.4448\n",
      "[   1754] D_loss: 0.1250 / G_loss: 3.3743\n",
      "[   1755] D_loss: 0.1468 / G_loss: 3.5079\n",
      "[   1756] D_loss: 0.2735 / G_loss: 3.9789\n",
      "[   1757] D_loss: 0.3010 / G_loss: 3.8185\n",
      "[   1758] D_loss: 0.1023 / G_loss: 3.4925\n",
      "[   1759] D_loss: 0.2045 / G_loss: 3.2662\n",
      "[   1760] D_loss: 0.1159 / G_loss: 3.3659\n",
      "[   1761] D_loss: 0.1718 / G_loss: 3.3621\n",
      "[   1762] D_loss: 0.1550 / G_loss: 3.6630\n",
      "[   1763] D_loss: 0.1325 / G_loss: 3.6143\n",
      "[   1764] D_loss: 0.1151 / G_loss: 3.7655\n",
      "[   1765] D_loss: 0.1933 / G_loss: 3.6627\n",
      "[   1766] D_loss: 0.1540 / G_loss: 3.5149\n",
      "[   1767] D_loss: 0.1914 / G_loss: 3.3827\n",
      "[   1768] D_loss: 0.1751 / G_loss: 3.4969\n",
      "[   1769] D_loss: 0.1662 / G_loss: 3.5992\n",
      "[   1770] D_loss: 0.3141 / G_loss: 3.3332\n",
      "[   1771] D_loss: 0.1286 / G_loss: 3.6495\n",
      "[   1772] D_loss: 0.1471 / G_loss: 3.4010\n",
      "[   1773] D_loss: 0.2841 / G_loss: 3.5503\n",
      "[   1774] D_loss: 0.1952 / G_loss: 3.7254\n",
      "[   1775] D_loss: 0.2255 / G_loss: 3.5607\n",
      "[   1776] D_loss: 0.0722 / G_loss: 3.4695\n",
      "[   1777] D_loss: 0.2615 / G_loss: 3.4529\n",
      "[   1778] D_loss: 0.1681 / G_loss: 3.5722\n",
      "[   1779] D_loss: 0.1873 / G_loss: 3.6499\n",
      "[   1780] D_loss: 0.1128 / G_loss: 3.6438\n",
      "[   1781] D_loss: 0.1487 / G_loss: 3.8618\n",
      "[   1782] D_loss: 0.1419 / G_loss: 3.5147\n",
      "[   1783] D_loss: 0.1345 / G_loss: 3.6566\n",
      "[   1784] D_loss: 0.2182 / G_loss: 3.5274\n",
      "[   1785] D_loss: 0.0672 / G_loss: 3.7451\n",
      "[   1786] D_loss: 0.0558 / G_loss: 3.6582\n",
      "[   1787] D_loss: 0.2895 / G_loss: 3.4907\n",
      "[   1788] D_loss: 0.2156 / G_loss: 3.6012\n",
      "[   1789] D_loss: 0.0889 / G_loss: 3.8173\n",
      "[   1790] D_loss: 0.1021 / G_loss: 3.7950\n",
      "[   1791] D_loss: 0.1295 / G_loss: 3.5533\n",
      "[   1792] D_loss: 0.1653 / G_loss: 3.5093\n",
      "[   1793] D_loss: 0.0910 / G_loss: 3.5928\n",
      "[   1794] D_loss: 0.1403 / G_loss: 3.3493\n",
      "[   1795] D_loss: 0.2178 / G_loss: 3.8819\n",
      "[   1796] D_loss: 0.0969 / G_loss: 3.3110\n",
      "[   1797] D_loss: 0.1376 / G_loss: 3.5069\n",
      "[   1798] D_loss: 0.1872 / G_loss: 3.5479\n",
      "[   1799] D_loss: 0.1278 / G_loss: 3.6545\n",
      "[   1800] D_loss: 0.1989 / G_loss: 3.5670\n",
      "[   1801] D_loss: 0.1073 / G_loss: 3.5039\n",
      "[   1802] D_loss: 0.1721 / G_loss: 3.8347\n",
      "[   1803] D_loss: 0.3478 / G_loss: 3.7829\n",
      "[   1804] D_loss: 0.1160 / G_loss: 3.4968\n",
      "[   1805] D_loss: 0.2890 / G_loss: 3.7131\n",
      "[   1806] D_loss: 0.1708 / G_loss: 3.5245\n",
      "[   1807] D_loss: 0.4094 / G_loss: 3.6585\n",
      "[   1808] D_loss: 0.2936 / G_loss: 3.7062\n",
      "[   1809] D_loss: 0.1902 / G_loss: 3.3311\n",
      "[   1810] D_loss: 0.2204 / G_loss: 3.4992\n",
      "[   1811] D_loss: 0.1211 / G_loss: 3.2906\n",
      "[   1812] D_loss: 0.2311 / G_loss: 3.8794\n",
      "[   1813] D_loss: 0.1549 / G_loss: 3.2293\n",
      "[   1814] D_loss: 0.2536 / G_loss: 3.3779\n",
      "[   1815] D_loss: 0.3499 / G_loss: 3.5375\n",
      "[   1816] D_loss: 0.2534 / G_loss: 3.3490\n",
      "[   1817] D_loss: 0.1354 / G_loss: 3.2145\n",
      "[   1818] D_loss: 0.1183 / G_loss: 3.2943\n",
      "[   1819] D_loss: 0.1670 / G_loss: 3.7615\n",
      "[   1820] D_loss: 0.2339 / G_loss: 3.4938\n",
      "[   1821] D_loss: 0.0927 / G_loss: 3.7011\n",
      "[   1822] D_loss: 0.3138 / G_loss: 3.2898\n",
      "[   1823] D_loss: 0.0401 / G_loss: 3.7471\n",
      "[   1824] D_loss: 0.1168 / G_loss: 3.3699\n",
      "[   1825] D_loss: 0.1184 / G_loss: 3.4371\n",
      "[   1826] D_loss: 0.3426 / G_loss: 3.3580\n",
      "[   1827] D_loss: 0.2775 / G_loss: 3.6852\n",
      "[   1828] D_loss: 0.2468 / G_loss: 3.7043\n",
      "[   1829] D_loss: 0.1487 / G_loss: 3.4618\n",
      "[   1830] D_loss: 0.1217 / G_loss: 3.3361\n",
      "[   1831] D_loss: 0.2625 / G_loss: 3.5340\n",
      "[   1832] D_loss: 0.1347 / G_loss: 3.5149\n",
      "[   1833] D_loss: 0.1696 / G_loss: 3.4435\n",
      "[   1834] D_loss: 0.1904 / G_loss: 3.5405\n",
      "[   1835] D_loss: 0.1217 / G_loss: 3.8015\n",
      "[   1836] D_loss: 0.2080 / G_loss: 3.4048\n",
      "[   1837] D_loss: 0.2960 / G_loss: 3.3476\n",
      "[   1838] D_loss: 0.1768 / G_loss: 3.8874\n",
      "[   1839] D_loss: 0.1800 / G_loss: 3.8083\n",
      "[   1840] D_loss: 0.1417 / G_loss: 3.1414\n",
      "[   1841] D_loss: 0.1729 / G_loss: 3.6433\n",
      "[   1842] D_loss: 0.2306 / G_loss: 3.1621\n",
      "[   1843] D_loss: 0.0888 / G_loss: 3.5019\n",
      "[   1844] D_loss: 0.2425 / G_loss: 3.6917\n",
      "[   1845] D_loss: 0.2192 / G_loss: 3.6988\n",
      "[   1846] D_loss: 0.0594 / G_loss: 3.9322\n",
      "[   1847] D_loss: 0.1104 / G_loss: 3.4057\n",
      "[   1848] D_loss: 0.1381 / G_loss: 3.4730\n",
      "[   1849] D_loss: 0.1050 / G_loss: 3.4418\n",
      "[   1850] D_loss: 0.2848 / G_loss: 3.6799\n",
      "[   1851] D_loss: 0.1901 / G_loss: 3.4012\n",
      "[   1852] D_loss: 0.0791 / G_loss: 3.5985\n",
      "[   1853] D_loss: 0.2392 / G_loss: 3.5035\n",
      "[   1854] D_loss: 0.1193 / G_loss: 3.6713\n",
      "[   1855] D_loss: 0.2292 / G_loss: 3.3584\n",
      "[   1856] D_loss: 0.1813 / G_loss: 3.6071\n",
      "[   1857] D_loss: 0.1331 / G_loss: 3.5365\n",
      "[   1858] D_loss: 0.1721 / G_loss: 3.5633\n",
      "[   1859] D_loss: 0.2115 / G_loss: 3.2957\n",
      "[   1860] D_loss: 0.1819 / G_loss: 3.9593\n",
      "[   1861] D_loss: 0.1524 / G_loss: 3.4145\n",
      "[   1862] D_loss: 0.1327 / G_loss: 3.7204\n",
      "[   1863] D_loss: 0.3333 / G_loss: 3.6418\n",
      "[   1864] D_loss: 0.1542 / G_loss: 3.7754\n",
      "[   1865] D_loss: 0.1902 / G_loss: 3.2048\n",
      "[   1866] D_loss: 0.2283 / G_loss: 3.2347\n",
      "[   1867] D_loss: 0.1643 / G_loss: 3.6668\n",
      "[   1868] D_loss: 0.2147 / G_loss: 3.4637\n",
      "[   1869] D_loss: 0.1651 / G_loss: 3.4552\n",
      "[   1870] D_loss: 0.2662 / G_loss: 3.5205\n",
      "[   1871] D_loss: 0.1524 / G_loss: 3.5029\n",
      "[   1872] D_loss: 0.1535 / G_loss: 3.7848\n",
      "[   1873] D_loss: 0.0731 / G_loss: 3.5968\n",
      "[   1874] D_loss: 0.3270 / G_loss: 3.4322\n",
      "[   1875] D_loss: 0.2798 / G_loss: 4.2154\n",
      "[   1876] D_loss: 0.1514 / G_loss: 3.4069\n",
      "[   1877] D_loss: 0.1630 / G_loss: 3.8604\n",
      "[   1878] D_loss: 0.2488 / G_loss: 3.7334\n",
      "[   1879] D_loss: 0.1415 / G_loss: 3.2979\n",
      "[   1880] D_loss: 0.1310 / G_loss: 3.3744\n",
      "[   1881] D_loss: 0.2951 / G_loss: 4.0142\n",
      "[   1882] D_loss: 0.1897 / G_loss: 3.4858\n",
      "[   1883] D_loss: 0.0381 / G_loss: 3.6471\n",
      "[   1884] D_loss: 0.1263 / G_loss: 3.8478\n",
      "[   1885] D_loss: 0.2111 / G_loss: 3.2504\n",
      "[   1886] D_loss: 0.2360 / G_loss: 3.4989\n",
      "[   1887] D_loss: 0.2091 / G_loss: 3.4542\n",
      "[   1888] D_loss: 0.4024 / G_loss: 4.0879\n",
      "[   1889] D_loss: 0.1850 / G_loss: 3.5735\n",
      "[   1890] D_loss: 0.2162 / G_loss: 3.5389\n",
      "[   1891] D_loss: 0.2874 / G_loss: 3.7370\n",
      "[   1892] D_loss: 0.1187 / G_loss: 3.6499\n",
      "[   1893] D_loss: 0.2036 / G_loss: 3.4630\n",
      "[   1894] D_loss: 0.1575 / G_loss: 3.6712\n",
      "[   1895] D_loss: 0.2524 / G_loss: 3.3006\n",
      "[   1896] D_loss: 0.2767 / G_loss: 3.4339\n",
      "[   1897] D_loss: 0.1268 / G_loss: 3.7170\n",
      "[   1898] D_loss: 0.1748 / G_loss: 3.4632\n",
      "[   1899] D_loss: 0.0709 / G_loss: 3.8048\n",
      "[   1900] D_loss: 0.1417 / G_loss: 3.7329\n",
      "[   1901] D_loss: 0.2543 / G_loss: 3.7091\n",
      "[   1902] D_loss: 0.3156 / G_loss: 3.5291\n",
      "[   1903] D_loss: 0.2238 / G_loss: 3.9875\n",
      "[   1904] D_loss: 0.1441 / G_loss: 3.3663\n",
      "[   1905] D_loss: 0.1784 / G_loss: 3.7644\n",
      "[   1906] D_loss: 0.2310 / G_loss: 4.0305\n",
      "[   1907] D_loss: 0.1130 / G_loss: 3.5537\n",
      "[   1908] D_loss: 0.1144 / G_loss: 3.5859\n",
      "[   1909] D_loss: 0.1712 / G_loss: 4.2907\n",
      "[   1910] D_loss: 0.1463 / G_loss: 3.2189\n",
      "[   1911] D_loss: 0.1188 / G_loss: 3.7611\n",
      "[   1912] D_loss: 0.2176 / G_loss: 3.7608\n",
      "[   1913] D_loss: 0.3228 / G_loss: 3.5501\n",
      "[   1914] D_loss: 0.2651 / G_loss: 3.7439\n",
      "[   1915] D_loss: 0.1659 / G_loss: 3.8858\n",
      "[   1916] D_loss: 0.1571 / G_loss: 3.6655\n",
      "[   1917] D_loss: 0.1932 / G_loss: 3.8545\n",
      "[   1918] D_loss: 0.0729 / G_loss: 3.6362\n",
      "[   1919] D_loss: 0.2858 / G_loss: 3.5057\n",
      "[   1920] D_loss: 0.4069 / G_loss: 4.3595\n",
      "[   1921] D_loss: 0.1513 / G_loss: 3.5132\n",
      "[   1922] D_loss: 0.1851 / G_loss: 3.3306\n",
      "[   1923] D_loss: 0.1196 / G_loss: 3.2802\n",
      "[   1924] D_loss: 0.1066 / G_loss: 3.4285\n",
      "[   1925] D_loss: 0.2409 / G_loss: 3.5736\n",
      "[   1926] D_loss: 0.1673 / G_loss: 3.4431\n",
      "[   1927] D_loss: 0.1691 / G_loss: 3.4575\n",
      "[   1928] D_loss: 0.0950 / G_loss: 3.2011\n",
      "[   1929] D_loss: 0.0423 / G_loss: 3.6841\n",
      "[   1930] D_loss: 0.1909 / G_loss: 3.6605\n",
      "[   1931] D_loss: 0.2796 / G_loss: 3.7595\n",
      "[   1932] D_loss: 0.1782 / G_loss: 3.4142\n",
      "[   1933] D_loss: 0.1303 / G_loss: 3.6778\n",
      "[   1934] D_loss: 0.2615 / G_loss: 3.6686\n",
      "[   1935] D_loss: 0.2881 / G_loss: 3.3553\n",
      "[   1936] D_loss: 0.1984 / G_loss: 3.5898\n",
      "[   1937] D_loss: 0.0919 / G_loss: 4.0396\n",
      "[   1938] D_loss: 0.1813 / G_loss: 3.4965\n",
      "[   1939] D_loss: 0.2549 / G_loss: 3.6001\n",
      "[   1940] D_loss: 0.0778 / G_loss: 3.5125\n",
      "[   1941] D_loss: 0.1277 / G_loss: 3.6782\n",
      "[   1942] D_loss: 0.2141 / G_loss: 3.4656\n",
      "[   1943] D_loss: 0.2567 / G_loss: 3.6201\n",
      "[   1944] D_loss: 0.2144 / G_loss: 3.9133\n",
      "[   1945] D_loss: 0.1942 / G_loss: 3.7111\n",
      "[   1946] D_loss: 0.1212 / G_loss: 3.3801\n",
      "[   1947] D_loss: 0.1573 / G_loss: 3.5595\n",
      "[   1948] D_loss: 0.0962 / G_loss: 3.5209\n",
      "[   1949] D_loss: 0.1006 / G_loss: 3.4518\n",
      "[   1950] D_loss: 0.1178 / G_loss: 3.4700\n",
      "[   1951] D_loss: 0.1985 / G_loss: 3.8290\n",
      "[   1952] D_loss: 0.0654 / G_loss: 3.8248\n",
      "[   1953] D_loss: 0.0552 / G_loss: 3.4844\n",
      "[   1954] D_loss: 0.1520 / G_loss: 3.7032\n",
      "[   1955] D_loss: 0.2804 / G_loss: 3.5923\n",
      "[   1956] D_loss: 0.1902 / G_loss: 3.9031\n",
      "[   1957] D_loss: 0.2556 / G_loss: 3.8630\n",
      "[   1958] D_loss: 0.1328 / G_loss: 3.7329\n",
      "[   1959] D_loss: 0.0993 / G_loss: 4.0702\n",
      "[   1960] D_loss: 0.1585 / G_loss: 3.5563\n",
      "[   1961] D_loss: 0.0889 / G_loss: 3.7024\n",
      "[   1962] D_loss: 0.1926 / G_loss: 3.6756\n",
      "[   1963] D_loss: 0.0439 / G_loss: 3.7022\n",
      "[   1964] D_loss: 0.1093 / G_loss: 3.6496\n",
      "[   1965] D_loss: 0.3195 / G_loss: 3.8493\n",
      "[   1966] D_loss: 0.3799 / G_loss: 3.8075\n",
      "[   1967] D_loss: 0.1978 / G_loss: 3.7801\n",
      "[   1968] D_loss: 0.1417 / G_loss: 3.6963\n",
      "[   1969] D_loss: 0.0832 / G_loss: 3.5433\n",
      "[   1970] D_loss: 0.1935 / G_loss: 3.7343\n",
      "[   1971] D_loss: 0.1491 / G_loss: 3.5783\n",
      "[   1972] D_loss: 0.1717 / G_loss: 3.4925\n",
      "[   1973] D_loss: 0.1557 / G_loss: 3.7252\n",
      "[   1974] D_loss: 0.1468 / G_loss: 3.7343\n",
      "[   1975] D_loss: 0.1590 / G_loss: 3.7528\n",
      "[   1976] D_loss: 0.1232 / G_loss: 3.4810\n",
      "[   1977] D_loss: 0.1446 / G_loss: 3.6612\n",
      "[   1978] D_loss: 0.1036 / G_loss: 3.7757\n",
      "[   1979] D_loss: 0.1001 / G_loss: 3.8374\n",
      "[   1980] D_loss: 0.1055 / G_loss: 3.6238\n",
      "[   1981] D_loss: 0.1385 / G_loss: 3.6546\n",
      "[   1982] D_loss: 0.2281 / G_loss: 3.9828\n",
      "[   1983] D_loss: 0.1493 / G_loss: 3.7678\n",
      "[   1984] D_loss: 0.1399 / G_loss: 3.8340\n",
      "[   1985] D_loss: 0.1245 / G_loss: 3.7853\n",
      "[   1986] D_loss: 0.1207 / G_loss: 3.6026\n",
      "[   1987] D_loss: 0.0996 / G_loss: 3.9328\n",
      "[   1988] D_loss: 0.1204 / G_loss: 3.7639\n",
      "[   1989] D_loss: 0.1100 / G_loss: 4.1523\n",
      "[   1990] D_loss: 0.0925 / G_loss: 3.3631\n",
      "[   1991] D_loss: 0.0574 / G_loss: 3.5603\n",
      "[   1992] D_loss: 0.0667 / G_loss: 3.7308\n",
      "[   1993] D_loss: 0.2377 / G_loss: 4.0479\n",
      "[   1994] D_loss: 0.1885 / G_loss: 3.5859\n",
      "[   1995] D_loss: 0.1006 / G_loss: 3.3277\n",
      "[   1996] D_loss: 0.0793 / G_loss: 3.8224\n",
      "[   1997] D_loss: 0.2886 / G_loss: 3.8042\n",
      "[   1998] D_loss: 0.1002 / G_loss: 4.0656\n",
      "[   1999] D_loss: 0.0554 / G_loss: 3.5089\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "# sess.run(tf.initialize_all_variables())\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "writer = tf.summary.FileWriter(\"./tmp2/\", sess.graph)\n",
    "\n",
    "epoch_n = 2000\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    # _ 는 원래 Y 임.\n",
    "    for _ in range(N // batch_size):\n",
    "        X_batch, _ = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_batch, Z: sample_Z(batch_size, Z_dim)})\n",
    "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(batch_size, Z_dim)})\n",
    "    \n",
    "    print(\"[{:7d}] D_loss: {:.4f} / G_loss: {:.4f}\".format(epoch, D_loss_curr, G_loss_curr))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)})\n",
    "        fig = plot(samples)\n",
    "        #     plt.show()\n",
    "        fig.savefig('out/{:0>4d}.png'.format(epoch), bbox_inches='tight')\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
