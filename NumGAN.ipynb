{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number-GAN\n",
    "\n",
    "* 그냥 noise z 와 input x 에 label y 를 concat 으로 때려박으면 되는 듯함\n",
    "* G(z) 의 loss 값이 D 에 따라 계산되기 때문에 y 를 따질 필요가 없음. 무슨 말이냐면:\n",
    "    * y=1 인 G(z) 에서 2를 만들었다고 하자.\n",
    "    * D(x) 를 계산할 때에도 y 를 때려박기 때문에, G(z) 의 loss 는 D(G(z)+y) 로 계산한다.\n",
    "    * 이 경우, G(z) 는 2 지만 y 가 1을 나타내기 때문에 D 가 fake 라고 판단할 수 있음 (암만 G(z)가 2를 잘 만들었다고 해도)\n",
    "    * 따라서 결국 G(z) 가 y 에 해당하는 이미지를 제너레이트하도록 학습이 됨\n",
    "* 굉장히 소소한 정보로 학습하는 거 같은데, 암튼 잘 되나 확인해보자.\n",
    "    * 노이즈 d 는 100이고, 라벨 d 는 10 (one-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "D_W1 = tf.Variable(xavier_init([794, 128]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "D_params = [D_W1, D_b1, D_W2, D_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Z is noised input\n",
    "# G(z) => generated new x\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "G_W1 = tf.Variable(xavier_init([110, 128]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "G_W2 = tf.Variable(xavier_init([128, 784]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "G_params = [G_W1, G_b1, G_W2, G_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "    D_a1 = tf.matmul(x, D_W1) + D_b1\n",
    "    D_h1 = tf.nn.relu(D_a1)\n",
    "    # what is logit?\n",
    "    # logit is inverse function of sigmoid function\n",
    "    # therefore, D_logit = logit(D_prob).\n",
    "    # 즉, sigmoid 함수에 들어가는 값이 logit 이라고 할 수 있음.\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    \n",
    "    return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    G_a1 = tf.matmul(z, G_W1) + G_b1\n",
    "    G_h1 = tf.nn.relu(G_a1)\n",
    "    # what is differ between logit <> log_prob ?\n",
    "    # 이것도 걍 로짓이 맞는거 같은데? logit_prob 의 약자일것 같음.\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "    \n",
    "    return G_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_label(z, y):\n",
    "    return tf.concat([z, y], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G_sample = generator(add_label(Z, Y))\n",
    "D_real, D_logit_real = discriminator(add_label(X, Y))\n",
    "D_fake, D_logit_fake = discriminator(add_label(G_sample, Y))\n",
    "\n",
    "# V1. paper-based learning (다른 코드를 보면 logit 을 이용해서 CE로 계산을 함. 이게 더 성능이 잘 나오나봄)\n",
    "# http://bamos.github.io/2016/08/09/deep-completion/ 참고\n",
    "\n",
    "# maximize x => minimize -x\n",
    "#D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1-D_fake))\n",
    "#G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "\n",
    "# V2.\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=D_params)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=G_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "Z_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D_losses = []\n",
    "G_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0] D_loss: 1.4891 / G_loss: 2.5781\n",
      "[   1000] D_loss: 0.0116 / G_loss: 9.4070\n",
      "[   2000] D_loss: 0.0205 / G_loss: 6.4561\n",
      "[   3000] D_loss: 0.0372 / G_loss: 5.4466\n",
      "[   4000] D_loss: 0.0468 / G_loss: 6.0719\n",
      "[   5000] D_loss: 0.1500 / G_loss: 6.3863\n",
      "[   6000] D_loss: 0.2849 / G_loss: 6.9143\n",
      "[   7000] D_loss: 0.2389 / G_loss: 4.6980\n",
      "[   8000] D_loss: 0.3017 / G_loss: 3.8641\n",
      "[   9000] D_loss: 0.5795 / G_loss: 3.3079\n",
      "[  10000] D_loss: 0.5581 / G_loss: 3.6674\n",
      "[  11000] D_loss: 0.5096 / G_loss: 3.0425\n",
      "[  12000] D_loss: 0.6135 / G_loss: 2.8697\n",
      "[  13000] D_loss: 0.6491 / G_loss: 2.9981\n",
      "[  14000] D_loss: 0.8409 / G_loss: 2.6111\n",
      "[  15000] D_loss: 0.7721 / G_loss: 3.1465\n",
      "[  16000] D_loss: 0.7877 / G_loss: 2.5332\n",
      "[  17000] D_loss: 0.6865 / G_loss: 2.0501\n",
      "[  18000] D_loss: 0.7051 / G_loss: 1.7502\n",
      "[  19000] D_loss: 0.7695 / G_loss: 2.2789\n",
      "[  20000] D_loss: 0.8306 / G_loss: 2.3188\n",
      "[  21000] D_loss: 0.6048 / G_loss: 1.8043\n",
      "[  22000] D_loss: 0.7720 / G_loss: 2.0681\n",
      "[  23000] D_loss: 0.7307 / G_loss: 1.7089\n",
      "[  24000] D_loss: 0.7489 / G_loss: 1.8189\n",
      "[  25000] D_loss: 0.5956 / G_loss: 2.2176\n",
      "[  26000] D_loss: 0.6347 / G_loss: 1.9345\n",
      "[  27000] D_loss: 0.7941 / G_loss: 2.0082\n",
      "[  28000] D_loss: 0.6883 / G_loss: 1.9310\n",
      "[  29000] D_loss: 0.8470 / G_loss: 1.8289\n",
      "[  30000] D_loss: 0.7644 / G_loss: 1.9354\n",
      "[  31000] D_loss: 0.6431 / G_loss: 2.0331\n",
      "[  32000] D_loss: 0.6404 / G_loss: 2.2442\n",
      "[  33000] D_loss: 0.8245 / G_loss: 2.1350\n",
      "[  34000] D_loss: 0.6940 / G_loss: 1.8817\n",
      "[  35000] D_loss: 0.8331 / G_loss: 1.6446\n",
      "[  36000] D_loss: 0.7414 / G_loss: 1.8150\n",
      "[  37000] D_loss: 0.6687 / G_loss: 1.5942\n",
      "[  38000] D_loss: 0.7078 / G_loss: 1.7934\n",
      "[  39000] D_loss: 0.7355 / G_loss: 2.0249\n",
      "[  40000] D_loss: 0.7494 / G_loss: 2.0541\n",
      "[  41000] D_loss: 0.7505 / G_loss: 1.7687\n",
      "[  42000] D_loss: 0.8210 / G_loss: 1.8930\n",
      "[  43000] D_loss: 0.8881 / G_loss: 1.8987\n",
      "[  44000] D_loss: 0.7305 / G_loss: 2.1888\n",
      "[  45000] D_loss: 0.8026 / G_loss: 2.0592\n",
      "[  46000] D_loss: 0.9409 / G_loss: 1.6693\n",
      "[  47000] D_loss: 0.7298 / G_loss: 1.8321\n",
      "[  48000] D_loss: 0.7837 / G_loss: 1.8469\n",
      "[  49000] D_loss: 0.7296 / G_loss: 1.7656\n",
      "[  50000] D_loss: 0.6975 / G_loss: 1.9775\n",
      "[  51000] D_loss: 0.7712 / G_loss: 1.8589\n",
      "[  52000] D_loss: 0.7055 / G_loss: 1.7841\n",
      "[  53000] D_loss: 0.6232 / G_loss: 1.9015\n",
      "[  54000] D_loss: 0.8671 / G_loss: 1.7748\n",
      "[  55000] D_loss: 0.9879 / G_loss: 1.7424\n",
      "[  56000] D_loss: 0.7897 / G_loss: 2.0177\n",
      "[  57000] D_loss: 0.6701 / G_loss: 1.7295\n",
      "[  58000] D_loss: 0.8727 / G_loss: 1.6148\n",
      "[  59000] D_loss: 0.7916 / G_loss: 1.5020\n",
      "[  60000] D_loss: 0.7136 / G_loss: 2.0011\n",
      "[  61000] D_loss: 0.8866 / G_loss: 1.7872\n",
      "[  62000] D_loss: 0.7544 / G_loss: 1.8405\n",
      "[  63000] D_loss: 0.7772 / G_loss: 1.7561\n",
      "[  64000] D_loss: 0.8795 / G_loss: 1.6708\n",
      "[  65000] D_loss: 0.7518 / G_loss: 1.7403\n",
      "[  66000] D_loss: 0.6870 / G_loss: 2.0429\n",
      "[  67000] D_loss: 0.8043 / G_loss: 1.7907\n",
      "[  68000] D_loss: 0.6261 / G_loss: 1.4600\n",
      "[  69000] D_loss: 0.8336 / G_loss: 1.5584\n",
      "[  70000] D_loss: 0.7360 / G_loss: 1.7582\n",
      "[  71000] D_loss: 0.7796 / G_loss: 1.7099\n",
      "[  72000] D_loss: 0.9297 / G_loss: 1.6984\n",
      "[  73000] D_loss: 0.8781 / G_loss: 1.6783\n",
      "[  74000] D_loss: 0.9501 / G_loss: 1.6044\n",
      "[  75000] D_loss: 0.7908 / G_loss: 1.4509\n",
      "[  76000] D_loss: 0.9263 / G_loss: 1.6156\n",
      "[  77000] D_loss: 0.8592 / G_loss: 1.7722\n",
      "[  78000] D_loss: 0.7716 / G_loss: 1.9273\n",
      "[  79000] D_loss: 0.8251 / G_loss: 1.6111\n",
      "[  80000] D_loss: 0.7827 / G_loss: 1.7751\n",
      "[  81000] D_loss: 0.8452 / G_loss: 1.7291\n",
      "[  82000] D_loss: 0.8014 / G_loss: 1.7123\n",
      "[  83000] D_loss: 0.9326 / G_loss: 1.3475\n",
      "[  84000] D_loss: 0.9631 / G_loss: 1.6169\n",
      "[  85000] D_loss: 0.7615 / G_loss: 1.8133\n",
      "[  86000] D_loss: 0.8859 / G_loss: 1.6548\n",
      "[  87000] D_loss: 0.8695 / G_loss: 1.8065\n",
      "[  88000] D_loss: 0.7662 / G_loss: 1.9639\n",
      "[  89000] D_loss: 0.7426 / G_loss: 1.8775\n",
      "[  90000] D_loss: 0.8288 / G_loss: 1.9270\n",
      "[  91000] D_loss: 0.8331 / G_loss: 1.7426\n",
      "[  92000] D_loss: 0.6929 / G_loss: 2.0963\n",
      "[  93000] D_loss: 0.9491 / G_loss: 1.8160\n",
      "[  94000] D_loss: 0.7830 / G_loss: 1.8394\n",
      "[  95000] D_loss: 0.8339 / G_loss: 1.5516\n",
      "[  96000] D_loss: 0.8391 / G_loss: 1.9543\n",
      "[  97000] D_loss: 0.7628 / G_loss: 1.6755\n",
      "[  98000] D_loss: 0.8413 / G_loss: 1.6936\n",
      "[  99000] D_loss: 0.8686 / G_loss: 1.8078\n",
      "[ 100000] D_loss: 0.6484 / G_loss: 1.7303\n",
      "[ 101000] D_loss: 0.8922 / G_loss: 1.8149\n",
      "[ 102000] D_loss: 0.7485 / G_loss: 1.6713\n",
      "[ 103000] D_loss: 0.7337 / G_loss: 1.7369\n",
      "[ 104000] D_loss: 0.6782 / G_loss: 1.9291\n",
      "[ 105000] D_loss: 0.7456 / G_loss: 1.9500\n",
      "[ 106000] D_loss: 0.6667 / G_loss: 1.5284\n",
      "[ 107000] D_loss: 0.6869 / G_loss: 1.7391\n",
      "[ 108000] D_loss: 0.9064 / G_loss: 1.6936\n",
      "[ 109000] D_loss: 0.7167 / G_loss: 1.8007\n",
      "[ 110000] D_loss: 0.7653 / G_loss: 1.7730\n",
      "[ 111000] D_loss: 0.8196 / G_loss: 1.7394\n",
      "[ 112000] D_loss: 0.7841 / G_loss: 1.6348\n",
      "[ 113000] D_loss: 0.8074 / G_loss: 1.5988\n",
      "[ 114000] D_loss: 0.7100 / G_loss: 1.8577\n",
      "[ 115000] D_loss: 0.8663 / G_loss: 1.6923\n",
      "[ 116000] D_loss: 0.8185 / G_loss: 1.7534\n",
      "[ 117000] D_loss: 0.7574 / G_loss: 1.8671\n",
      "[ 118000] D_loss: 0.7622 / G_loss: 1.7729\n",
      "[ 119000] D_loss: 0.7724 / G_loss: 1.7290\n",
      "[ 120000] D_loss: 0.9418 / G_loss: 1.8200\n",
      "[ 121000] D_loss: 0.8212 / G_loss: 1.8419\n",
      "[ 122000] D_loss: 0.7800 / G_loss: 1.6762\n",
      "[ 123000] D_loss: 0.8598 / G_loss: 1.8320\n",
      "[ 124000] D_loss: 0.7720 / G_loss: 1.7536\n",
      "[ 125000] D_loss: 0.7569 / G_loss: 1.6741\n",
      "[ 126000] D_loss: 0.8480 / G_loss: 1.7610\n",
      "[ 127000] D_loss: 0.8405 / G_loss: 2.0209\n",
      "[ 128000] D_loss: 0.8302 / G_loss: 1.9191\n",
      "[ 129000] D_loss: 0.7300 / G_loss: 2.0659\n",
      "[ 130000] D_loss: 0.7884 / G_loss: 1.7144\n",
      "[ 131000] D_loss: 0.7163 / G_loss: 1.8981\n",
      "[ 132000] D_loss: 0.6901 / G_loss: 1.6736\n",
      "[ 133000] D_loss: 0.7469 / G_loss: 1.8283\n",
      "[ 134000] D_loss: 1.0183 / G_loss: 1.7801\n",
      "[ 135000] D_loss: 0.6376 / G_loss: 1.8589\n",
      "[ 136000] D_loss: 0.6634 / G_loss: 1.8022\n",
      "[ 137000] D_loss: 0.8744 / G_loss: 1.6581\n",
      "[ 138000] D_loss: 0.7465 / G_loss: 2.1458\n",
      "[ 139000] D_loss: 0.8081 / G_loss: 1.8636\n",
      "[ 140000] D_loss: 0.7754 / G_loss: 2.1621\n",
      "[ 141000] D_loss: 0.8068 / G_loss: 1.4262\n",
      "[ 142000] D_loss: 0.8408 / G_loss: 2.0950\n",
      "[ 143000] D_loss: 0.6402 / G_loss: 1.6323\n",
      "[ 144000] D_loss: 0.7542 / G_loss: 1.6535\n",
      "[ 145000] D_loss: 0.7758 / G_loss: 1.6115\n",
      "[ 146000] D_loss: 0.8874 / G_loss: 1.7159\n",
      "[ 147000] D_loss: 0.8388 / G_loss: 1.4230\n",
      "[ 148000] D_loss: 0.6919 / G_loss: 2.1612\n",
      "[ 149000] D_loss: 0.7480 / G_loss: 1.9633\n",
      "[ 150000] D_loss: 0.8921 / G_loss: 2.0788\n",
      "[ 151000] D_loss: 0.7155 / G_loss: 2.0799\n",
      "[ 152000] D_loss: 0.7219 / G_loss: 1.6504\n",
      "[ 153000] D_loss: 0.7825 / G_loss: 1.9173\n",
      "[ 154000] D_loss: 0.7304 / G_loss: 1.9103\n",
      "[ 155000] D_loss: 0.7735 / G_loss: 1.7069\n",
      "[ 156000] D_loss: 0.8189 / G_loss: 1.9999\n",
      "[ 157000] D_loss: 0.7557 / G_loss: 2.0875\n",
      "[ 158000] D_loss: 0.7226 / G_loss: 1.8780\n",
      "[ 159000] D_loss: 0.7903 / G_loss: 1.7296\n",
      "[ 160000] D_loss: 0.7601 / G_loss: 1.8714\n",
      "[ 161000] D_loss: 0.9917 / G_loss: 2.0409\n",
      "[ 162000] D_loss: 0.6517 / G_loss: 1.8998\n",
      "[ 163000] D_loss: 0.7226 / G_loss: 1.8081\n",
      "[ 164000] D_loss: 0.7684 / G_loss: 1.7303\n",
      "[ 165000] D_loss: 0.5604 / G_loss: 1.9793\n",
      "[ 166000] D_loss: 0.6530 / G_loss: 1.8501\n",
      "[ 167000] D_loss: 0.7303 / G_loss: 2.0307\n",
      "[ 168000] D_loss: 0.8685 / G_loss: 1.8741\n",
      "[ 169000] D_loss: 0.7719 / G_loss: 1.5988\n",
      "[ 170000] D_loss: 0.6480 / G_loss: 2.0362\n",
      "[ 171000] D_loss: 0.7497 / G_loss: 1.6610\n",
      "[ 172000] D_loss: 0.6694 / G_loss: 2.1853\n",
      "[ 173000] D_loss: 0.7354 / G_loss: 1.8802\n",
      "[ 174000] D_loss: 0.6936 / G_loss: 2.0403\n",
      "[ 175000] D_loss: 0.8405 / G_loss: 1.8349\n",
      "[ 176000] D_loss: 0.7066 / G_loss: 2.0094\n",
      "[ 177000] D_loss: 0.7687 / G_loss: 1.8764\n",
      "[ 178000] D_loss: 0.7644 / G_loss: 1.8054\n",
      "[ 179000] D_loss: 0.7602 / G_loss: 2.1650\n",
      "[ 180000] D_loss: 0.7971 / G_loss: 1.8445\n",
      "[ 181000] D_loss: 0.7233 / G_loss: 2.1328\n",
      "[ 182000] D_loss: 0.8134 / G_loss: 1.9760\n",
      "[ 183000] D_loss: 0.8086 / G_loss: 1.9254\n",
      "[ 184000] D_loss: 0.6836 / G_loss: 2.2697\n",
      "[ 185000] D_loss: 0.7042 / G_loss: 1.6524\n",
      "[ 186000] D_loss: 0.7083 / G_loss: 2.0367\n",
      "[ 187000] D_loss: 0.7118 / G_loss: 1.8651\n",
      "[ 188000] D_loss: 0.8177 / G_loss: 2.5692\n",
      "[ 189000] D_loss: 0.7774 / G_loss: 1.9297\n",
      "[ 190000] D_loss: 0.7596 / G_loss: 1.8696\n",
      "[ 191000] D_loss: 0.6567 / G_loss: 1.8590\n",
      "[ 192000] D_loss: 0.7176 / G_loss: 2.1811\n",
      "[ 193000] D_loss: 0.5775 / G_loss: 1.8436\n",
      "[ 194000] D_loss: 0.7672 / G_loss: 2.2698\n",
      "[ 195000] D_loss: 0.7871 / G_loss: 2.0031\n",
      "[ 196000] D_loss: 0.6393 / G_loss: 1.6556\n",
      "[ 197000] D_loss: 0.7081 / G_loss: 2.2078\n",
      "[ 198000] D_loss: 0.8489 / G_loss: 2.1667\n",
      "[ 199000] D_loss: 0.8074 / G_loss: 1.9299\n",
      "[ 200000] D_loss: 0.6879 / G_loss: 1.7832\n",
      "[ 201000] D_loss: 0.6832 / G_loss: 2.3601\n",
      "[ 202000] D_loss: 0.7707 / G_loss: 2.4273\n",
      "[ 203000] D_loss: 0.6879 / G_loss: 2.0600\n",
      "[ 204000] D_loss: 0.8076 / G_loss: 1.9841\n",
      "[ 205000] D_loss: 0.6182 / G_loss: 2.0482\n",
      "[ 206000] D_loss: 0.6452 / G_loss: 2.4395\n",
      "[ 207000] D_loss: 0.8225 / G_loss: 1.9322\n",
      "[ 208000] D_loss: 0.7044 / G_loss: 2.2752\n",
      "[ 209000] D_loss: 0.6872 / G_loss: 2.3143\n",
      "[ 210000] D_loss: 0.5983 / G_loss: 2.1337\n",
      "[ 211000] D_loss: 0.5932 / G_loss: 2.3101\n",
      "[ 212000] D_loss: 0.7101 / G_loss: 2.2314\n",
      "[ 213000] D_loss: 0.7883 / G_loss: 1.9903\n",
      "[ 214000] D_loss: 0.6951 / G_loss: 1.9483\n",
      "[ 215000] D_loss: 0.7820 / G_loss: 2.1220\n",
      "[ 216000] D_loss: 0.6507 / G_loss: 2.0742\n",
      "[ 217000] D_loss: 0.7547 / G_loss: 2.0855\n",
      "[ 218000] D_loss: 0.5763 / G_loss: 2.1749\n",
      "[ 219000] D_loss: 0.6823 / G_loss: 2.2416\n",
      "[ 220000] D_loss: 0.5896 / G_loss: 1.9250\n",
      "[ 221000] D_loss: 0.7162 / G_loss: 1.9467\n",
      "[ 222000] D_loss: 0.7103 / G_loss: 1.9380\n",
      "[ 223000] D_loss: 0.6467 / G_loss: 2.4727\n",
      "[ 224000] D_loss: 0.6346 / G_loss: 2.0955\n",
      "[ 225000] D_loss: 0.5282 / G_loss: 2.0541\n",
      "[ 226000] D_loss: 0.7461 / G_loss: 2.1948\n",
      "[ 227000] D_loss: 0.7806 / G_loss: 2.2360\n",
      "[ 228000] D_loss: 0.7630 / G_loss: 2.1545\n",
      "[ 229000] D_loss: 0.6655 / G_loss: 2.1144\n",
      "[ 230000] D_loss: 0.6210 / G_loss: 2.0627\n",
      "[ 231000] D_loss: 0.6472 / G_loss: 2.0598\n",
      "[ 232000] D_loss: 0.7840 / G_loss: 2.1712\n",
      "[ 233000] D_loss: 0.8011 / G_loss: 2.0648\n",
      "[ 234000] D_loss: 0.7355 / G_loss: 2.1750\n",
      "[ 235000] D_loss: 0.7496 / G_loss: 2.2691\n",
      "[ 236000] D_loss: 0.7444 / G_loss: 2.0396\n",
      "[ 237000] D_loss: 0.6552 / G_loss: 2.3537\n",
      "[ 238000] D_loss: 0.6652 / G_loss: 2.3287\n",
      "[ 239000] D_loss: 0.7134 / G_loss: 2.1231\n",
      "[ 240000] D_loss: 0.7494 / G_loss: 1.9931\n",
      "[ 241000] D_loss: 0.7799 / G_loss: 2.0520\n",
      "[ 242000] D_loss: 0.6232 / G_loss: 2.2814\n",
      "[ 243000] D_loss: 0.6450 / G_loss: 1.8429\n",
      "[ 244000] D_loss: 0.6214 / G_loss: 2.4423\n",
      "[ 245000] D_loss: 0.6366 / G_loss: 2.3860\n",
      "[ 246000] D_loss: 0.5595 / G_loss: 2.0347\n",
      "[ 247000] D_loss: 0.5378 / G_loss: 2.2592\n",
      "[ 248000] D_loss: 0.6247 / G_loss: 2.6158\n",
      "[ 249000] D_loss: 0.6544 / G_loss: 2.3115\n",
      "[ 250000] D_loss: 0.4916 / G_loss: 2.3480\n",
      "[ 251000] D_loss: 0.6776 / G_loss: 1.9882\n",
      "[ 252000] D_loss: 0.5746 / G_loss: 2.2725\n",
      "[ 253000] D_loss: 0.7339 / G_loss: 2.2653\n",
      "[ 254000] D_loss: 0.5539 / G_loss: 2.0612\n",
      "[ 255000] D_loss: 0.7559 / G_loss: 2.3741\n",
      "[ 256000] D_loss: 0.6192 / G_loss: 2.4831\n",
      "[ 257000] D_loss: 0.6258 / G_loss: 2.3160\n",
      "[ 258000] D_loss: 0.6364 / G_loss: 2.1125\n",
      "[ 259000] D_loss: 0.5484 / G_loss: 2.1987\n",
      "[ 260000] D_loss: 0.6696 / G_loss: 2.3811\n",
      "[ 261000] D_loss: 0.5931 / G_loss: 2.4729\n",
      "[ 262000] D_loss: 0.5418 / G_loss: 2.3371\n",
      "[ 263000] D_loss: 0.7259 / G_loss: 2.4368\n",
      "[ 264000] D_loss: 0.6592 / G_loss: 2.3117\n",
      "[ 265000] D_loss: 0.5627 / G_loss: 2.3565\n",
      "[ 266000] D_loss: 0.6908 / G_loss: 2.1820\n",
      "[ 267000] D_loss: 0.5905 / G_loss: 2.1097\n",
      "[ 268000] D_loss: 0.6405 / G_loss: 2.1463\n",
      "[ 269000] D_loss: 0.5908 / G_loss: 2.1645\n",
      "[ 270000] D_loss: 0.6165 / G_loss: 2.4312\n",
      "[ 271000] D_loss: 0.7467 / G_loss: 2.2385\n",
      "[ 272000] D_loss: 0.6962 / G_loss: 2.3144\n",
      "[ 273000] D_loss: 0.5233 / G_loss: 2.4891\n",
      "[ 274000] D_loss: 0.6546 / G_loss: 2.2842\n",
      "[ 275000] D_loss: 0.5205 / G_loss: 2.1390\n",
      "[ 276000] D_loss: 0.7237 / G_loss: 2.3706\n",
      "[ 277000] D_loss: 0.5943 / G_loss: 2.4359\n",
      "[ 278000] D_loss: 0.7221 / G_loss: 2.2790\n",
      "[ 279000] D_loss: 0.7805 / G_loss: 2.2970\n",
      "[ 280000] D_loss: 0.6802 / G_loss: 2.4124\n",
      "[ 281000] D_loss: 0.7463 / G_loss: 2.6014\n",
      "[ 282000] D_loss: 0.5813 / G_loss: 2.2923\n",
      "[ 283000] D_loss: 0.7136 / G_loss: 2.1303\n",
      "[ 284000] D_loss: 0.6005 / G_loss: 2.3128\n",
      "[ 285000] D_loss: 0.7492 / G_loss: 2.1173\n",
      "[ 286000] D_loss: 0.5578 / G_loss: 2.7026\n",
      "[ 287000] D_loss: 0.6202 / G_loss: 1.9986\n",
      "[ 288000] D_loss: 0.7356 / G_loss: 2.4403\n",
      "[ 289000] D_loss: 0.5985 / G_loss: 2.4401\n",
      "[ 290000] D_loss: 0.7648 / G_loss: 2.1037\n",
      "[ 291000] D_loss: 0.6024 / G_loss: 2.1843\n",
      "[ 292000] D_loss: 0.6513 / G_loss: 2.3156\n",
      "[ 293000] D_loss: 0.5954 / G_loss: 2.4745\n",
      "[ 294000] D_loss: 0.6340 / G_loss: 2.3482\n",
      "[ 295000] D_loss: 0.6294 / G_loss: 2.1883\n",
      "[ 296000] D_loss: 0.5557 / G_loss: 2.1318\n",
      "[ 297000] D_loss: 0.5441 / G_loss: 2.4668\n",
      "[ 298000] D_loss: 0.6065 / G_loss: 2.5017\n",
      "[ 299000] D_loss: 0.6326 / G_loss: 2.7978\n",
      "[ 300000] D_loss: 0.5685 / G_loss: 2.5890\n",
      "[ 301000] D_loss: 0.7168 / G_loss: 2.3702\n",
      "[ 302000] D_loss: 0.6050 / G_loss: 2.3419\n",
      "[ 303000] D_loss: 0.6062 / G_loss: 2.5175\n",
      "[ 304000] D_loss: 0.6057 / G_loss: 2.5161\n",
      "[ 305000] D_loss: 0.5516 / G_loss: 2.4134\n",
      "[ 306000] D_loss: 0.5579 / G_loss: 2.2557\n",
      "[ 307000] D_loss: 0.7512 / G_loss: 2.7659\n",
      "[ 308000] D_loss: 0.6148 / G_loss: 2.1950\n",
      "[ 309000] D_loss: 0.5577 / G_loss: 2.3398\n",
      "[ 310000] D_loss: 0.5783 / G_loss: 2.5816\n",
      "[ 311000] D_loss: 0.6140 / G_loss: 2.3282\n",
      "[ 312000] D_loss: 0.6536 / G_loss: 2.7815\n",
      "[ 313000] D_loss: 0.6802 / G_loss: 2.4376\n",
      "[ 314000] D_loss: 0.6536 / G_loss: 2.3128\n",
      "[ 315000] D_loss: 0.5896 / G_loss: 2.5784\n",
      "[ 316000] D_loss: 0.6440 / G_loss: 2.1695\n",
      "[ 317000] D_loss: 0.5602 / G_loss: 2.3747\n",
      "[ 318000] D_loss: 0.5872 / G_loss: 2.4023\n",
      "[ 319000] D_loss: 0.6016 / G_loss: 2.6409\n",
      "[ 320000] D_loss: 0.6057 / G_loss: 2.3719\n",
      "[ 321000] D_loss: 0.7573 / G_loss: 2.5303\n",
      "[ 322000] D_loss: 0.6002 / G_loss: 2.4896\n",
      "[ 323000] D_loss: 0.5132 / G_loss: 2.3262\n",
      "[ 324000] D_loss: 0.5526 / G_loss: 2.5320\n",
      "[ 325000] D_loss: 0.8125 / G_loss: 2.3447\n",
      "[ 326000] D_loss: 0.5900 / G_loss: 2.4473\n",
      "[ 327000] D_loss: 0.5619 / G_loss: 2.2956\n",
      "[ 328000] D_loss: 0.6137 / G_loss: 2.2719\n",
      "[ 329000] D_loss: 0.7989 / G_loss: 2.4383\n",
      "[ 330000] D_loss: 0.6535 / G_loss: 2.9466\n",
      "[ 331000] D_loss: 0.6892 / G_loss: 2.2501\n",
      "[ 332000] D_loss: 0.6785 / G_loss: 2.1045\n",
      "[ 333000] D_loss: 0.5676 / G_loss: 2.3116\n",
      "[ 334000] D_loss: 0.5915 / G_loss: 2.1845\n",
      "[ 335000] D_loss: 0.7139 / G_loss: 2.4737\n",
      "[ 336000] D_loss: 0.5656 / G_loss: 2.1132\n",
      "[ 337000] D_loss: 0.6160 / G_loss: 2.2087\n",
      "[ 338000] D_loss: 0.5303 / G_loss: 2.8193\n",
      "[ 339000] D_loss: 0.5878 / G_loss: 2.2711\n",
      "[ 340000] D_loss: 0.5854 / G_loss: 2.7699\n",
      "[ 341000] D_loss: 0.5757 / G_loss: 2.3094\n",
      "[ 342000] D_loss: 0.6687 / G_loss: 2.5138\n",
      "[ 343000] D_loss: 0.6362 / G_loss: 2.0966\n",
      "[ 344000] D_loss: 0.5626 / G_loss: 2.1260\n",
      "[ 345000] D_loss: 0.5828 / G_loss: 2.3907\n",
      "[ 346000] D_loss: 0.6178 / G_loss: 2.4336\n",
      "[ 347000] D_loss: 0.5537 / G_loss: 2.2556\n",
      "[ 348000] D_loss: 0.7045 / G_loss: 2.1145\n",
      "[ 349000] D_loss: 0.4605 / G_loss: 2.4687\n",
      "[ 350000] D_loss: 0.5556 / G_loss: 2.8395\n",
      "[ 351000] D_loss: 0.5683 / G_loss: 2.3493\n",
      "[ 352000] D_loss: 0.5999 / G_loss: 2.2935\n",
      "[ 353000] D_loss: 0.5379 / G_loss: 2.2682\n",
      "[ 354000] D_loss: 0.5756 / G_loss: 2.2671\n",
      "[ 355000] D_loss: 0.6176 / G_loss: 2.4308\n",
      "[ 356000] D_loss: 0.6035 / G_loss: 2.5033\n",
      "[ 357000] D_loss: 0.5418 / G_loss: 2.7479\n",
      "[ 358000] D_loss: 0.5446 / G_loss: 2.5002\n",
      "[ 359000] D_loss: 0.5546 / G_loss: 2.0981\n",
      "[ 360000] D_loss: 0.6287 / G_loss: 2.4167\n",
      "[ 361000] D_loss: 0.4633 / G_loss: 3.1901\n",
      "[ 362000] D_loss: 0.5212 / G_loss: 2.7396\n",
      "[ 363000] D_loss: 0.5056 / G_loss: 2.4651\n",
      "[ 364000] D_loss: 0.7924 / G_loss: 2.1849\n",
      "[ 365000] D_loss: 0.6419 / G_loss: 2.1979\n",
      "[ 366000] D_loss: 0.6076 / G_loss: 2.6427\n",
      "[ 367000] D_loss: 0.6900 / G_loss: 2.8735\n",
      "[ 368000] D_loss: 0.6109 / G_loss: 2.6683\n",
      "[ 369000] D_loss: 0.7379 / G_loss: 2.0786\n",
      "[ 370000] D_loss: 0.7887 / G_loss: 2.4884\n",
      "[ 371000] D_loss: 0.5296 / G_loss: 2.9255\n",
      "[ 372000] D_loss: 0.5320 / G_loss: 2.3927\n",
      "[ 373000] D_loss: 0.4605 / G_loss: 2.8099\n",
      "[ 374000] D_loss: 0.7322 / G_loss: 2.6517\n",
      "[ 375000] D_loss: 0.5428 / G_loss: 2.6964\n",
      "[ 376000] D_loss: 0.5210 / G_loss: 2.1924\n",
      "[ 377000] D_loss: 0.4505 / G_loss: 2.8353\n",
      "[ 378000] D_loss: 0.5015 / G_loss: 2.4599\n",
      "[ 379000] D_loss: 0.4139 / G_loss: 2.2664\n",
      "[ 380000] D_loss: 0.4826 / G_loss: 2.7775\n",
      "[ 381000] D_loss: 0.5784 / G_loss: 2.4961\n",
      "[ 382000] D_loss: 0.6230 / G_loss: 2.4869\n",
      "[ 383000] D_loss: 0.6098 / G_loss: 2.3029\n",
      "[ 384000] D_loss: 0.5767 / G_loss: 2.4469\n",
      "[ 385000] D_loss: 0.6085 / G_loss: 2.0350\n",
      "[ 386000] D_loss: 0.7422 / G_loss: 2.6058\n",
      "[ 387000] D_loss: 0.4794 / G_loss: 2.5515\n",
      "[ 388000] D_loss: 0.4152 / G_loss: 2.5564\n",
      "[ 389000] D_loss: 0.5745 / G_loss: 2.3650\n",
      "[ 390000] D_loss: 0.4187 / G_loss: 2.3685\n",
      "[ 391000] D_loss: 0.5237 / G_loss: 2.1344\n",
      "[ 392000] D_loss: 0.5455 / G_loss: 2.4401\n",
      "[ 393000] D_loss: 0.5246 / G_loss: 2.4968\n",
      "[ 394000] D_loss: 0.6065 / G_loss: 2.5289\n",
      "[ 395000] D_loss: 0.5179 / G_loss: 2.4644\n",
      "[ 396000] D_loss: 0.6148 / G_loss: 2.6065\n",
      "[ 397000] D_loss: 0.6403 / G_loss: 2.5439\n",
      "[ 398000] D_loss: 0.6204 / G_loss: 2.4975\n",
      "[ 399000] D_loss: 0.7242 / G_loss: 2.3550\n",
      "[ 400000] D_loss: 0.5628 / G_loss: 2.6302\n",
      "[ 401000] D_loss: 0.5224 / G_loss: 2.6442\n",
      "[ 402000] D_loss: 0.4783 / G_loss: 2.2667\n",
      "[ 403000] D_loss: 0.6504 / G_loss: 2.9481\n",
      "[ 404000] D_loss: 0.5074 / G_loss: 3.0753\n",
      "[ 405000] D_loss: 0.5245 / G_loss: 2.6070\n",
      "[ 406000] D_loss: 0.4892 / G_loss: 2.6235\n",
      "[ 407000] D_loss: 0.4854 / G_loss: 2.3584\n",
      "[ 408000] D_loss: 0.5945 / G_loss: 2.6823\n",
      "[ 409000] D_loss: 0.5697 / G_loss: 2.5543\n",
      "[ 410000] D_loss: 0.6257 / G_loss: 2.3569\n",
      "[ 411000] D_loss: 0.5623 / G_loss: 2.6362\n",
      "[ 412000] D_loss: 0.4611 / G_loss: 2.2985\n",
      "[ 413000] D_loss: 0.7027 / G_loss: 2.4154\n",
      "[ 414000] D_loss: 0.5368 / G_loss: 2.5681\n",
      "[ 415000] D_loss: 0.5589 / G_loss: 2.7900\n",
      "[ 416000] D_loss: 0.5000 / G_loss: 2.9541\n",
      "[ 417000] D_loss: 0.5672 / G_loss: 2.4375\n",
      "[ 418000] D_loss: 0.5721 / G_loss: 2.8015\n",
      "[ 419000] D_loss: 0.6168 / G_loss: 2.5727\n",
      "[ 420000] D_loss: 0.5901 / G_loss: 2.3181\n",
      "[ 421000] D_loss: 0.6077 / G_loss: 2.9006\n",
      "[ 422000] D_loss: 0.5833 / G_loss: 2.5094\n",
      "[ 423000] D_loss: 0.5452 / G_loss: 2.5222\n",
      "[ 424000] D_loss: 0.5724 / G_loss: 2.3469\n",
      "[ 425000] D_loss: 0.5462 / G_loss: 2.4294\n",
      "[ 426000] D_loss: 0.6172 / G_loss: 2.4589\n",
      "[ 427000] D_loss: 0.5469 / G_loss: 2.5526\n",
      "[ 428000] D_loss: 0.5819 / G_loss: 2.3660\n",
      "[ 429000] D_loss: 0.5895 / G_loss: 2.8526\n",
      "[ 430000] D_loss: 0.5465 / G_loss: 2.5346\n",
      "[ 431000] D_loss: 0.5674 / G_loss: 2.4461\n",
      "[ 432000] D_loss: 0.6351 / G_loss: 2.8350\n",
      "[ 433000] D_loss: 0.5172 / G_loss: 2.3910\n",
      "[ 434000] D_loss: 0.5792 / G_loss: 2.3391\n",
      "[ 435000] D_loss: 0.5445 / G_loss: 2.3456\n",
      "[ 436000] D_loss: 0.6065 / G_loss: 2.5123\n",
      "[ 437000] D_loss: 0.6357 / G_loss: 2.7869\n",
      "[ 438000] D_loss: 0.6186 / G_loss: 2.7219\n",
      "[ 439000] D_loss: 0.4847 / G_loss: 2.8510\n",
      "[ 440000] D_loss: 0.4805 / G_loss: 2.6419\n",
      "[ 441000] D_loss: 0.5403 / G_loss: 2.6105\n",
      "[ 442000] D_loss: 0.6000 / G_loss: 2.7680\n",
      "[ 443000] D_loss: 0.5920 / G_loss: 2.4245\n",
      "[ 444000] D_loss: 0.5605 / G_loss: 2.6132\n",
      "[ 445000] D_loss: 0.6439 / G_loss: 3.3401\n",
      "[ 446000] D_loss: 0.6037 / G_loss: 2.3800\n",
      "[ 447000] D_loss: 0.5209 / G_loss: 2.2511\n",
      "[ 448000] D_loss: 0.5022 / G_loss: 2.5886\n",
      "[ 449000] D_loss: 0.5040 / G_loss: 2.5907\n",
      "[ 450000] D_loss: 0.4981 / G_loss: 2.7213\n",
      "[ 451000] D_loss: 0.6329 / G_loss: 2.3126\n",
      "[ 452000] D_loss: 0.5611 / G_loss: 2.1874\n",
      "[ 453000] D_loss: 0.6601 / G_loss: 2.7268\n",
      "[ 454000] D_loss: 0.4198 / G_loss: 2.4057\n",
      "[ 455000] D_loss: 0.7982 / G_loss: 2.9488\n",
      "[ 456000] D_loss: 0.6401 / G_loss: 2.8889\n",
      "[ 457000] D_loss: 0.6244 / G_loss: 2.2309\n",
      "[ 458000] D_loss: 0.5926 / G_loss: 2.2201\n",
      "[ 459000] D_loss: 0.5307 / G_loss: 2.8424\n",
      "[ 460000] D_loss: 0.6312 / G_loss: 2.9873\n",
      "[ 461000] D_loss: 0.5724 / G_loss: 2.1580\n",
      "[ 462000] D_loss: 0.6481 / G_loss: 2.8190\n",
      "[ 463000] D_loss: 0.5018 / G_loss: 2.8033\n",
      "[ 464000] D_loss: 0.6333 / G_loss: 2.4960\n",
      "[ 465000] D_loss: 0.4882 / G_loss: 2.2687\n",
      "[ 466000] D_loss: 0.5702 / G_loss: 2.4422\n",
      "[ 467000] D_loss: 0.5179 / G_loss: 2.4585\n",
      "[ 468000] D_loss: 0.4748 / G_loss: 2.2539\n",
      "[ 469000] D_loss: 0.7052 / G_loss: 2.7726\n",
      "[ 470000] D_loss: 0.5251 / G_loss: 2.4824\n",
      "[ 471000] D_loss: 0.4681 / G_loss: 2.3147\n",
      "[ 472000] D_loss: 0.8533 / G_loss: 2.2167\n",
      "[ 473000] D_loss: 0.4433 / G_loss: 2.7576\n",
      "[ 474000] D_loss: 0.4900 / G_loss: 2.1939\n",
      "[ 475000] D_loss: 0.5428 / G_loss: 3.0732\n",
      "[ 476000] D_loss: 0.5891 / G_loss: 2.3307\n",
      "[ 477000] D_loss: 0.5944 / G_loss: 2.6041\n",
      "[ 478000] D_loss: 0.5156 / G_loss: 2.3162\n",
      "[ 479000] D_loss: 0.5926 / G_loss: 2.6243\n",
      "[ 480000] D_loss: 0.6108 / G_loss: 2.3323\n",
      "[ 481000] D_loss: 0.5127 / G_loss: 2.6588\n",
      "[ 482000] D_loss: 0.4916 / G_loss: 3.3696\n",
      "[ 483000] D_loss: 0.6351 / G_loss: 3.0799\n",
      "[ 484000] D_loss: 0.6093 / G_loss: 2.5900\n",
      "[ 485000] D_loss: 0.6287 / G_loss: 2.5812\n",
      "[ 486000] D_loss: 0.4680 / G_loss: 2.7560\n",
      "[ 487000] D_loss: 0.5683 / G_loss: 2.5797\n",
      "[ 488000] D_loss: 0.5290 / G_loss: 3.0892\n",
      "[ 489000] D_loss: 0.5353 / G_loss: 3.0811\n",
      "[ 490000] D_loss: 0.6484 / G_loss: 2.6595\n",
      "[ 491000] D_loss: 0.6194 / G_loss: 2.8517\n",
      "[ 492000] D_loss: 0.5810 / G_loss: 2.5342\n",
      "[ 493000] D_loss: 0.4442 / G_loss: 2.8297\n",
      "[ 494000] D_loss: 0.6147 / G_loss: 2.1219\n",
      "[ 495000] D_loss: 0.4901 / G_loss: 2.6685\n",
      "[ 496000] D_loss: 0.5182 / G_loss: 1.9849\n",
      "[ 497000] D_loss: 0.6198 / G_loss: 2.4677\n",
      "[ 498000] D_loss: 0.6256 / G_loss: 2.7438\n",
      "[ 499000] D_loss: 0.4189 / G_loss: 2.7993\n",
      "[ 500000] D_loss: 0.5324 / G_loss: 2.5761\n",
      "[ 501000] D_loss: 0.5157 / G_loss: 2.5833\n",
      "[ 502000] D_loss: 0.5235 / G_loss: 3.2401\n",
      "[ 503000] D_loss: 0.4887 / G_loss: 2.5193\n",
      "[ 504000] D_loss: 0.6657 / G_loss: 2.9434\n",
      "[ 505000] D_loss: 0.5289 / G_loss: 2.3616\n",
      "[ 506000] D_loss: 0.5421 / G_loss: 2.7377\n",
      "[ 507000] D_loss: 0.5981 / G_loss: 2.3369\n",
      "[ 508000] D_loss: 0.5528 / G_loss: 2.5116\n",
      "[ 509000] D_loss: 0.5286 / G_loss: 2.8143\n",
      "[ 510000] D_loss: 0.5886 / G_loss: 2.9240\n",
      "[ 511000] D_loss: 0.5746 / G_loss: 2.5589\n",
      "[ 512000] D_loss: 0.6192 / G_loss: 2.5210\n",
      "[ 513000] D_loss: 0.6693 / G_loss: 2.4533\n",
      "[ 514000] D_loss: 0.6200 / G_loss: 2.7387\n",
      "[ 515000] D_loss: 0.5209 / G_loss: 2.6734\n",
      "[ 516000] D_loss: 0.6607 / G_loss: 2.9776\n",
      "[ 517000] D_loss: 0.7756 / G_loss: 2.2690\n",
      "[ 518000] D_loss: 0.4775 / G_loss: 2.6824\n",
      "[ 519000] D_loss: 0.5770 / G_loss: 2.8072\n",
      "[ 520000] D_loss: 0.5170 / G_loss: 2.5307\n",
      "[ 521000] D_loss: 0.5224 / G_loss: 2.7973\n",
      "[ 522000] D_loss: 0.5109 / G_loss: 2.6848\n",
      "[ 523000] D_loss: 0.5576 / G_loss: 2.4045\n",
      "[ 524000] D_loss: 0.4270 / G_loss: 3.0249\n",
      "[ 525000] D_loss: 0.5251 / G_loss: 2.5075\n",
      "[ 526000] D_loss: 0.6818 / G_loss: 2.8234\n",
      "[ 527000] D_loss: 0.4703 / G_loss: 2.4683\n",
      "[ 528000] D_loss: 0.5011 / G_loss: 2.4967\n",
      "[ 529000] D_loss: 0.6826 / G_loss: 2.9992\n",
      "[ 530000] D_loss: 0.5052 / G_loss: 3.1109\n",
      "[ 531000] D_loss: 0.5633 / G_loss: 2.1789\n",
      "[ 532000] D_loss: 0.5778 / G_loss: 2.6154\n",
      "[ 533000] D_loss: 0.5257 / G_loss: 2.3054\n",
      "[ 534000] D_loss: 0.5508 / G_loss: 2.8421\n",
      "[ 535000] D_loss: 0.4749 / G_loss: 2.6744\n",
      "[ 536000] D_loss: 0.5390 / G_loss: 3.2650\n",
      "[ 537000] D_loss: 0.4916 / G_loss: 2.4733\n",
      "[ 538000] D_loss: 0.5145 / G_loss: 2.6962\n",
      "[ 539000] D_loss: 0.5498 / G_loss: 2.4012\n",
      "[ 540000] D_loss: 0.5055 / G_loss: 2.8689\n",
      "[ 541000] D_loss: 0.5279 / G_loss: 2.6587\n",
      "[ 542000] D_loss: 0.7082 / G_loss: 3.2818\n",
      "[ 543000] D_loss: 0.5713 / G_loss: 2.6201\n",
      "[ 544000] D_loss: 0.4586 / G_loss: 2.7009\n",
      "[ 545000] D_loss: 0.4890 / G_loss: 2.4703\n",
      "[ 546000] D_loss: 0.7216 / G_loss: 2.6333\n",
      "[ 547000] D_loss: 0.6372 / G_loss: 2.5485\n",
      "[ 548000] D_loss: 0.7793 / G_loss: 2.5839\n",
      "[ 549000] D_loss: 0.4502 / G_loss: 2.6452\n",
      "[ 550000] D_loss: 0.6107 / G_loss: 2.5902\n",
      "[ 551000] D_loss: 0.6225 / G_loss: 2.5538\n",
      "[ 552000] D_loss: 0.4728 / G_loss: 2.9879\n",
      "[ 553000] D_loss: 0.5043 / G_loss: 2.7208\n",
      "[ 554000] D_loss: 0.5965 / G_loss: 2.9913\n",
      "[ 555000] D_loss: 0.4095 / G_loss: 2.3818\n",
      "[ 556000] D_loss: 0.5227 / G_loss: 2.6245\n",
      "[ 557000] D_loss: 0.6249 / G_loss: 2.3916\n",
      "[ 558000] D_loss: 0.5287 / G_loss: 2.6139\n",
      "[ 559000] D_loss: 0.5446 / G_loss: 2.3724\n",
      "[ 560000] D_loss: 0.6468 / G_loss: 1.9886\n",
      "[ 561000] D_loss: 0.5600 / G_loss: 2.8284\n",
      "[ 562000] D_loss: 0.6564 / G_loss: 2.8396\n",
      "[ 563000] D_loss: 0.5769 / G_loss: 2.9571\n",
      "[ 564000] D_loss: 0.5907 / G_loss: 2.6614\n",
      "[ 565000] D_loss: 0.6054 / G_loss: 2.8669\n",
      "[ 566000] D_loss: 0.6385 / G_loss: 2.9072\n",
      "[ 567000] D_loss: 0.4228 / G_loss: 2.3993\n",
      "[ 568000] D_loss: 0.5220 / G_loss: 2.4742\n",
      "[ 569000] D_loss: 0.4024 / G_loss: 2.7119\n",
      "[ 570000] D_loss: 0.4519 / G_loss: 2.4575\n",
      "[ 571000] D_loss: 0.5624 / G_loss: 2.4844\n",
      "[ 572000] D_loss: 0.3833 / G_loss: 2.3654\n",
      "[ 573000] D_loss: 0.4957 / G_loss: 2.5846\n",
      "[ 574000] D_loss: 0.5489 / G_loss: 2.5333\n",
      "[ 575000] D_loss: 0.5450 / G_loss: 2.6229\n",
      "[ 576000] D_loss: 0.6360 / G_loss: 2.8338\n",
      "[ 577000] D_loss: 0.5237 / G_loss: 2.7157\n",
      "[ 578000] D_loss: 0.6360 / G_loss: 2.2738\n",
      "[ 579000] D_loss: 0.5387 / G_loss: 2.5227\n",
      "[ 580000] D_loss: 0.4201 / G_loss: 2.8568\n",
      "[ 581000] D_loss: 0.4450 / G_loss: 2.7871\n",
      "[ 582000] D_loss: 0.6018 / G_loss: 2.6200\n",
      "[ 583000] D_loss: 0.4938 / G_loss: 2.8717\n",
      "[ 584000] D_loss: 0.4935 / G_loss: 2.7597\n",
      "[ 585000] D_loss: 0.5117 / G_loss: 2.6039\n",
      "[ 586000] D_loss: 0.5502 / G_loss: 2.4298\n",
      "[ 587000] D_loss: 0.4685 / G_loss: 2.6085\n",
      "[ 588000] D_loss: 0.4898 / G_loss: 2.9874\n",
      "[ 589000] D_loss: 0.4661 / G_loss: 2.8363\n",
      "[ 590000] D_loss: 0.4670 / G_loss: 3.2253\n",
      "[ 591000] D_loss: 0.5943 / G_loss: 2.3638\n",
      "[ 592000] D_loss: 0.6250 / G_loss: 2.3886\n",
      "[ 593000] D_loss: 0.6550 / G_loss: 2.7685\n",
      "[ 594000] D_loss: 0.4589 / G_loss: 2.8076\n",
      "[ 595000] D_loss: 0.4988 / G_loss: 2.6709\n",
      "[ 596000] D_loss: 0.4588 / G_loss: 2.8465\n",
      "[ 597000] D_loss: 0.4744 / G_loss: 2.8178\n",
      "[ 598000] D_loss: 0.6938 / G_loss: 2.1303\n",
      "[ 599000] D_loss: 0.4652 / G_loss: 2.5065\n",
      "[ 600000] D_loss: 0.6012 / G_loss: 2.7841\n",
      "[ 601000] D_loss: 0.5686 / G_loss: 2.5000\n",
      "[ 602000] D_loss: 0.4152 / G_loss: 2.6090\n",
      "[ 603000] D_loss: 0.6311 / G_loss: 2.8298\n",
      "[ 604000] D_loss: 0.5244 / G_loss: 3.5308\n",
      "[ 605000] D_loss: 0.6838 / G_loss: 2.2934\n",
      "[ 606000] D_loss: 0.6550 / G_loss: 2.7997\n",
      "[ 607000] D_loss: 0.5841 / G_loss: 2.3492\n",
      "[ 608000] D_loss: 0.6809 / G_loss: 2.1937\n",
      "[ 609000] D_loss: 0.6367 / G_loss: 2.8440\n",
      "[ 610000] D_loss: 0.5313 / G_loss: 2.8963\n",
      "[ 611000] D_loss: 0.4906 / G_loss: 2.6478\n",
      "[ 612000] D_loss: 0.4744 / G_loss: 2.7131\n",
      "[ 613000] D_loss: 0.5274 / G_loss: 2.2601\n",
      "[ 614000] D_loss: 0.6033 / G_loss: 2.9356\n",
      "[ 615000] D_loss: 0.7027 / G_loss: 2.4538\n",
      "[ 616000] D_loss: 0.5329 / G_loss: 2.1974\n",
      "[ 617000] D_loss: 0.5848 / G_loss: 3.0165\n",
      "[ 618000] D_loss: 0.6875 / G_loss: 2.8916\n",
      "[ 619000] D_loss: 0.5787 / G_loss: 2.8651\n",
      "[ 620000] D_loss: 0.7219 / G_loss: 2.4881\n",
      "[ 621000] D_loss: 0.6164 / G_loss: 2.6000\n",
      "[ 622000] D_loss: 0.5734 / G_loss: 2.3619\n",
      "[ 623000] D_loss: 0.8618 / G_loss: 2.5671\n",
      "[ 624000] D_loss: 0.5809 / G_loss: 2.8220\n",
      "[ 625000] D_loss: 0.5499 / G_loss: 2.7407\n",
      "[ 626000] D_loss: 0.5542 / G_loss: 2.3605\n",
      "[ 627000] D_loss: 0.4816 / G_loss: 3.0190\n",
      "[ 628000] D_loss: 0.4745 / G_loss: 2.4932\n",
      "[ 629000] D_loss: 0.5347 / G_loss: 2.8456\n",
      "[ 630000] D_loss: 0.6648 / G_loss: 2.5307\n",
      "[ 631000] D_loss: 0.4225 / G_loss: 2.9379\n",
      "[ 632000] D_loss: 0.6927 / G_loss: 2.5707\n",
      "[ 633000] D_loss: 0.5689 / G_loss: 2.5251\n",
      "[ 634000] D_loss: 0.5804 / G_loss: 2.6820\n",
      "[ 635000] D_loss: 0.5776 / G_loss: 2.8184\n",
      "[ 636000] D_loss: 0.5694 / G_loss: 2.8378\n",
      "[ 637000] D_loss: 0.6161 / G_loss: 2.4967\n",
      "[ 638000] D_loss: 0.5759 / G_loss: 2.4584\n",
      "[ 639000] D_loss: 0.5499 / G_loss: 2.4938\n",
      "[ 640000] D_loss: 0.5661 / G_loss: 2.8104\n",
      "[ 641000] D_loss: 0.5479 / G_loss: 2.5825\n",
      "[ 642000] D_loss: 0.4983 / G_loss: 2.6010\n",
      "[ 643000] D_loss: 0.5118 / G_loss: 2.7000\n",
      "[ 644000] D_loss: 0.5207 / G_loss: 2.3731\n",
      "[ 645000] D_loss: 0.6838 / G_loss: 2.8410\n",
      "[ 646000] D_loss: 0.5620 / G_loss: 2.3319\n",
      "[ 647000] D_loss: 0.5216 / G_loss: 2.4535\n",
      "[ 648000] D_loss: 0.6130 / G_loss: 2.5463\n",
      "[ 649000] D_loss: 0.6804 / G_loss: 2.4699\n",
      "[ 650000] D_loss: 0.6366 / G_loss: 2.7884\n",
      "[ 651000] D_loss: 0.6404 / G_loss: 2.5057\n",
      "[ 652000] D_loss: 0.5135 / G_loss: 2.3876\n",
      "[ 653000] D_loss: 0.5262 / G_loss: 2.5930\n",
      "[ 654000] D_loss: 0.4145 / G_loss: 2.9911\n",
      "[ 655000] D_loss: 0.4589 / G_loss: 2.7532\n",
      "[ 656000] D_loss: 0.6712 / G_loss: 2.4549\n",
      "[ 657000] D_loss: 0.4564 / G_loss: 2.4605\n",
      "[ 658000] D_loss: 0.6098 / G_loss: 2.5181\n",
      "[ 659000] D_loss: 0.5268 / G_loss: 2.7930\n",
      "[ 660000] D_loss: 0.5057 / G_loss: 2.8835\n",
      "[ 661000] D_loss: 0.5230 / G_loss: 2.7333\n",
      "[ 662000] D_loss: 0.5413 / G_loss: 2.6025\n",
      "[ 663000] D_loss: 0.4717 / G_loss: 2.9310\n",
      "[ 664000] D_loss: 0.5902 / G_loss: 2.6604\n",
      "[ 665000] D_loss: 0.5711 / G_loss: 2.7984\n",
      "[ 666000] D_loss: 0.6276 / G_loss: 2.3139\n",
      "[ 667000] D_loss: 0.5679 / G_loss: 2.7048\n",
      "[ 668000] D_loss: 0.5385 / G_loss: 2.5081\n",
      "[ 669000] D_loss: 0.5258 / G_loss: 3.1937\n",
      "[ 670000] D_loss: 0.5475 / G_loss: 2.3758\n",
      "[ 671000] D_loss: 0.5302 / G_loss: 2.3936\n",
      "[ 672000] D_loss: 0.6646 / G_loss: 3.0878\n",
      "[ 673000] D_loss: 0.6190 / G_loss: 3.2163\n",
      "[ 674000] D_loss: 0.4963 / G_loss: 2.6698\n",
      "[ 675000] D_loss: 0.6118 / G_loss: 2.4256\n",
      "[ 676000] D_loss: 0.6457 / G_loss: 3.1478\n",
      "[ 677000] D_loss: 0.5881 / G_loss: 2.6211\n",
      "[ 678000] D_loss: 0.4988 / G_loss: 2.7266\n",
      "[ 679000] D_loss: 0.4512 / G_loss: 2.5028\n",
      "[ 680000] D_loss: 0.5121 / G_loss: 2.6597\n",
      "[ 681000] D_loss: 0.4763 / G_loss: 2.6957\n",
      "[ 682000] D_loss: 0.5704 / G_loss: 2.5917\n",
      "[ 683000] D_loss: 0.7122 / G_loss: 3.0243\n",
      "[ 684000] D_loss: 0.5383 / G_loss: 2.9727\n",
      "[ 685000] D_loss: 0.4639 / G_loss: 2.4287\n",
      "[ 686000] D_loss: 0.5802 / G_loss: 2.5631\n",
      "[ 687000] D_loss: 0.6287 / G_loss: 2.6603\n",
      "[ 688000] D_loss: 0.5729 / G_loss: 2.7525\n",
      "[ 689000] D_loss: 0.4295 / G_loss: 2.7701\n",
      "[ 690000] D_loss: 0.6097 / G_loss: 2.1953\n",
      "[ 691000] D_loss: 0.5439 / G_loss: 2.9446\n",
      "[ 692000] D_loss: 0.6427 / G_loss: 2.2175\n",
      "[ 693000] D_loss: 0.5663 / G_loss: 2.6827\n",
      "[ 694000] D_loss: 0.6434 / G_loss: 2.5493\n",
      "[ 695000] D_loss: 0.8134 / G_loss: 2.8415\n",
      "[ 696000] D_loss: 0.5505 / G_loss: 2.7313\n",
      "[ 697000] D_loss: 0.6385 / G_loss: 2.8238\n",
      "[ 698000] D_loss: 0.3638 / G_loss: 2.5387\n",
      "[ 699000] D_loss: 0.5792 / G_loss: 2.9686\n",
      "[ 700000] D_loss: 0.5651 / G_loss: 2.3181\n",
      "[ 701000] D_loss: 0.5664 / G_loss: 3.0801\n",
      "[ 702000] D_loss: 0.5687 / G_loss: 2.8220\n",
      "[ 703000] D_loss: 0.5946 / G_loss: 2.2208\n",
      "[ 704000] D_loss: 0.4916 / G_loss: 2.7272\n",
      "[ 705000] D_loss: 0.5445 / G_loss: 2.4771\n",
      "[ 706000] D_loss: 0.4756 / G_loss: 2.5437\n",
      "[ 707000] D_loss: 0.6634 / G_loss: 2.6321\n",
      "[ 708000] D_loss: 0.5809 / G_loss: 2.7130\n",
      "[ 709000] D_loss: 0.6393 / G_loss: 2.4536\n",
      "[ 710000] D_loss: 0.4477 / G_loss: 2.4215\n",
      "[ 711000] D_loss: 0.4864 / G_loss: 2.5556\n",
      "[ 712000] D_loss: 0.5678 / G_loss: 2.4475\n",
      "[ 713000] D_loss: 0.5727 / G_loss: 2.8223\n",
      "[ 714000] D_loss: 0.5283 / G_loss: 3.1523\n",
      "[ 715000] D_loss: 0.5964 / G_loss: 2.3647\n",
      "[ 716000] D_loss: 0.5489 / G_loss: 2.9633\n",
      "[ 717000] D_loss: 0.5274 / G_loss: 2.4462\n",
      "[ 718000] D_loss: 0.5737 / G_loss: 2.2886\n",
      "[ 719000] D_loss: 0.4678 / G_loss: 2.7051\n",
      "[ 720000] D_loss: 0.5426 / G_loss: 2.6484\n",
      "[ 721000] D_loss: 0.5824 / G_loss: 2.6215\n",
      "[ 722000] D_loss: 0.3877 / G_loss: 2.6493\n",
      "[ 723000] D_loss: 0.5214 / G_loss: 2.4095\n",
      "[ 724000] D_loss: 0.6303 / G_loss: 2.7943\n",
      "[ 725000] D_loss: 0.6058 / G_loss: 2.2678\n",
      "[ 726000] D_loss: 0.5977 / G_loss: 2.6768\n",
      "[ 727000] D_loss: 0.6783 / G_loss: 2.3700\n",
      "[ 728000] D_loss: 0.5858 / G_loss: 2.7185\n",
      "[ 729000] D_loss: 0.6631 / G_loss: 2.8506\n",
      "[ 730000] D_loss: 0.5413 / G_loss: 2.3454\n",
      "[ 731000] D_loss: 0.5904 / G_loss: 2.5728\n",
      "[ 732000] D_loss: 0.5076 / G_loss: 3.0143\n",
      "[ 733000] D_loss: 0.5107 / G_loss: 2.4600\n",
      "[ 734000] D_loss: 0.5766 / G_loss: 2.7421\n",
      "[ 735000] D_loss: 0.7346 / G_loss: 2.5596\n",
      "[ 736000] D_loss: 0.4538 / G_loss: 3.2604\n",
      "[ 737000] D_loss: 0.5565 / G_loss: 2.5185\n",
      "[ 738000] D_loss: 0.5143 / G_loss: 2.5412\n",
      "[ 739000] D_loss: 0.6466 / G_loss: 2.5076\n",
      "[ 740000] D_loss: 0.5262 / G_loss: 2.8060\n",
      "[ 741000] D_loss: 0.5694 / G_loss: 2.2147\n",
      "[ 742000] D_loss: 0.7942 / G_loss: 2.6388\n",
      "[ 743000] D_loss: 0.7751 / G_loss: 2.7886\n",
      "[ 744000] D_loss: 0.6286 / G_loss: 2.5158\n",
      "[ 745000] D_loss: 0.5879 / G_loss: 2.2790\n",
      "[ 746000] D_loss: 0.5586 / G_loss: 2.4633\n",
      "[ 747000] D_loss: 0.4284 / G_loss: 2.3645\n",
      "[ 748000] D_loss: 0.4703 / G_loss: 2.5858\n",
      "[ 749000] D_loss: 0.6681 / G_loss: 3.1414\n",
      "[ 750000] D_loss: 0.5563 / G_loss: 2.7672\n",
      "[ 751000] D_loss: 0.5364 / G_loss: 2.4407\n",
      "[ 752000] D_loss: 0.5049 / G_loss: 2.7016\n",
      "[ 753000] D_loss: 0.6360 / G_loss: 2.4223\n",
      "[ 754000] D_loss: 0.6278 / G_loss: 2.9015\n",
      "[ 755000] D_loss: 0.5446 / G_loss: 3.0776\n",
      "[ 756000] D_loss: 0.5635 / G_loss: 2.4683\n",
      "[ 757000] D_loss: 0.6291 / G_loss: 2.6703\n",
      "[ 758000] D_loss: 0.6235 / G_loss: 2.8928\n",
      "[ 759000] D_loss: 0.6062 / G_loss: 2.8931\n",
      "[ 760000] D_loss: 0.4450 / G_loss: 2.8094\n",
      "[ 761000] D_loss: 0.6035 / G_loss: 2.5287\n",
      "[ 762000] D_loss: 0.5664 / G_loss: 2.8149\n",
      "[ 763000] D_loss: 0.4731 / G_loss: 2.6047\n",
      "[ 764000] D_loss: 0.6305 / G_loss: 2.8608\n",
      "[ 765000] D_loss: 0.4306 / G_loss: 2.3884\n",
      "[ 766000] D_loss: 0.6350 / G_loss: 2.4797\n",
      "[ 767000] D_loss: 0.5949 / G_loss: 2.5675\n",
      "[ 768000] D_loss: 0.4364 / G_loss: 2.6405\n",
      "[ 769000] D_loss: 0.5715 / G_loss: 3.2429\n",
      "[ 770000] D_loss: 0.5335 / G_loss: 2.7137\n",
      "[ 771000] D_loss: 0.5192 / G_loss: 2.6507\n",
      "[ 772000] D_loss: 0.4792 / G_loss: 2.5983\n",
      "[ 773000] D_loss: 0.4733 / G_loss: 2.6334\n",
      "[ 774000] D_loss: 0.5894 / G_loss: 3.6681\n",
      "[ 775000] D_loss: 0.3780 / G_loss: 2.6740\n",
      "[ 776000] D_loss: 0.4504 / G_loss: 2.9101\n",
      "[ 777000] D_loss: 0.7127 / G_loss: 2.4951\n",
      "[ 778000] D_loss: 0.4033 / G_loss: 2.7489\n",
      "[ 779000] D_loss: 0.4725 / G_loss: 2.8813\n",
      "[ 780000] D_loss: 0.6244 / G_loss: 2.5942\n",
      "[ 781000] D_loss: 0.5547 / G_loss: 3.0360\n",
      "[ 782000] D_loss: 0.6949 / G_loss: 2.3886\n",
      "[ 783000] D_loss: 0.5268 / G_loss: 2.5502\n",
      "[ 784000] D_loss: 0.6489 / G_loss: 2.4171\n",
      "[ 785000] D_loss: 0.4258 / G_loss: 3.0676\n",
      "[ 786000] D_loss: 0.5044 / G_loss: 3.2078\n",
      "[ 787000] D_loss: 0.5880 / G_loss: 2.4989\n",
      "[ 788000] D_loss: 0.4220 / G_loss: 3.6561\n",
      "[ 789000] D_loss: 0.5698 / G_loss: 2.9626\n",
      "[ 790000] D_loss: 0.5145 / G_loss: 2.8224\n",
      "[ 791000] D_loss: 0.6275 / G_loss: 2.6083\n",
      "[ 792000] D_loss: 0.4392 / G_loss: 2.8543\n",
      "[ 793000] D_loss: 0.5455 / G_loss: 2.6863\n",
      "[ 794000] D_loss: 0.5652 / G_loss: 2.7406\n",
      "[ 795000] D_loss: 0.6773 / G_loss: 2.6397\n",
      "[ 796000] D_loss: 0.4746 / G_loss: 2.5650\n",
      "[ 797000] D_loss: 0.5163 / G_loss: 2.5724\n",
      "[ 798000] D_loss: 0.5377 / G_loss: 2.8980\n",
      "[ 799000] D_loss: 0.6389 / G_loss: 2.6410\n",
      "[ 800000] D_loss: 0.7166 / G_loss: 2.8835\n",
      "[ 801000] D_loss: 0.4459 / G_loss: 2.4313\n",
      "[ 802000] D_loss: 0.4497 / G_loss: 2.7230\n",
      "[ 803000] D_loss: 0.4525 / G_loss: 2.7564\n",
      "[ 804000] D_loss: 0.4358 / G_loss: 2.7600\n",
      "[ 805000] D_loss: 0.4050 / G_loss: 2.8318\n",
      "[ 806000] D_loss: 0.4841 / G_loss: 2.8461\n",
      "[ 807000] D_loss: 0.5496 / G_loss: 2.4923\n",
      "[ 808000] D_loss: 0.4892 / G_loss: 2.6181\n",
      "[ 809000] D_loss: 0.4787 / G_loss: 3.0070\n",
      "[ 810000] D_loss: 0.5868 / G_loss: 2.2411\n",
      "[ 811000] D_loss: 0.4838 / G_loss: 2.6643\n",
      "[ 812000] D_loss: 0.6499 / G_loss: 2.9518\n",
      "[ 813000] D_loss: 0.3803 / G_loss: 2.7220\n",
      "[ 814000] D_loss: 0.4658 / G_loss: 2.2997\n",
      "[ 815000] D_loss: 0.5589 / G_loss: 3.0070\n",
      "[ 816000] D_loss: 0.5794 / G_loss: 2.6601\n",
      "[ 817000] D_loss: 0.4662 / G_loss: 2.6596\n",
      "[ 818000] D_loss: 0.4639 / G_loss: 2.8341\n",
      "[ 819000] D_loss: 0.5568 / G_loss: 2.9496\n",
      "[ 820000] D_loss: 0.5965 / G_loss: 2.5025\n",
      "[ 821000] D_loss: 0.5246 / G_loss: 2.6065\n",
      "[ 822000] D_loss: 0.4928 / G_loss: 2.7816\n",
      "[ 823000] D_loss: 0.3880 / G_loss: 3.4511\n",
      "[ 824000] D_loss: 0.6881 / G_loss: 2.9959\n",
      "[ 825000] D_loss: 0.4430 / G_loss: 3.0423\n",
      "[ 826000] D_loss: 0.5357 / G_loss: 2.6298\n",
      "[ 827000] D_loss: 0.5889 / G_loss: 2.5009\n",
      "[ 828000] D_loss: 0.6183 / G_loss: 2.6909\n",
      "[ 829000] D_loss: 0.5564 / G_loss: 2.8915\n",
      "[ 830000] D_loss: 0.5496 / G_loss: 2.8314\n",
      "[ 831000] D_loss: 0.6113 / G_loss: 2.5606\n",
      "[ 832000] D_loss: 0.5613 / G_loss: 2.5797\n",
      "[ 833000] D_loss: 0.4347 / G_loss: 2.6507\n",
      "[ 834000] D_loss: 0.6084 / G_loss: 2.4519\n",
      "[ 835000] D_loss: 0.5118 / G_loss: 2.9149\n",
      "[ 836000] D_loss: 0.5467 / G_loss: 3.4184\n",
      "[ 837000] D_loss: 0.5217 / G_loss: 2.9332\n",
      "[ 838000] D_loss: 0.5664 / G_loss: 2.4060\n",
      "[ 839000] D_loss: 0.5119 / G_loss: 2.7845\n",
      "[ 840000] D_loss: 0.6448 / G_loss: 3.0470\n",
      "[ 841000] D_loss: 0.6322 / G_loss: 2.6312\n",
      "[ 842000] D_loss: 0.5033 / G_loss: 2.7170\n",
      "[ 843000] D_loss: 0.4852 / G_loss: 2.6002\n",
      "[ 844000] D_loss: 0.5378 / G_loss: 2.5975\n",
      "[ 845000] D_loss: 0.4620 / G_loss: 3.4418\n",
      "[ 846000] D_loss: 0.5587 / G_loss: 3.0655\n",
      "[ 847000] D_loss: 0.6222 / G_loss: 2.8382\n",
      "[ 848000] D_loss: 0.4717 / G_loss: 2.2756\n",
      "[ 849000] D_loss: 0.4282 / G_loss: 2.9268\n",
      "[ 850000] D_loss: 0.5000 / G_loss: 3.2431\n",
      "[ 851000] D_loss: 0.5010 / G_loss: 2.5829\n",
      "[ 852000] D_loss: 0.3703 / G_loss: 3.5348\n",
      "[ 853000] D_loss: 0.5075 / G_loss: 3.0475\n",
      "[ 854000] D_loss: 0.5553 / G_loss: 2.5842\n",
      "[ 855000] D_loss: 0.6024 / G_loss: 3.3466\n",
      "[ 856000] D_loss: 0.4114 / G_loss: 2.7458\n",
      "[ 857000] D_loss: 0.4940 / G_loss: 3.1913\n",
      "[ 858000] D_loss: 0.5900 / G_loss: 2.9547\n",
      "[ 859000] D_loss: 0.5582 / G_loss: 2.6709\n",
      "[ 860000] D_loss: 0.4162 / G_loss: 2.8970\n",
      "[ 861000] D_loss: 0.4830 / G_loss: 2.6903\n",
      "[ 862000] D_loss: 0.5635 / G_loss: 2.9230\n",
      "[ 863000] D_loss: 0.4888 / G_loss: 2.5986\n",
      "[ 864000] D_loss: 0.5619 / G_loss: 2.7483\n",
      "[ 865000] D_loss: 0.5778 / G_loss: 2.5944\n",
      "[ 866000] D_loss: 0.5174 / G_loss: 3.0794\n",
      "[ 867000] D_loss: 0.5555 / G_loss: 2.8458\n",
      "[ 868000] D_loss: 0.5257 / G_loss: 3.1223\n",
      "[ 869000] D_loss: 0.4927 / G_loss: 3.0806\n",
      "[ 870000] D_loss: 0.5401 / G_loss: 2.7532\n",
      "[ 871000] D_loss: 0.4480 / G_loss: 2.7094\n",
      "[ 872000] D_loss: 0.5359 / G_loss: 2.5864\n",
      "[ 873000] D_loss: 0.5080 / G_loss: 3.2200\n",
      "[ 874000] D_loss: 0.4844 / G_loss: 2.7819\n",
      "[ 875000] D_loss: 0.5651 / G_loss: 2.8521\n",
      "[ 876000] D_loss: 0.3846 / G_loss: 2.6618\n",
      "[ 877000] D_loss: 0.4979 / G_loss: 3.0827\n",
      "[ 878000] D_loss: 0.6813 / G_loss: 2.6237\n",
      "[ 879000] D_loss: 0.4452 / G_loss: 2.6366\n",
      "[ 880000] D_loss: 0.6201 / G_loss: 3.0170\n",
      "[ 881000] D_loss: 0.5085 / G_loss: 2.6988\n",
      "[ 882000] D_loss: 0.6061 / G_loss: 2.8832\n",
      "[ 883000] D_loss: 0.4474 / G_loss: 3.2813\n",
      "[ 884000] D_loss: 0.5253 / G_loss: 2.6791\n",
      "[ 885000] D_loss: 0.5522 / G_loss: 2.6329\n",
      "[ 886000] D_loss: 0.5469 / G_loss: 2.6268\n",
      "[ 887000] D_loss: 0.6332 / G_loss: 2.8675\n",
      "[ 888000] D_loss: 0.5637 / G_loss: 2.9497\n",
      "[ 889000] D_loss: 0.5849 / G_loss: 2.9049\n",
      "[ 890000] D_loss: 0.5181 / G_loss: 2.7319\n",
      "[ 891000] D_loss: 0.5344 / G_loss: 2.8651\n",
      "[ 892000] D_loss: 0.5048 / G_loss: 2.9277\n",
      "[ 893000] D_loss: 0.4677 / G_loss: 2.9461\n",
      "[ 894000] D_loss: 0.6010 / G_loss: 2.7818\n",
      "[ 895000] D_loss: 0.5447 / G_loss: 2.9165\n",
      "[ 896000] D_loss: 0.5903 / G_loss: 2.7240\n",
      "[ 897000] D_loss: 0.5031 / G_loss: 2.6325\n",
      "[ 898000] D_loss: 0.4457 / G_loss: 2.9654\n",
      "[ 899000] D_loss: 0.7726 / G_loss: 2.0714\n",
      "[ 900000] D_loss: 0.5767 / G_loss: 3.0656\n",
      "[ 901000] D_loss: 0.4303 / G_loss: 2.5830\n",
      "[ 902000] D_loss: 0.4183 / G_loss: 2.9035\n",
      "[ 903000] D_loss: 0.5098 / G_loss: 2.6987\n",
      "[ 904000] D_loss: 0.4595 / G_loss: 2.2790\n",
      "[ 905000] D_loss: 0.5661 / G_loss: 3.0633\n",
      "[ 906000] D_loss: 0.6218 / G_loss: 3.0293\n",
      "[ 907000] D_loss: 0.5404 / G_loss: 2.8291\n",
      "[ 908000] D_loss: 0.6033 / G_loss: 2.8998\n",
      "[ 909000] D_loss: 0.4130 / G_loss: 2.7811\n",
      "[ 910000] D_loss: 0.5307 / G_loss: 3.1497\n",
      "[ 911000] D_loss: 0.4710 / G_loss: 2.6528\n",
      "[ 912000] D_loss: 0.5495 / G_loss: 2.9969\n",
      "[ 913000] D_loss: 0.3682 / G_loss: 3.1690\n",
      "[ 914000] D_loss: 0.4689 / G_loss: 2.5425\n",
      "[ 915000] D_loss: 0.4960 / G_loss: 2.1774\n",
      "[ 916000] D_loss: 0.5198 / G_loss: 2.9031\n",
      "[ 917000] D_loss: 0.5444 / G_loss: 3.0222\n",
      "[ 918000] D_loss: 0.4934 / G_loss: 2.9057\n",
      "[ 919000] D_loss: 0.5290 / G_loss: 3.2199\n",
      "[ 920000] D_loss: 0.4748 / G_loss: 2.9674\n",
      "[ 921000] D_loss: 0.4664 / G_loss: 3.2813\n",
      "[ 922000] D_loss: 0.6154 / G_loss: 3.0535\n",
      "[ 923000] D_loss: 0.5874 / G_loss: 2.7543\n",
      "[ 924000] D_loss: 0.5956 / G_loss: 2.5545\n",
      "[ 925000] D_loss: 0.4754 / G_loss: 2.5123\n",
      "[ 926000] D_loss: 0.5835 / G_loss: 2.4726\n",
      "[ 927000] D_loss: 0.5441 / G_loss: 3.0073\n",
      "[ 928000] D_loss: 0.4409 / G_loss: 2.2724\n",
      "[ 929000] D_loss: 0.5143 / G_loss: 2.7841\n",
      "[ 930000] D_loss: 0.4697 / G_loss: 3.0417\n",
      "[ 931000] D_loss: 0.5777 / G_loss: 2.8665\n",
      "[ 932000] D_loss: 0.4961 / G_loss: 3.1548\n",
      "[ 933000] D_loss: 0.7164 / G_loss: 2.7104\n",
      "[ 934000] D_loss: 0.5065 / G_loss: 2.8777\n",
      "[ 935000] D_loss: 0.4930 / G_loss: 2.8004\n",
      "[ 936000] D_loss: 0.4970 / G_loss: 2.6702\n",
      "[ 937000] D_loss: 0.5264 / G_loss: 2.5861\n",
      "[ 938000] D_loss: 0.5374 / G_loss: 2.7890\n",
      "[ 939000] D_loss: 0.3841 / G_loss: 3.2559\n",
      "[ 940000] D_loss: 0.3467 / G_loss: 2.8254\n",
      "[ 941000] D_loss: 0.5288 / G_loss: 2.8541\n",
      "[ 942000] D_loss: 0.4998 / G_loss: 2.7451\n",
      "[ 943000] D_loss: 0.5682 / G_loss: 2.3969\n",
      "[ 944000] D_loss: 0.3806 / G_loss: 2.7579\n",
      "[ 945000] D_loss: 0.4271 / G_loss: 3.0445\n",
      "[ 946000] D_loss: 0.5408 / G_loss: 2.6744\n",
      "[ 947000] D_loss: 0.5412 / G_loss: 2.4344\n",
      "[ 948000] D_loss: 0.4600 / G_loss: 2.8995\n",
      "[ 949000] D_loss: 0.4801 / G_loss: 2.8277\n",
      "[ 950000] D_loss: 0.4266 / G_loss: 2.8114\n",
      "[ 951000] D_loss: 0.5417 / G_loss: 2.5787\n",
      "[ 952000] D_loss: 0.6610 / G_loss: 2.5899\n",
      "[ 953000] D_loss: 0.4619 / G_loss: 2.5516\n",
      "[ 954000] D_loss: 0.5055 / G_loss: 2.8651\n",
      "[ 955000] D_loss: 0.4914 / G_loss: 2.8496\n",
      "[ 956000] D_loss: 0.5859 / G_loss: 3.1703\n",
      "[ 957000] D_loss: 0.4800 / G_loss: 2.9493\n",
      "[ 958000] D_loss: 0.6916 / G_loss: 2.4077\n",
      "[ 959000] D_loss: 0.4493 / G_loss: 2.8098\n",
      "[ 960000] D_loss: 0.5318 / G_loss: 2.8890\n",
      "[ 961000] D_loss: 0.4169 / G_loss: 3.2043\n",
      "[ 962000] D_loss: 0.4170 / G_loss: 3.2255\n",
      "[ 963000] D_loss: 0.4365 / G_loss: 2.6389\n",
      "[ 964000] D_loss: 0.5241 / G_loss: 2.5792\n",
      "[ 965000] D_loss: 0.4023 / G_loss: 2.9314\n",
      "[ 966000] D_loss: 0.4703 / G_loss: 3.4595\n",
      "[ 967000] D_loss: 0.5555 / G_loss: 2.5974\n",
      "[ 968000] D_loss: 0.5845 / G_loss: 2.4890\n",
      "[ 969000] D_loss: 0.5909 / G_loss: 2.4903\n",
      "[ 970000] D_loss: 0.5638 / G_loss: 2.8399\n",
      "[ 971000] D_loss: 0.5142 / G_loss: 2.7352\n",
      "[ 972000] D_loss: 0.4415 / G_loss: 2.8639\n",
      "[ 973000] D_loss: 0.6516 / G_loss: 3.0344\n",
      "[ 974000] D_loss: 0.5503 / G_loss: 3.1214\n",
      "[ 975000] D_loss: 0.6251 / G_loss: 2.8753\n",
      "[ 976000] D_loss: 0.6386 / G_loss: 2.5077\n",
      "[ 977000] D_loss: 0.4606 / G_loss: 2.9404\n",
      "[ 978000] D_loss: 0.4933 / G_loss: 2.5825\n",
      "[ 979000] D_loss: 0.5057 / G_loss: 2.7647\n",
      "[ 980000] D_loss: 0.4398 / G_loss: 2.9461\n",
      "[ 981000] D_loss: 0.5553 / G_loss: 2.4536\n",
      "[ 982000] D_loss: 0.5714 / G_loss: 2.6034\n",
      "[ 983000] D_loss: 0.5882 / G_loss: 2.4812\n",
      "[ 984000] D_loss: 0.5463 / G_loss: 3.1431\n",
      "[ 985000] D_loss: 0.4421 / G_loss: 2.5334\n",
      "[ 986000] D_loss: 0.5261 / G_loss: 2.8061\n",
      "[ 987000] D_loss: 0.5015 / G_loss: 3.0183\n",
      "[ 988000] D_loss: 0.6452 / G_loss: 3.2470\n",
      "[ 989000] D_loss: 0.5267 / G_loss: 2.7014\n",
      "[ 990000] D_loss: 0.5122 / G_loss: 2.3976\n",
      "[ 991000] D_loss: 0.5329 / G_loss: 2.8598\n",
      "[ 992000] D_loss: 0.3947 / G_loss: 2.7057\n",
      "[ 993000] D_loss: 0.5719 / G_loss: 2.8690\n",
      "[ 994000] D_loss: 0.4529 / G_loss: 2.8812\n",
      "[ 995000] D_loss: 0.4829 / G_loss: 2.3578\n",
      "[ 996000] D_loss: 0.5373 / G_loss: 2.4138\n",
      "[ 997000] D_loss: 0.4508 / G_loss: 2.4290\n",
      "[ 998000] D_loss: 0.6188 / G_loss: 2.8934\n",
      "[ 999000] D_loss: 0.4925 / G_loss: 3.3750\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "# sess.run(tf.initialize_all_variables())\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for it in range(1000000):\n",
    "    if it % 1000 == 0:\n",
    "        y = np.zeros([16, 10])\n",
    "        for i in range(16):\n",
    "            y[i, i%10] = 1\n",
    "        samples = sess.run(G_sample, feed_dict={Y: y, Z: sample_Z(16, Z_dim)})\n",
    "        \n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out_num/{:0>4d}.png'.format(idx), bbox_inches='tight')\n",
    "        idx += 1\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # _ 는 원래 Y 임.\n",
    "    X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "    \n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_batch, Y: Y_batch, Z: sample_Z(batch_size, Z_dim)})\n",
    "    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Y: Y_batch, Z: sample_Z(batch_size, Z_dim)})\n",
    "    \n",
    "    if it % 1000 == 0:\n",
    "        print(\"[{:7d}] D_loss: {:.4f} / G_loss: {:.4f}\".format(it, D_loss_curr, G_loss_curr))\n",
    "#         D_losses.append(D_loss_curr)\n",
    "#         G_losses.append(G_loss_curr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator accuracy comparision by number\n",
    "\n",
    "* 각 숫자에 대해서 discriminator 의 정확도를 비교해보자.\n",
    "* Vanilla GAN 의 수행 결과가 1로 수렴하는것으로 봐서는 1이 가장 정확해야 함.\n",
    "* D_fake_sum 은 fake data (generated by G) 에 대한 판단 결과이므로, 높을수록 generator 가 잘 생성한다 (discriminator 를 잘 속인다) 는 의미다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 218.727233887\n",
      "[1] 236.713623047\n",
      "[2] 127.534362793\n",
      "[3] 213.932510376\n",
      "[4] 137.461715698\n",
      "[5] 173.825912476\n",
      "[6] 165.678924561\n",
      "[7] 133.268173218\n",
      "[8] 187.24105835\n",
      "[9] 184.819854736\n"
     ]
    }
   ],
   "source": [
    "for num in range(10):\n",
    "    N = 1000 # 각 숫자당 테스트횟수\n",
    "    y = np.zeros([N, 10])\n",
    "    y[:, num] = 1.\n",
    "    D_fake_res = sess.run(D_fake, feed_dict={Y: y, Z: sample_Z(N, Z_dim)})\n",
    "    D_fake_sum = np.sum(D_fake_res)\n",
    "    print(\"[{}] {}\".format(num, D_fake_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
